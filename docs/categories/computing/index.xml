<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computing on Boettiger Group</title>
    <link>/categories/computing/</link>
    <description>Recent content in Computing on Boettiger Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Aug 2015 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/computing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/2015/08/27/nimble-model-construction/</link>
      <pubDate>Thu, 27 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/08/27/nimble-model-construction/</guid>
      <description>library(&amp;quot;nimble&amp;quot;) ## ## Attaching package: &amp;#39;nimble&amp;#39; ## The following object is masked from &amp;#39;package:stats&amp;#39;: ## ## simulate library(&amp;quot;lazyeval&amp;quot;) library(&amp;quot;methods&amp;quot;) Can we declare the nimbleCode parts in sections? Such as defining the model and priors in separate, reusable code blocks? Here’s a “model” block:
model &amp;lt;- nimbleCode({ for (i in 1:N){ theta[i] ~ dgamma(alpha,beta) lambda[i] &amp;lt;- theta[i]*t[i] x[i] ~ dpois(lambda[i]) } }) We may also wish to define the priors separately, and with specified list of hyperparameters for the priors.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2015/05/15/diagrammer/</link>
      <pubDate>Fri, 15 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/05/15/diagrammer/</guid>
      <description>Exploring ways for using DiagrammeR to generate graphs/plots that can be exported to svg and included in knitr documents.
devtools::install_github(&amp;quot;rich-iannone/DiagrammeR&amp;quot;) library(&amp;quot;DiagrammeR&amp;quot;) n &amp;lt;- c(&amp;quot;archive&amp;quot;, &amp;quot;index&amp;quot;, &amp;quot;share&amp;quot;, &amp;quot;discover&amp;quot;, &amp;quot;use&amp;quot;, &amp;quot;attribute&amp;quot;) nodes &amp;lt;- create_nodes(nodes = c(&amp;quot;Repository\n Roles&amp;quot;, n, &amp;quot;Researcher\n Roles&amp;quot;), shape = &amp;quot;circle&amp;quot;, color = c(rep(&amp;quot;PowderBlue&amp;quot;, 4), rep(&amp;quot;Linen&amp;quot;, 4)), style = &amp;quot;filled&amp;quot;) edges &amp;lt;- create_edges(from = n, to = c(n[-1], n[1]), color = &amp;quot;gray&amp;quot;, penwidth = 4) graph &amp;lt;- create_graph(nodes = nodes, edges = edges, graph_attrs = c(&amp;quot;layout = circo&amp;quot;)) out &amp;lt;- render_graph(graph) Render as SVG</description>
    </item>
    
    <item>
      <title></title>
      <link>/2015/04/15/misc-notes-on-cloud-providers/</link>
      <pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/15/misc-notes-on-cloud-providers/</guid>
      <description>ongoing notes comparing computing resources
Comparing compute resources:
 AWS EC2, DigitalOcean, and Google Compute Engine look significantly cheaper than Rackspace, Azure, etc. (The Economist has estimated that AWS EC2 prices are roughly 3% below cost).
 DigitalOcean tends to have the lowest price point for on-demand use of a given set of resources; usually comparable to the Amazon or EC2 persistent (e.g. monthly) use. All-inclusive pricing (storage, networking etc) and limited options make choices simple and total costs easy to anticipate, though lack the flexibility to fine-tune a configuration to pay for exactly what you need.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2015/03/24/docker-machine-notes/</link>
      <pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/03/24/docker-machine-notes/</guid>
      <description>Docker recently released docker-machine to make managing multiple remote machines locally easier. Docker distributes binaries of docker-machine for most major architectures ready-to-go, potentially making it easier to get started on Windows and Mac as well.
Set credentials in environmental variables so we don’t have to pass them on the command line each time:
DIGITALOCEAN_ACCESS_TOKEN = XXX and create the docker-machine:
docker-machine create --driver digitalocean --digitalocean-size &amp;quot;1gb&amp;quot; server-name where server-name is any name you want to give your server and DO_PAT is your access token (say, saved as an environmental variable).</description>
    </item>
    
    <item>
      <title></title>
      <link>/2015/03/02/notes/</link>
      <pubDate>Mon, 02 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/03/02/notes/</guid>
      <description>Misc tricks  docker X11 sharing  docker run -ti --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:ro r-base  devtools::install_version devtools::use_readme_rmd (with git commit hooks)
 How to specify a custom action for the knit button: add knit: (function(inputFile, encoding) ...) to the top-level YAML header; e.g. to control output directory:
  rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(dirname(inputFile), basename(inputFile), &amp;quot;.pdf&amp;quot;))  </description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/26/coreos-cluster-gotchas/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/26/coreos-cluster-gotchas/</guid>
      <description>Overall impression is that CoreOS is a promising way to easily set up a highly available cluster (e.g. when most important thing is that a service stays up when a node goes down) since it can migrate a containerized app to a new machine rather than having to already have the same app running on all machines. Either way a load-balancer needs to handle the addressing, which is do-able but somewhat tricky.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/24/coreos-docker-registries-etc/</link>
      <pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/24/coreos-docker-registries-etc/</guid>
      <description>A secure docker registry Running one&amp;rsquo;s own docker registry is far more elegant than moving tarballs between machines (e.g. when migrating between servers, particularly for images that may contain sensitive data such as security credentials). While it&amp;rsquo;s super convenient to have a containerized version of the Docker registry ready for action, it doesn&amp;rsquo;t do much good without putting it behind an HTTPS server (otherwise we have to restart our entire docker service with the insecure flag to permit communication with an unauthenticated registry &amp;ndash; doesn&amp;rsquo;t sound like a good idea).</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/19/coreos-and-other-infrastructure-notes/</link>
      <pubDate>Wed, 19 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/19/coreos-and-other-infrastructure-notes/</guid>
      <description>CoreOS? Security model looks excellent. Some things not so clear:
 In a single node setup, what happens with updates? Would containers being run directly come down and not go back up automatically? In general, how effective or troublesome is it to run a single, low-demand app on a single node CoreOS rather than, say, an ubuntu image (e.g. just to benefit from the security updates model)? For instance, would an update cause a running app to exit in this scenario?</description>
    </item>
    
    <item>
      <title>Goodbye Jekyll?</title>
      <link>/2014/10/28/jekyll-free/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/28/jekyll-free/</guid>
      <description>The great strength of Jekyll is in providing a really convenient HTML templating system through the _layouts and _includes directories and Liquid variables (including the auto-populated ones like page.previous.url).
For quickly deploying simple sites though, this is often unnecessary: one or two layout files will suffice, and an _includes directory is not so useful with only a single layout. The ease of maintenance by having a template divided into modular chunks is somewhat trumped by the greater simplicity of copying a single template or set of templates over into a new directory.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/20/notes/</link>
      <pubDate>Mon, 20 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/20/notes/</guid>
      <description>Keep thinking about this quote from Jeroen Oom&amp;rsquo;s recent piece on the arxiv:
 The role and shape of data is the main characteristic that distinguishes scientific computing. In most general purpose programming languages, data structures are instances of classes with well-defined fields and methods. [&amp;hellip;] Strictly defined structures make it possible to write code implementing all required operations in advance without knowing the actual content of the data. It also creates a clear separation between developers and users [emphasis added].</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/16/gitlab-and-other-configuration-notes/</link>
      <pubDate>Thu, 16 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/16/gitlab-and-other-configuration-notes/</guid>
      <description>Updating gitlab setup:
docker pull sameersbn/redis:latest docker pull sameersbn/gitlab:7.3.2-1 docker pull sameersbn/postgresql:latest mkdir -p /opt/gitlab/data mkdir -p /opt/postgresql/data docker run --name=postgresql -d \ -e &#39;DB_NAME=gitlabhq_production&#39; -e &#39;DB_USER=gitlab&#39; -e &#39;DB_PASS=password&#39; \ -v /opt/postgresql/data:/var/lib/postgresql \ sameersbn/postgresql:latest docker run --name=redis -d sameersbn/redis:latest docker run --name=gitlab -d \ --link postgresql:postgresql \ --link redis:redisio \ -p 10080:80 -p 10022:22 \ -v /opt/gitlab/data:/home/git/data \ sameersbn/gitlab:7.3.2-1  More consisely, do this with fig
gitlab: image: sameersbn/gitlab:7.3.2-1 links: - postgres - redis:redisio ports: - &amp;quot;10080:80&amp;quot; - &amp;quot;10022:22&amp;quot; volumes: - /opt/gitlab/data:/home/git/data environment: - SMTP_USER=USER@gmail.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/09/22/containerizing-my-development-environment/</link>
      <pubDate>Mon, 22 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/09/22/containerizing-my-development-environment/</guid>
      <description>A key challenge for reproducible research is developing solutions that integrate easily into a researcher&amp;rsquo;s existing workflow. Having to move all of one&amp;rsquo;s development onto remote machines, into a particular piece of workflow software or IDE, or even just constrained to a window running a local virtual machine in an unfamiliar or primitive environment isn&amp;rsquo;t particularly appealing. In my experience this doesn&amp;rsquo;t reflect the workflow of even those folks already concerned about reproducibility, and is, I suspect, a major barrier in adoption of such tools.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/09/09/server-backups/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/09/09/server-backups/</guid>
      <description>Digital Ocean Snapshots At $0.02 per gig per month, this looks like this is the cheapest way to make complete backups.
The process is rather manual: we have to sudo poweroff the droplet and then trigger the snapshot (the container will come back online after that, though we have to restart the services / active docker containers). We also have to delete old snapshots manually. Some of this can be automated from the API.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/09/08/server-security-basics/</link>
      <pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/09/08/server-security-basics/</guid>
      <description>Security configuration We set up SSH key-only login on non-standard port, with root login forbidden. We then set up ufw firewall, fail2ban, and tripwire.
 Configure an SSH key login. Next, Create a user, add to sudoers, and then disable root login.. Edits /etc/ssh/sshd_config:
 Disabling root logins. (We&amp;rsquo;ll need to add ourselves to sudo first: (adduser, edit /etc/sudoers) Change ssh port from default to something else. Whitelist user login ids   Additionally, let&amp;rsquo;s be sure to disable password authentication: Add PasswordAuthentication no to /etc/ssh/sshd_config.</description>
    </item>
    
  </channel>
</rss>