<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computation on Boettiger Group</title>
    <link>/categories/computation/</link>
    <description>Recent content in Computation on Boettiger Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/computation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/2014/12/29/steps-to-a-more-portable-workflow/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/12/29/steps-to-a-more-portable-workflow/</guid>
      <description>Tweaking Dockerfile for labnotebook to be simpler and more portable.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/12/11/rrhack-notes/</link>
      <pubDate>Thu, 11 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/12/11/rrhack-notes/</guid>
      <description>Misc notes from #rrhack. Much more detail under our Github organization, including the issues tracker &amp;amp; wiki for the meeting and the start of the repositories for the various teaching modules.
Also see rrhack slack forum and twitter hashtag.
Key questions  Audience and motivation. Reproducibility isn&amp;rsquo;t a goal, it&amp;rsquo;s a means to an ends. Accelerating science is the goal. Motivation should be to accelerate &amp;amp; scale your own science.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/12/07/self-destroy-droplet-on-completion/</link>
      <pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/12/07/self-destroy-droplet-on-completion/</guid>
      <description>Scott&amp;rsquo;s analogsea package provides a great way to script commands for cloud instances on the digitalocean platform. for instance, we can use analogsea to automatically start an instance of the desired size, submit a computationally intensive task, and then terminate the instance when the task completes successfully. This last step is particularly convenient since it makes it easier to use a very powerful (and thus expensive) instance for a short time, knowing it will terminate and avoid extra charges while idle.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/17/wssspe-feedback/</link>
      <pubDate>Mon, 17 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/17/wssspe-feedback/</guid>
      <description>WSSSPE working groups: Reproducibility, Reuse, and Sharing (Neil Che Hong) Our group focused on journal policies regarding software papers. Our objectives were:
 A Survey of journals that publish software papers. The Software Sustainability Institute already maintains a list
 A summary of policies each of these journals has in place regarding software papers. (e.g. licensing requirements, repository requirements, required sections in the manuscripts regarding installation or tests, etc).</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/13/draft-wssspe-thoughts/</link>
      <pubDate>Thu, 13 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/13/draft-wssspe-thoughts/</guid>
      <description>Scratching out notes for lightning talk at WSSSPE&amp;hellip;
(CB from ropensci, Karthik too. )
 Responses to the challenges of academic software can be divided into roughly two categories. Give them better tools! Give them better training!
We do software: over 50 packages for accessing data repositories, integrating and manipulating, annotating and publishing data. We do training: workshops, online tutorials, developing course materials. But most importantly, we aim to build community.</description>
    </item>
    
    <item>
      <title>Dear DockerHub users: please configure your repository links</title>
      <link>/2014/11/07/dear-docker-hub-users/</link>
      <pubDate>Fri, 07 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/07/dear-docker-hub-users/</guid>
      <description>The DockerHub is a great resource for discovering and distributing Dockerfiles. Many users sharing public images take advantage of the Docker Hub&amp;rsquo;s Automated Build configuration, which is excellent as this automatically allows the Hub to display the Dockerfile and provides some medium of security above simply downloading and running some untrusted binary black box.
Unfortunately, far fewer users configure Repository Links to trigger builds to update even when the resulting Dockerfile is unchanged.</description>
    </item>
    
    <item>
      <title>linking binaries from other containers</title>
      <link>/2014/11/05/notes/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/05/notes/</guid>
      <description>Been thinking about this for a while, but @benmarwick &amp;rsquo;s examples with --volumes-from convinced me to give this a try.
While there&amp;rsquo;s an obvious level of convenience in having something like LaTeX bundled into the hadleyverse container so that users can build nice pdfs, if often feels not very docker-esque to me to just throw the kitchen sink into a container. At the risk of some added complexity, we can provide LaTeX from a dedicated TeX container to a container that doesn&amp;rsquo;t have it built in, like rocker/rstudio.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/11/03/three-interfaces-for-docker/</link>
      <pubDate>Mon, 03 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/03/three-interfaces-for-docker/</guid>
      <description>Here I outline three broad, different strategies for incorporating Docker into a user&amp;rsquo;s workflow, particularly from the perspective of an instructor getting a group of students up and running in a containerized environment, but also in the context of more generic collaborations. The options require progressively more setup and result in a progressively more &amp;lsquo;native&amp;rsquo; feel to running Docker. My emphasis is on running Dockerized R applications and RStudio, though much the same thing can be accomplished with iPython notebooks and many other web apps.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/29/the-viability-of-dockerized-research/</link>
      <pubDate>Wed, 29 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/29/the-viability-of-dockerized-research/</guid>
      <description>In the spirit of Greg&amp;rsquo;s post &amp;amp; the discussion here, I&amp;rsquo;d be interested in hearing more on people&amp;rsquo;s thoughts about the viability of Dockerized software both in the teaching classroom and beyond, as students continue in their research.
To start off, I thought I&amp;rsquo;d share a few of my own impressions, largely to give a target for anyone to push back against. In doing so I will also try to outline what I see as the features and potential workflow would look like, in hopes that it does not require any actual experience with docker to have an opinion on whether or not a particular strategy makes sense pedagogically.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/21/docker-and-user-permissions-crazyness/</link>
      <pubDate>Tue, 21 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/21/docker-and-user-permissions-crazyness/</guid>
      <description>Lots of crazyness getting to the bottom of permissions changes, as discussed in:
 rocker issues tracker Stackoverflow question Docker mailing list  Long story short: docker cares only about UIDs, so we have to explicitly make sure these match. Some very good answers including from Docker core-team members on the discussion list. Overall approach outlined at the end of the rocker issues tracker.
Here&amp;rsquo;s the SO version of the question, for my reference:</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/09/lessons-learned-in-writing-dockerfiles/</link>
      <pubDate>Thu, 09 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/09/lessons-learned-in-writing-dockerfiles/</guid>
      <description>Writing dockerfiles is pretty straight forward. Nevertheless, a little extra care goes a long way. Docker&amp;rsquo;s own Best Practices are a great starting point, covering everything from formatting to use of certain commands. In Rocker, We&amp;rsquo;ve tried to follow all of these suggestions and have found them very helpful. In particular:
 Minimize the number of layers, but use \ to break commands across multiple lines,
 Always run apt-get update &amp;amp;&amp;amp; apt-get install -y .</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/07/notes/</link>
      <pubDate>Tue, 07 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/07/notes/</guid>
      <description>Misc Docker Notes  funny business with Locales, see #19  Misc notes on littler  INSTALL page is outdated. Instead, do:  apt-get update \ &amp;amp;&amp;amp; apt-get build-dep -y littler \ &amp;amp;&amp;amp; apt-get install autoconf git \ &amp;amp;&amp;amp; git clone https://github.com/eddelbuettel/littler.git \ &amp;amp;&amp;amp; /littler/./bootstrap  May want to symlink too:
ln -s /littler/examples/install.r /usr/local/bin/install.r \ &amp;amp;&amp;amp; ln -s /littler/examples/install2.r /usr/local/bin/install2.r \ &amp;amp;&amp;amp; ln -s /littler/examples/installGithub.r /usr/local/bin/installGithub.r \ &amp;amp;&amp;amp; ln -s /littler/examples/testInstalled.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/10/02/notes/</link>
      <pubDate>Thu, 02 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/10/02/notes/</guid>
      <description>docker 2014-09-29:
 Discussion with Dirk on repositories, library paths, versions. library paths: apt-get users the usr/lib path, while user-run install commands (e.g. install.packages) uses usr/local/lib/, path. Dirk recommends that /usr/local/lib/R/site-library is configured to be user-writable for package installation, rather than installing into home. building directly from CRAN building dependencies: apt-get build-dep, needs the corresponding deb-src lines. issues and tweaks to littler see PR #2  2014-10-01:
 Discussion on minimal images Discussion on analogsea + docker Blog coverage of Dirk&amp;rsquo;s talk on our Docker work.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/09/24/notes/</link>
      <pubDate>Wed, 24 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/09/24/notes/</guid>
      <description>rocker / docker  Talking over strategy with Dirk, see summary in #1 and follow-up issues, #3, #4, #5.  rdataone &amp;amp; EML Trying to fix travis issues. Much craziness.
travis.sh doesn&amp;rsquo;t provide notes on setup for repos where the R package is in a subdirectory. Looks like cd commands are persistent though throughout a travis file. okay.
 dataone is imported by EML but suggested by dataone. Since install_github likes to install the suggests list, this creates problems:</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/09/05/drone-ci-and-docker/</link>
      <pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/09/05/drone-ci-and-docker/</guid>
      <description>Drone CI: Continous integration in custom docker environments Having gotten accustomed to Docker, configuring the appropriate build environment for a Continuous Integration system like Travis CI or Shippable CI starts to feel incredibly tedious and archaic (particularly if you work primarily in a language like R or haskell that usually isn&amp;rsquo;t supported out of the box).
 We do not have to hack together a custom image environment We can build and test our environment locally instead of having to rely on trial-and-error pushes to the CI server We do not have to download, compile and install the development environment each time, (which frequently takes longer than the CI checks themselves and can break)  (Shippable provides a persistent environment too, by preserving the state of your &amp;lsquo;minion&amp;rsquo;.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2014/05/02/scientific-computing-notes/</link>
      <pubDate>Fri, 02 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/05/02/scientific-computing-notes/</guid>
      <description>In my experience EC2 is good for some things but not for others. Consequently having some funding allocated for it would be great, but it would still be necessary to have other resources as well. I&amp;rsquo;ve found EC2 very good for running some reasonably portable analysis where you temporarily want some extra processors or memory. On the other hand, I&amp;rsquo;ve had some frustrations with it as well. You don&amp;rsquo;t have a persistent development environment unless you explicitly make and maintain a machine image, which means installing any software dependencies from scratch; sometimes a particular nuisance when you aren&amp;rsquo;t familiar with the architecture.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/09/11/isees-workforce-development-day-2/</link>
      <pubDate>Wed, 11 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/09/11/isees-workforce-development-day-2/</guid>
      <description>Near the end of another excellent day of discussions we were asked to put pen to paper to outline our own views in writing. My own contribution was the following notebook scribble.
My own vision of what ISEES should be doing for workforce development:
ISEES would employ a multi-scale approach to improve basic computational expertise across the entire workforce necessary to fully participate in the modern software-immersed world of research through the development of training standards, while using a working-group model focused on emerging researcher-developer communities which would simultaneously improve the quality and synthesis of existing software tools while also delivering essential skills to the most transformative members of the research software communities.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/09/11/extending-data-frame-class/</link>
      <pubDate>Wed, 11 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/09/11/extending-data-frame-class/</guid>
      <description>I&amp;rsquo;d like to define a class that acts just like a data.frame, just like the data.table class does, but contains some additional metadata (e.g. the units associated with the columns) and has some additional methods associated with it (e.g. that might do something with those units) while also working with any function that simply knows how to handle data.frame objects.
How might this be done?
I&amp;rsquo;m not really sure where to start on this, so below is a summary of my attempt so far and some further puzzles I have stumbled across (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/09/10/isees-training-workshop-day-1/</link>
      <pubDate>Tue, 10 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/09/10/isees-training-workshop-day-1/</guid>
      <description>Education / Workforce Development Group Cannot do grand synthesis opaquely and be science. must be able to scale and evolve.
How did data succeed? What features really made the difference and what were just sideshows?
 Sticks (Funder and journal mandates?) Working groups (NCEAS) Training external forces: there is more data.
  Why one center? Why not 15 centers? Divided by discipline rather than region&amp;hellip;. synthesize the synthesis centers</description>
    </item>
    
    <item>
      <title></title>
      <link>/2012/07/27/csgf-conference/</link>
      <pubDate>Fri, 27 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>/2012/07/27/csgf-conference/</guid>
      <description>Conference Agenda Alumni-organized Plus-One Day Agenda Great chance to talk with US Secretary of the Department of Energy Stephen Chu Interesting leadership training session lead by improv acting professional Raquel Holmes of ImprovScience
  Video recording of my talk  Slides:</description>
    </item>
    
    <item>
      <title>Jordan&#39;s bigdata talk in Math/Stats series</title>
      <link>/2012/04/05/jordans-bigdata-talk-in-mathstats-series/</link>
      <pubDate>Thu, 05 Apr 2012 15:48:44 +0000</pubDate>
      
      <guid>/2012/04/05/jordans-bigdata-talk-in-mathstats-series/</guid>
      <description>Really excellent talk by Professor Michael Jordan from the Berkeley Statistics department visiting us at Davis yesterday. Haven&amp;rsquo;t seen a talk cover technical content with remarkable clarity for the algorithms and insights involved, while also covering such a breadth of material. My notes didn&amp;rsquo;t really keep up, but transcribing some of my scribbling into electronic form will have to wait until I have more time. (Their pubs are probably a better reference anyway.</description>
    </item>
    
    <item>
      <title>Relevant initiatives from today&#39;s Big Data press release</title>
      <link>/2012/03/29/relevant-initiatives-from-todays-big-data-press-release/</link>
      <pubDate>Thu, 29 Mar 2012 22:01:54 +0000</pubDate>
      
      <guid>/2012/03/29/relevant-initiatives-from-todays-big-data-press-release/</guid>
      <description>An exciting day for Big Data (particularly if your Berkeley).
NSF  Encouraging research universities to develop interdisciplinary graduate programs to prepare the next generation of data scientists and engineers; Funding a $10 million Expeditions in Computing project based at the University of California, Berkeley, that will integrate three powerful approaches for turning data into information - machine learning, cloud computing, and crowd sourcing; Providing the first round of grants to support “EarthCube” - a system that will allow geoscientists to access, analyze and share information about our planet; Convening researchers across disciplines to determine how Big Data can transform teaching and learning.</description>
    </item>
    
    <item>
      <title>Tuesday: pandoc citations notes, various</title>
      <link>/2012/03/27/tuesday-pandoc-citations-notes-various/</link>
      <pubDate>Tue, 27 Mar 2012 22:22:14 +0000</pubDate>
      
      <guid>/2012/03/27/tuesday-pandoc-citations-notes-various/</guid>
      <description>Markdown over latex? knitr may have solved the fundamental challenge of all Sweave/latex users with Markdown. Too many of us must be able to work with collaborators who don&amp;rsquo;t know LaTeX and journals that won&amp;rsquo;t accept LaTeX. It&amp;rsquo;s a sad state of the affairs, and has improved slowly for journals (at least many will take pdfs for initial submissions) if not for collaborators. Consequently getting away from LaTeX would be nice, but Word or Google docs just won&amp;rsquo;t cut it.</description>
    </item>
    
    <item>
      <title>Citations in markdown using knitr</title>
      <link>/2012/03/24/citations-in-markdown-using-knitr/</link>
      <pubDate>Sat, 24 Mar 2012 21:33:27 +0000</pubDate>
      
      <guid>/2012/03/24/citations-in-markdown-using-knitr/</guid>
      <description>I am finding myself more and more drawn to markdown rather then tex/Rnw as my standard format (not least of which is the ease of displaying the files on github, particularly now that we have automatic image uploading). One thing I miss from latex is the citation commands. (I understand these can be provided to markdown via Pandoc, but I&amp;rsquo;d like to simply have to knit the document, and not then run it through pandoc, latex, or another interpreter).</description>
    </item>
    
    <item>
      <title>Friday: some random notes</title>
      <link>/2012/03/20/friday-some-random-notes/</link>
      <pubDate>Tue, 20 Mar 2012 16:53:08 +0000</pubDate>
      
      <guid>/2012/03/20/friday-some-random-notes/</guid>
      <description>Computational tricks  tables in github markdown via knitr. clever Getting custom C/C++ CUDA working in R  Data management and sharing solutions for large files  NERSC uses globus online for interface for transferring terabytes/hr, pretty clever. NERSC also enables web hosting to share data.
 Software-Carpentry style Data management a nice start. An open solution could stick the data on figshare (or elsewhere), write the readmes in markdown and keep them on github with links to the data.</description>
    </item>
    
    <item>
      <title>Elegant &amp; fast data manipulation with data.table</title>
      <link>/2012/02/12/elegant-fast-data-manipulation-with-data-table/</link>
      <pubDate>Sun, 12 Feb 2012 13:39:03 +0000</pubDate>
      
      <guid>/2012/02/12/elegant-fast-data-manipulation-with-data-table/</guid>
      <description>Just learned about the R data.table package (ht @recology_) makes R data frames into ultra-fast, SQL-like objects.
One thing we get is some very nice and powerful syntax. Consider some simple data of replicate time series:
time &amp;lt;- rep(1:10, 10) replicate &amp;lt;- sort(time) value &amp;lt;- rnorm(100) df &amp;lt;- data.frame(replicate, time, value)  To apply a function to each set of replicates, instead of
sapply(1:max(df$replicate), function(i) mean( df[df$replicate == i,]$value) )  We can use</description>
    </item>
    
    <item>
      <title>Monday - Sweave workflow</title>
      <link>/2012/01/09/monday-13/</link>
      <pubDate>Mon, 09 Jan 2012 22:09:03 +0000</pubDate>
      
      <guid>/2012/01/09/monday-13/</guid>
      <description>Bibtex, Mendeley &amp;amp; Sweave Mendeley&amp;rsquo;s repeated entries in the bibfile is particularly annoying with my sweave workflow, as it throws an error from bibtex command that Make doesn&amp;rsquo;t want to ignore.
Also, in writing the documentation I&amp;rsquo;d like to point the .Rnw file to my global library&amp;rsquo;s bibtex document, but then this means having the .Rnw/tex file generated to by the package pointing to a file that&amp;rsquo;s outside the package.</description>
    </item>
    
    <item>
      <title>Better dynamic documents (Sweave) with syntax highlighting, caching, etc</title>
      <link>/2011/12/12/better-sweave-dynamic-documents-with-syntax-highlighting-easier-tools/</link>
      <pubDate>Mon, 12 Dec 2011 17:26:47 +0000</pubDate>
      
      <guid>/2011/12/12/better-sweave-dynamic-documents-with-syntax-highlighting-easier-tools/</guid>
      <description>The highlight package is a simple solution for very nice syntax highlighted code boxes in latex documents. Requires switching the driver, which is best done from within R and requires creating a makefile though. Needs the &amp;ldquo;highlight&amp;rdquo; package installed. Here&amp;rsquo;s a simple makefile.
A wealth/mess of Sweave related packages on the CRAN taskview for Reproducible Research (now what other software platform has a the equivalent of a Reproducible Research task view?</description>
    </item>
    
    <item>
      <title>Friday: wrightscape, ggplot, wp memory</title>
      <link>/2011/11/25/friday-wrightscape-ggplot-wp-memory/</link>
      <pubDate>Fri, 25 Nov 2011 20:32:59 +0000</pubDate>
      
      <guid>/2011/11/25/friday-wrightscape-ggplot-wp-memory/</guid>
      <description>more sub-model comparisons wrightscape sub-models:
 a1 independent alphas, global theta, sigmas
 bm &amp;ldquo;brownie&amp;rdquo; (alpha = 0, indep sigmas)
 a2 independent alpha, theta, global sigma
 s1 indep sigma, global alpha, theta
 s2 indep sigma, theta, global alpha
  Basic Nelder Mead Simulated annealing associated parameter estimates
This approach is really not finding a very robust solution.
Plotting with stats_summary() Using stats_summary instead of melt and cast computations and line and ribbon geometries for adding statistical layers to plots.</description>
    </item>
    
    <item>
      <title>some configuration notes: RStudio setup, syntax highlighting in sweave</title>
      <link>/2011/11/22/some-configuration-notes-rstudio-setup-syntax-highlighting-in-sweave/</link>
      <pubDate>Tue, 22 Nov 2011 18:51:19 +0000</pubDate>
      
      <guid>/2011/11/22/some-configuration-notes-rstudio-setup-syntax-highlighting-in-sweave/</guid>
      <description>Installing R-Studio on my dreamhost VPS server. Dreamhost runs debian lenny, which is a bit dated, making it necessary to install from source. (Would be easy on a modern ubuntu server). Here we go.
First, we probably want to install R from source to begin with (Lenny comes with cutting-edge R 2.7.1&amp;hellip;). This needs to be done with shared libs enabled. Also I needed to run it without recommended-packages to get the initial install working</description>
    </item>
    
    <item>
      <title>Parallel hpc on clusters in R, MPI</title>
      <link>/2011/11/10/parallel-computing-on-clusters-in-r-mpi/</link>
      <pubDate>Thu, 10 Nov 2011 16:49:17 +0000</pubDate>
      
      <guid>/2011/11/10/parallel-computing-on-clusters-in-r-mpi/</guid>
      <description>Some notes for reference on parallel computing strategies on R with dedicated high performance clusters.
parallelization in R For cluster computing in R, I&amp;rsquo;ve found the &amp;ldquo;snow&amp;rdquo; package with MPI to be the most flexible and robust solution. snowfall is a useful package to use multiple cores on a single processor, but requesting all 16 threads on a node is a much more difficult demand for the queue to fill than 16 threads on all nodes.</description>
    </item>
    
    <item>
      <title>Sunday</title>
      <link>/2011/11/06/sunday-5/</link>
      <pubDate>Sun, 06 Nov 2011 14:38:18 +0000</pubDate>
      
      <guid>/2011/11/06/sunday-5/</guid>
      <description>Computing notes A few configuration related things from migrating to the new laptop.
Clipboard and Vim vim clipboard - install vim-gnome &amp;ldquo;+y and &amp;ldquo;+p interact between vim and global clipboard, and y, shift+insert work within terminal.
make a repository bare so it can be pushed to (as opposed to moving to it&amp;rsquo;s working directory and running a pull). From stackoverflow:
mv repo/.git repo.git; rm -rf repo cd repo.git git config --bool core.</description>
    </item>
    
    <item>
      <title>Sparkleshare configuration</title>
      <link>/2011/11/04/sparkleshare-configuration/</link>
      <pubDate>Fri, 04 Nov 2011 22:35:11 +0000</pubDate>
      
      <guid>/2011/11/04/sparkleshare-configuration/</guid>
      <description>Configured sparkleshare across my linux server &amp;amp; laptops, and android phone. Provides a dropbox-style syncing and version history for all files using git. I can sync as much space as I have hardisk capacity, with as many users and machines as I like, at no cost. Wonderful.
Basic setup Get sparkleshare working on a server is dead simple, Can also use a github server, gitorious server, etc. Setup is clearly documented, which is really just doing a bunch of standard stuff if you have a server or github already set up.</description>
    </item>
    
    <item>
      <title>server file management from bowser: ajaxplorer, mollify</title>
      <link>/2011/11/01/server-file-management-from-bowser-ajaxplorer-mollify/</link>
      <pubDate>Tue, 01 Nov 2011 20:18:38 +0000</pubDate>
      
      <guid>/2011/11/01/server-file-management-from-bowser-ajaxplorer-mollify/</guid>
      <description>A few notes on trying out some server-based file sharing solutions.
ajaXplorer Kind of amazingly good and easy to install, with configuration-free functionality like previews of images and playing mp3s. More secure file sharing (at least, public link not generated until asked for, can have password assigned to it). Sharing is the only step not activated by default. Create a folder called &amp;ldquo;public&amp;rdquo; in the ajaxplorer directory, and this feature will appear.</description>
    </item>
    
    <item>
      <title>Optimal Control Exploration</title>
      <link>/2011/10/19/optimal-control-exploration/</link>
      <pubDate>Wed, 19 Oct 2011 23:01:10 +0000</pubDate>
      
      <guid>/2011/10/19/optimal-control-exploration/</guid>
      <description>Translating Jim&amp;rsquo;s Matlab code.
Adapted for Octave.
R version.
Document review:
 Chapter 04_Sanchirico_Final.pdf
 SanchiricoOptimalSpace_JEEM.pdf (Sanchirico &amp;amp; Wilen, 2005)
 Optimal_Rebuilding_AJAE2010.pdf, Optimal_Rebuilding_Supp_Material.pdf (Sanchirico et. al. 2010)
 Sanchirico_editsembedded.pdf: Book Chapter 1: Economically Optimal Management of a Metapopulation
  Reviewing Bett&amp;rsquo;s Practical Methods for Optimal Control using Nonlinear Programming. (( Practical Methods for Optimal Control and Estimation Using Nonlinear Programming, Second Edition (Advances in Design and Control) John T.</description>
    </item>
    
    <item>
      <title>Algorithms Group: Literate programming</title>
      <link>/2011/10/17/algorithms-group-literate-programming/</link>
      <pubDate>Mon, 17 Oct 2011 18:41:42 +0000</pubDate>
      
      <guid>/2011/10/17/algorithms-group-literate-programming/</guid>
      <description>Algorithms group today took a brake from discussing algorithms for an aside on coding practices. Today we focused primarily on literate programming tools for making better documented, more readable code. We covered:
 Sweave
 Google style guide, also see R core-team developer Hadley Wickam&amp;rsquo;sstyle guide (modified from Google)
 R coding practices grab-bag: using functionalized code, {&amp;hellip;}, S3 vs S4 classes.
 roxygen, roxygen2
  Next time: Github, then we&amp;rsquo;ll spend 30 minutes collaboratively adding roxygen documentation to our existing codes and committing changes over git.</description>
    </item>
    
    <item>
      <title>rfishbase Tutorial - updated &amp; extended</title>
      <link>/2011/10/12/rfishbase-tutorial-updated-extended/</link>
      <pubDate>Wed, 12 Oct 2011 11:59:49 +0000</pubDate>
      
      <guid>/2011/10/12/rfishbase-tutorial-updated-extended/</guid>
      <description>Updated rfishbase package. Include querying for a list of scientific names and the addition of more quantitative traits, though the selection is still somewhat limited. Demo includes how to grab some data to match a phylogenetic tree, and some regular expression (grep) searches for feeding behavior.
Source code
Windows binary also available.
# demo.R rm(list=ls()) require(rfishbase) ###### Grabbing, Loading &amp;amp; Caching Fishbase Data ###### ## Download and parse data for first 40 ids (36 fish) #fish.</description>
    </item>
    
    <item>
      <title>Algorithms Group, HMM meets EM</title>
      <link>/2011/10/10/algorithms-group-hmm-meets-em/</link>
      <pubDate>Mon, 10 Oct 2011 18:42:03 +0000</pubDate>
      
      <guid>/2011/10/10/algorithms-group-hmm-meets-em/</guid>
      <description>Alisa reviews HMM, EM, and shows us how to combine them.
The EM version treats the transition probabilities (exchanging coins) and emission probabilities (chance of heads/tails under each coin) as unknowns. Make a guess, calculate the blue line from before. This can be used to calculate a new guess that will hill climb to the locally optimum solution (EM).
New guess for the transition probability is:
$$ A_{lk} = \frac{f_i bi a{lk} ek(x{i+1}) }{P(x)} $$</description>
    </item>
    
    <item>
      <title>Friday, wordpress backups, pmc edits</title>
      <link>/2011/09/30/friday-wordpress-backups-pmc-edits/</link>
      <pubDate>Fri, 30 Sep 2011 17:51:31 +0000</pubDate>
      
      <guid>/2011/09/30/friday-wordpress-backups-pmc-edits/</guid>
      <description>Wordpress local copy  configured mysqldump backup, following dreamhost wiki (added cron job manually instead of in panel).  edit local wp-config.php to use localhost. Get the database name, user name and password from the lines above it, which are used later.
define(&#39;DB_HOST&#39;, &#39;localhost&#39;); // ...and the server MySQL is running on  create user: log in as root
mysql -u root -p  and create user
mysql&amp;gt; create database database_name mysql&amp;gt; create user &#39;username&#39;@&#39;localhost&#39; identified by &#39;password&#39;; mysql&amp;gt; create database database_name mysql&amp;gt; grant usage on *.</description>
    </item>
    
    <item>
      <title>memory management, cron</title>
      <link>/2011/09/07/memory-management-cron/</link>
      <pubDate>Wed, 07 Sep 2011 21:28:03 +0000</pubDate>
      
      <guid>/2011/09/07/memory-management-cron/</guid>
      <description>Create the memcheck.sh script that will write a mem.log file with the data, and agnuplot script that will plot it.
[gist id=1201994]
Then set up a cron job. First, set the editor to something functional:
export EDITOR=vi  then, crontab -e to edit the cron table. Add this line to run the command every 2 minutes (see wikipedia entry for quick intro), and plot it every 15 minutes:
*/2 * * * * /home/cboettig/.</description>
    </item>
    
    <item>
      <title>Algorithms group: Hidden Markov Models</title>
      <link>/2011/08/29/algorithms-group-hidden-markov-models/</link>
      <pubDate>Mon, 29 Aug 2011 21:43:42 +0000</pubDate>
      
      <guid>/2011/08/29/algorithms-group-hidden-markov-models/</guid>
      <description>Great group discussion on Hidden Markov Models tag-teamed by Yaniv and Alisa. Walked through the coin flip example, creating the forward algorithm, and the posterior decoding. In this case it is assumed model and parameters are known: two coins, known ratios, known rate of swapping; goal is to guess which coin is being used in a given interval. Algorithm does quite well, as shown in Yaniv&amp;rsquo;s awesome graph.
As usual in the notebook, click on the graph to get link-through to the code.</description>
    </item>
    
    <item>
      <title>Monday: scaling wrightscape examples</title>
      <link>/2011/08/29/monday-scaling-wrightscape-examples/</link>
      <pubDate>Mon, 29 Aug 2011 18:27:20 +0000</pubDate>
      
      <guid>/2011/08/29/monday-scaling-wrightscape-examples/</guid>
      <description>syntaxHighligher isn&amp;rsquo;t working. no idea why, posted to forums.
 Still no luck compiling gsl on carver. Can&amp;rsquo;t install gsl R package or compile wrightscape C code. posted C issue to NERSC, R issue to statscicomp.
   Commits to taxize pushed to Scott.
wrightscape parrotfish example Convergence test: with simple (common) initial values (alpha=.1, sigma=1), the different methods behave mostly appropriately: outer nested models out-perform inner ones, and likelihoods roughly agree between ouch package methods and wrightscape equivalents:</description>
    </item>
    
    <item>
      <title>Sunday: scaling, runs</title>
      <link>/2011/08/28/sunday-scaling-runs/</link>
      <pubDate>Sun, 28 Aug 2011 22:33:59 +0000</pubDate>
      
      <guid>/2011/08/28/sunday-scaling-runs/</guid>
      <description>laptop: ubuntu upgrades: 10.04 LTS -&amp;gt; 10.10 -&amp;gt; 11.04, geesh.
Set up for running primates.R in likelihood mode with abstracted parallelization:
Carver trouble getting wrightscape installed: even after module load gsl, cannot find gsl library. Testing on zero in mpi mode, 16. Testing on farm, mpi mode with snow, 16 cores.
hmm, high (9%) memory usage on primates.R on zero&amp;hellip; monitoring situation closely&amp;hellip; Only .3% on farm so far, which has 24G instead of zero&amp;rsquo;s 32Gig RAM.</description>
    </item>
    
    <item>
      <title>Saturday: working on computational scaling, ion, etc</title>
      <link>/2011/08/27/saturday-working-on-computational-scaling-abstraction-etc/</link>
      <pubDate>Sat, 27 Aug 2011 23:44:09 +0000</pubDate>
      
      <guid>/2011/08/27/saturday-working-on-computational-scaling-abstraction-etc/</guid>
      <description>Can I get a generic montecarlotest function working for both warningsignals and pmc?
Needs to be able to handle S4 classes as well as S3 classes, or need to define the methods update, simulate, loglik and getParameters for any object that would use the method. i.e. these need to be defined in warningsignals for its functions and in wrightscape/pmc for its creatures. then the generic can be used. Implementing now&amp;hellip; Completed.</description>
    </item>
    
    <item>
      <title>rropensci and some taxonomy in R with taxize</title>
      <link>/2011/08/26/rfishbase-and-some-taxonomy-in-r-with-taxize/</link>
      <pubDate>Fri, 26 Aug 2011 23:16:22 +0000</pubDate>
      
      <guid>/2011/08/26/rfishbase-and-some-taxonomy-in-r-with-taxize/</guid>
      <description>rfishbase Went through demo of rfishbase with Tomomi. Improved error handling, added a few data types and a few use cases.
I always forget that I have to drop nulls by indexing, not my return values of sapply.
x[!sapply(x, is.null)]  Very annoyed with fishbase id numbers (being discontinuous listings and unable to query xml by anything more intelligent). Querying all fishbase ids 1:30000, I get only 999 fish. hmm.</description>
    </item>
    
    <item>
      <title>FishBASE from R: some XML parsing</title>
      <link>/2011/08/26/fishbase-from-r-some-xml-parsing/</link>
      <pubDate>Fri, 26 Aug 2011 10:51:29 +0000</pubDate>
      
      <guid>/2011/08/26/fishbase-from-r-some-xml-parsing/</guid>
      <description>cross-posted from Wainwright Lab blog, archiving in the notebook here. This early tutorial includes some background on XML parsing from R using XPath. See the later rfishbase tutorial for more functionality.
In lab known for its quality data collection, high-speed video style, writing the weekly blog post can be a bit of a challenge for the local code monkey. That&amp;rsquo;s right, no videos today. But lucky for me, even this group can still make good use of publicly available data.</description>
    </item>
    
    <item>
      <title>Tuesday: rropensci, Rmpi, manuscripts, ...</title>
      <link>/2011/08/23/tuesday-rfishbase-rmpi-manuscripts/</link>
      <pubDate>Tue, 23 Aug 2011 15:10:24 +0000</pubDate>
      
      <guid>/2011/08/23/tuesday-rfishbase-rmpi-manuscripts/</guid>
      <description>rfishbase In answer to Tomomi&amp;rsquo;s question from last week, have basic functionality as an R package on github, rfishbase. Learned some slightly richer XML parsing in the process.
XML Notes Had to identify blocks by a child node that specifies the identity, and then find the sibling node that contains the content I needed. Goes something like this:
&amp;lt;dataObject&amp;gt; &amp;lt;dc:identifier&amp;gt;FB-Size-2&amp;lt;/dc:identifier&amp;gt; &amp;lt;dc:description&amp;gt; Text I need &amp;lt;/dc:description&amp;gt; &amp;lt;/dataObject&amp;gt;  Which I parsed in R as:</description>
    </item>
    
    <item>
      <title>Wednesday: migrating codes into MPI</title>
      <link>/2011/08/17/wednesday-migrating-codes-into-mpi/</link>
      <pubDate>Wed, 17 Aug 2011 22:53:33 +0000</pubDate>
      
      <guid>/2011/08/17/wednesday-migrating-codes-into-mpi/</guid>
      <description>Parellelization/Scaling of code MPI on farm cluster
Got my MPI codes running.
Much better way to get jobs into the queue, asking for 16 threads that don&amp;rsquo;t have to be on the same node is much faster. Can also ask for 161 threads, but will wait longer in the queue.
The trick to getting this to run was mostly getting the library set correctly. Setting the library path at the top of the script did the trick:</description>
    </item>
    
    <item>
      <title>Monday: Algorithms Discussion Group, EM finished</title>
      <link>/2011/07/25/monday-algorithms-discussion-group-em-finished/</link>
      <pubDate>Mon, 25 Jul 2011 17:58:55 +0000</pubDate>
      
      <guid>/2011/07/25/monday-algorithms-discussion-group-em-finished/</guid>
      <description>Finished up the EM algorithm today. Key was getting the right function to maximize. Turns out wikipedia has a very nice write up of this very example, but in our notation:
$$ E_p \log(p f_1) + (1-E_p) \log( (1-p) f_2 )$$
Where $E_p$ comes from the expectation step.
Jamie has joined us and has set up a github repository for the discussion group. Find the successful abstract algorithm there.</description>
    </item>
    
    <item>
      <title>CSGF Conference Notes</title>
      <link>/2011/07/24/csgf-conference-notes/</link>
      <pubDate>Sun, 24 Jul 2011 14:30:43 +0000</pubDate>
      
      <guid>/2011/07/24/csgf-conference-notes/</guid>
      <description>Just returned from 5 days in Washington, DC for the annual computational science graduate fellowship conference. No live notes/tweets, so just attempting to synthesize some of the major ideas before I get back to paper writing. Most of the conference material, including workshop slides, are archived online.
The high performance computing workshop made two things clear to me &amp;ndash; a lot of parallel computing can be done without lots of communication between nodes (i.</description>
    </item>
    
    <item>
      <title>CSGF 2010 Computing Practices Survey Summary</title>
      <link>/2011/07/24/csgf-2010-computing-practices-survey-summary/</link>
      <pubDate>Sun, 24 Jul 2011 14:11:07 +0000</pubDate>
      
      <guid>/2011/07/24/csgf-2010-computing-practices-survey-summary/</guid>
      <description>Thanks to everyone who participated in the CSGF 2010 Survey.With too many things going on this year, I didn&amp;rsquo;t conduct a survey at the 2011 Conference. But as people were asking about the survey, I thought I could at least make last year&amp;rsquo;s summary a bit more accessible. The following comes from the pdf report I submitted to Krell and a few example figures from data.This year&amp;rsquo;s survey was longer and broader, recieving over 90 responses from fellows and alumni.</description>
    </item>
    
    <item>
      <title>Segue: Easy cloud hpc in R, now with custom packages</title>
      <link>/2011/07/07/segue-easy-cloud-computing-in-r-now-with-custom-packages/</link>
      <pubDate>Thu, 07 Jul 2011 16:45:05 +0000</pubDate>
      
      <guid>/2011/07/07/segue-easy-cloud-computing-in-r-now-with-custom-packages/</guid>
      <description>After a few helpful emails from package creator JD Long, I have the segue package running with custom R packages. The package is available on Google code. With two lines of code I can start submitting jobs to very large clusters of computers on the Amazon cloud. For a basic introduction to the package see Jeff Breen&amp;rsquo;s post.
Quick notes on updating using mercurial: Since I&amp;rsquo;ve already pulled the code using</description>
    </item>
    
    <item>
      <title>EM Algorithm</title>
      <link>/2011/06/13/em-algorithm-2/</link>
      <pubDate>Mon, 13 Jun 2011 22:39:32 +0000</pubDate>
      
      <guid>/2011/06/13/em-algorithm-2/</guid>
      <description>Yaniv ran us through our second session on EM algorithms. We implemented a simple case described in this tutorial.
[gist id=1028113]
Code doesn&amp;rsquo;t reflect the abstraction of the algorithm into a proper Expectation step and Maximization step. We attempted this generalization:
[gist id=1028122]
Missed something in framing this correctly, since the maximization step includes a function that doesn&amp;rsquo;t depend on s[1]. Any ideas?</description>
    </item>
    
    <item>
      <title>Thursday Meetings: Duncan; Alan</title>
      <link>/2011/05/26/thursday-6/</link>
      <pubDate>Thu, 26 May 2011 23:53:05 +0000</pubDate>
      
      <guid>/2011/05/26/thursday-6/</guid>
      <description>Duncan Meeting  ROAuth/RCurl question. Mendeley is OAuth 1, github is OAuth 2 (simpler). Both are implemented in pure R in another package, may be good. OAuth 1 constructs complicated http urls with secret, and token, and random key, need to escape chars, etc. OAuth 2 is https, just uses token. Authentication options: Jeff&amp;rsquo;s ROAuth/Duncan&amp;rsquo;s mod, other OAuth package. RMendeley&amp;ndash; windows binary, competition? socialR &amp;ndash; discussed strategy to get a figure with matching code and data.</description>
    </item>
    
    <item>
      <title>Monday: some code-tricks, Algorithms group planning</title>
      <link>/2011/05/23/monday-5/</link>
      <pubDate>Mon, 23 May 2011 23:00:18 +0000</pubDate>
      
      <guid>/2011/05/23/monday-5/</guid>
      <description>Teary Some discussion on combining Greek letters with variables in R. Previously have solved this with substitute() command,
x = 4.5 plot(1:10, main = substitute(paste(&amp;quot;Kendall &amp;quot;, tau == val), list(val = x[1])))  but Peter has a more elegant solution that can be vectorized:
sapply(interaction(&amp;quot;sigma&amp;quot;,1:5, sep=&amp;quot; == &amp;quot;),function(x)parse(text=x)) #for instance plot(0) legend(&amp;quot;topright&amp;quot;,legend=sapply(interaction(&amp;quot;sigma&amp;quot;,1:5, sep=&amp;quot; == &amp;quot;),function(x)parse(text=x)),lty=2)  Better yet, bquote has a mechanism to evaluate an argument vs express the argument as a symbol, see gist id=1108159.</description>
    </item>
    
    <item>
      <title>Algorithms Discussion Group: MCMC</title>
      <link>/2011/05/04/algorithms-discussion-group-mcmc/</link>
      <pubDate>Wed, 04 May 2011 17:32:42 +0000</pubDate>
      
      <guid>/2011/05/04/algorithms-discussion-group-mcmc/</guid>
      <description>Implemented a basic MCMC routine in our little algorithms discussion group today, works quite nicely once you remember to use differences of log probabilities instead of ratios. Code and results below.
[gist id=956311]</description>
    </item>
    
    <item>
      <title>wrightscape profiling</title>
      <link>/2011/04/24/wrightscape-profiling/</link>
      <pubDate>Sun, 24 Apr 2011 18:50:11 +0000</pubDate>
      
      <guid>/2011/04/24/wrightscape-profiling/</guid>
      <description>Optimization at the R level is substantially slower than at the C level:
system.time(wright_test = wright(data,tree,regimes, alpha=ou2@sqrt.alpha^2, sigma=ou2@sigma)) user system elapsed 82.157 0.068 82.346 system.time(ws2 = wrightscape(trait, labrid$tree, regime=labrid$regimes, (ou2@sqrt.alpha)^2, ou2@sigma, theta=ou2@theta[[1]])) iter: 229, llik = 34.750, size = 0.000, par values = 0.000069 2.456353 0.000026 0.225234 0.973002 0.488713 user system elapsed 4.964 0.000 4.971  This is possibly due to repeating the lca matrix calculation. As it&amp;rsquo;s not done in a separate R function yet, the profiler cannot tell us this.</description>
    </item>
    
    <item>
      <title>Algorithms Discussion Group: algorithms Continued</title>
      <link>/2011/04/18/algorithms-discussion-group-abc-continued/</link>
      <pubDate>Mon, 18 Apr 2011 14:11:24 +0000</pubDate>
      
      <guid>/2011/04/18/algorithms-discussion-group-abc-continued/</guid>
      <description>Our little informal ABC group met again today to continue our discussion of ABC methods from last week. The updated code is included in the gist below. ((Nick asked about gists &amp;ndash; they are code boxes provided by Github that are easy to embed into blogs, etc. They provide automatic syntax highlighting and version management. For instance, I just opened last week&amp;rsquo;s gist and started editing during today&amp;rsquo;s session &amp;ndash; when I save it I get a new version ID, so last week&amp;rsquo;s post still points to last week&amp;rsquo;s code, but it doesn&amp;rsquo;t create a second copy, so you can tell if you have the right version.</description>
    </item>
    
    <item>
      <title>Monday: algorithms Meeting</title>
      <link>/2011/04/11/monday-abc-meeting-2/</link>
      <pubDate>Mon, 11 Apr 2011 16:21:09 +0000</pubDate>
      
      <guid>/2011/04/11/monday-abc-meeting-2/</guid>
      <description>Yaniv has started an algorithms discussion group. We just met with a few students to discuss implementing Approximate Bayesian Computing methods from scratch. Most of us had read (Beaumont, 2010) and (Csilléry et. al. 2010) but also followed [cite source=&amp;ldquo;pubmed&amp;rdquo;]12524368[/cite] most helpful during the session.
[gist id=914487]
This gets us as far as the regression step, Figure 1 of (Csilléry et. al. 2010). This in part distinguishes the approach from simple rejection sampling.</description>
    </item>
    
  </channel>
</rss>