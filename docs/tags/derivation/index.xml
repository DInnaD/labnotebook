<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Derivation on Boettiger Group</title>
    <link>/tags/derivation/</link>
    <description>Recent content in Derivation on Boettiger Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2013 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/derivation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/2013/12/10/notes/</link>
      <pubDate>Tue, 10 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/12/10/notes/</guid>
      <description>Consider the model
$$d X_t = \alpha \left(\theta - X_t\right)dt + \sigma dB_t $$
By Ito Isometry, we have:
$$\langle X \rangle_t = E_t(X) = \theta \left(1 - e^{-\alpha t} \right) + X_0 e^{-\alpha t}$$
and
$$\langle X^2 \rangle_t - \langle X \rangle_t^2 = V_t(X) = \frac{\sigma^2}{2 \alpha }\left(1 - e^{-2 \alpha t} \right)$$
If we assume discrete, uniform sampling of spacing $\tau$, then we have,
$$P(\theta | X; \alpha, \sigma) = \sqrt{\frac{1}{2\pi V_{\tau} }}^{T-1} \exp\left(-\frac{\sumt^{T-1} \left(X{t+1} - E\right)^2 }{2V}\right) $$</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/06/05/semi-analytic-posteriors/</link>
      <pubDate>Wed, 05 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/06/05/semi-analytic-posteriors/</guid>
      <description>The difficulty in comparing the nonparametric Bayesian inference against parametric Bayesian inference is ensuring that the poorer performance of the latter is not do to numerical limitations of the MCMC (no one is quite so worried about the cases where the mcmc solution appears to work well&amp;hellip;) Convergence is almost impossible to truly establish, and lots of pathologies (correlations between variables, particularly without simulataneous updating) can frustrate it considerably. While multiple chains and long run times are the reasonable default, for simple enough models we can take a more direct approach.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/06/04/analytic-marginalizing-for-posteriors/</link>
      <pubDate>Tue, 04 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/06/04/analytic-marginalizing-for-posteriors/</guid>
      <description>Consider the model
$$ X_{t+1} = X_t r e^{-\beta X_t + \sigma Z_t } $$
with $Z_t$ a unit normal random variable. The likelihood of the sequence of $T$ observations of $X$ under this model is thus
$$P(X | r, \beta, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}^{T-1}} \exp\left(\frac{\sumt^{T-1} \left(\log X{t+1} - \log X_t - \log r + \beta X_t\right)^2 }{2 \sigma^2}\right) $$
To integrate out $r$, $P(X | \beta, \sigma) = \int P(X | r, \beta, \sigma ) P&amp;reg; dr$, we&amp;rsquo;ll make this look like a Gaussian in $\log r$ by completing the square; getting the square on the outside of the sum.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/04/24/empirical-demonstration-of-that-clever-derivation/</link>
      <pubDate>Wed, 24 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/04/24/empirical-demonstration-of-that-clever-derivation/</guid>
      <description>Another unexpected gem from keeping an online notebook: Empirical evidence for the expected escape time problem! Getting to that in a moment&amp;hellip;.
After Marc pointed out to me that there is a clever analytic demonstration to back my claims about realized exit times being fast (posted earlier in my notebook under the title, a clever derivation of realized escape times, I&amp;rsquo;ve enjoyed a bit of a tangent in understanding this result that has long since been worked out by mathematicians but appears to be periodically rediscovered in ecological and evolutionary work in the most delightful ways.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/04/05/a-clever-derivation-on-realized-escape-times/</link>
      <pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/04/05/a-clever-derivation-on-realized-escape-times/</guid>
      <description>I have often observed (e.g. Boettiger &amp;amp; Hastings (2012) ) that if purely stochastic transitions out of a potential well happen, then they happen fast (dispite the fact that waiting time for them to happen is very long). Yesterday Marc mentioned an interesting proof of this from Don Ludwig, who was able to show that the waiting time to reach a large deviation in a time $t$, given that it reaches it at all, is of the same order as the return time from the large deviation to the equilibrium, $\log(L/\sigma)$.</description>
    </item>
    
    <item>
      <title>Tues: Fitting the limiting OU models to each bifurcation</title>
      <link>/2010/11/30/tues-fitting-the-limiting-ou-models-to-each-bifurcation/</link>
      <pubDate>Tue, 30 Nov 2010 22:56:03 +0000</pubDate>
      
      <guid>/2010/11/30/tues-fitting-the-limiting-ou-models-to-each-bifurcation/</guid>
      <description>Yesterday I derived the linearized SDEs for the saddle node bifurcation :
$$ dX = 2\sqrt{r(t)} (\sqrt{r(t)} + \theta -X) dt + \sigma \sqrt{\sqrt{r(t)}+\theta} dB_t $$
and the transcritical bifurcation
$$ dX = r(t) (K - X) dt + \sqrt{K} dB_t $$
Now to implement them for fitting by likelihood using generic $ r(t) $. I go back to the warning() model implemented in sde_likelihood.R code. We can just solve the odes directly:</description>
    </item>
    
    <item>
      <title>Further treatment of the limiting models</title>
      <link>/2010/11/29/further-treatment-of-the-limiting-models/</link>
      <pubDate>Mon, 29 Nov 2010 19:01:29 +0000</pubDate>
      
      <guid>/2010/11/29/further-treatment-of-the-limiting-models/</guid>
      <description>Have spent the last three days building and testing the infrastructure to fit the canonical form of a saddle-node (fold) bifurcation by likelihood. Actually I don&amp;rsquo;t use the canonical form straight up: $ dx/dt = x^2 +r$ cannot be fit directly in this manner, since it needs a couple scale transformations on the variables to get the units right (a multiple for the unit scale and an additive factor for the zero point), and more importantly, isn&amp;rsquo;t a stochastic model so cannot assign probabilities to the outcomes.</description>
    </item>
    
    <item>
      <title>Likelihoods for quadratic models cont</title>
      <link>/2010/11/28/likelihoods-for-quadratic-models-cont/</link>
      <pubDate>Sun, 28 Nov 2010 21:14:09 +0000</pubDate>
      
      <guid>/2010/11/28/likelihoods-for-quadratic-models-cont/</guid>
      <description>Considering the fits of the quadratic models from yesterday.
[flickr-gallery mode=&amp;ldquo;tag&amp;rdquo; tags=&amp;ldquo;quadraticfitslides&amp;rdquo; tag_mode=&amp;ldquo;all&amp;rdquo;]
Analysis of fits Of course these fits rely entirely on mapping the location of the stable point and the slope through that point, something they still cannot do all that well. In this parameterization the the stable intercept is $$ \hat x = \sqrt{r}+\theta $$ and slope at intercept is $$ \frac{\partial f(\hat x)}{\partial x} = -2\sqrt{r} $$ This creates a numerical optimization problem, since there is high importance on getting the mean right, and less on the variance, the routine tends to fix the combination of r and theta to get the mean right, and then cannot change r to get the slope correct without getting penalized for the loss of likelihood in changing the mean.</description>
    </item>
    
    <item>
      <title>Warning signals: Likelihoods Straight-up</title>
      <link>/2010/11/24/warning-signals-likelihoods-straight-up/</link>
      <pubDate>Wed, 24 Nov 2010 21:44:57 +0000</pubDate>
      
      <guid>/2010/11/24/warning-signals-likelihoods-straight-up/</guid>
      <description>Meeting with Alan this morning. I haven&amp;rsquo;t solved likelihood of power spectra (previous couple entries in Stoch. Pop / warning signals), though after meeting with Sebastian (11&amp;frasl;9) probably the best method works from the correlation function. Define a new stochastic process Y $$ Y_1 = X_1 X_2 + X_2 X_3 + X_3 X_4 + \ldots $$ $$ Y_2 = X_1 X_3 + X_2 X_4 + \ldots $$ $$ Y_3 = \ldots $$</description>
    </item>
    
    <item>
      <title></title>
      <link>/2009/08/25/diffusion-solution/</link>
      <pubDate>Tue, 25 Aug 2009 00:00:00 +0000</pubDate>
      
      <guid>/2009/08/25/diffusion-solution/</guid>
      <description>editorial note: These notes pre-date the formal start of my online laboratory notebook, Feb 2 2010: The Lab Notebook Goes Open and were adapted from a LaTeX document in which I kept notes on this topic during my summer at IIASA. Lacking a proper notebook then, documents like this one were updated periodically and occassionally branched into new ones. The post date represents the last time the LaTeX document was edited in the course of that research.</description>
    </item>
    
  </channel>
</rss>