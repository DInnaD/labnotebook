<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Open Science on Boettiger Group</title>
    <link>/tags/open-science/</link>
    <description>Recent content in Open Science on Boettiger Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 May 2011 18:39:49 +0000</lastBuildDate>
    
	<atom:link href="/tags/open-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Building a semantic notebook</title>
      <link>/2011/05/08/building-a-semantic-notebook/</link>
      <pubDate>Sun, 08 May 2011 18:39:49 +0000</pubDate>
      
      <guid>/2011/05/08/building-a-semantic-notebook/</guid>
      <description>What would a semantic lab notebook look like? What would be possible with such a structure, ideally? What is already possible now? There&amp;rsquo;s a lot that can be done with an electronic notebook that adds value over a paper notebook: browsing by categories and tags, embedding links, time-stamping entries, searching the full text, and having automated trackbacks or pingbacks when a post is mentioned in another post or appears elsewhere on the web.</description>
    </item>
    
    <item>
      <title>Citation tools &amp; Future of Publishing</title>
      <link>/2011/02/16/citation-tools-future-of-publishing/</link>
      <pubDate>Wed, 16 Feb 2011 23:04:50 +0000</pubDate>
      
      <guid>/2011/02/16/citation-tools-future-of-publishing/</guid>
      <description>There has been a lot of rapid development in scientific tools based on a Wordpress platform recently, perhaps spurred in part by the recent Beyond-the-PDF and sessions in Science Online conferences. A new discussion group has emerged around these tools, as described by Martin Fenner.
I have been exploring better tools for citation management within my lab notebook. Recently I have been using the papercite plugin to add citations from my BibTeX files, which are generated from Mendeley, as I described earlier.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Research  Wrote summary function using bootstrap confidence intervals for the bootstraps of the likelihoods directly. On interpreting the bootstraps of likelihoods directly:   The extent to which the distributions themselves are distinct provides an indication of the ability to distinguish between models on the given phylogenetic tree and the actual data. For instance, if the tree was a star tree and the data produced by either BM or OU1, then all distributions would fall on top of one another.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Earlier today, Scott Collins, the president of the Ecological Society of America has announced that the society will now accept articles that have previously been posted on preprint servers. This comes on the heels of a growing discussion in our community. Ethan White has a good summary over on Jabberwocky Ecology.
Many voices have joined the discussion over the past month, and it is exciting and vindicating to see the Society engage and discuss these questions.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>At home after birth of Edward since Jan 19th, leaving little time for research. Assembled notes from a few side projects I&amp;rsquo;ve moved forward in the wee hours. Covers the past several days.
XML and RDFa parsing I have been using the lab notebook to explore technology for scientific communication, including making use of semantic and linked data concepts. To illustrate some of the potential these tools can offer I&amp;rsquo;ve started working on a short post with examples parsing and exploring the XML and RDFa data in the notebook.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Prosecutor  Add figures for both examples with and without ASS text to accomidate both examples ROC curves? proof-reading  ews-review  looking over revisions, touch-ups See issues log for details  Links  Impressive set of tools provided by the Whitehouse open data project In particular, see the tool for generating a RESTful API from CSV files and the &amp;ldquo;common core&amp;rdquo; metadata definitions. Notebook complies with many of these (throught the Dublin core RDFa), but looks like I could benefit from adding some more terms from the Data Catalog Vocabulary.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Writing out some advice for our GSOC student on XML parsing. (Now filed as RNeXML/#11
Here is some quick background on different ways we might go about extracting NeXML into an R object we want to work with. We can use S4 classes, R&amp;rsquo;s native data.frame and list types, or extract specific terms of interest with xpath. I illustrate each of these below using the example &amp;ldquo;trees.xml&amp;rdquo; in your repository.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Sun Co-Leading the Workshop on R, DataONE and open science.
All material on github
Mon  Ives Plenary  Talking to Dave, Vasilis, Tye,
 Lunch Vasilis, Ryan Batt
 2:30 Chesson L100H / Ryan Batt L100J
 3:20 O&amp;rsquo;Regan L100H
 3:40 Adler L100H
 4 Yaekel M100HC
 ropensci meetup, funded by Github. Great turnout, great success. (And rare shot of dev team together in person).
 Theoretical ecology mixer</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Other services: Tangible infrastructure support services  Discovery registration system. Includes:  Software review and certification Use and quality metrics Ticket tracking Seed funding to join the ecosystem   Community Building  user defined registry  // Hackathon?
Consulting services  Support software lifecycle
 Function: Assist in full lifecycle software development: e.g. (and order notwithstanding) initiation, licensing, maintenance, hardening, extensible, release, decommissioning
 Mechanisms:
 Call for Proposals - to solicit consulting help (or to nominate a project to receive help) (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Work Work  Finished review for R Journal (Wednesday) Finished review for TE. Began review for NSF updated service list (vita) Handling reimbursement of expenses  Misc  Whoa, we can/should now use http://doi.org instead of http://dx.doi.org  Software Sustainability materials Seems like a popular new item, listing a couple recent articles in one place here:
 Perspective in Science Magazine; also a critique and author&amp;rsquo;s rebuttal in a later edition.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Education / Workforce Development Group Cannot do grand synthesis opaquely and be science. must be able to scale and evolve.
How did data succeed? What features really made the difference and what were just sideshows?
 Sticks (Funder and journal mandates?) Working groups (NCEAS) Training external forces: there is more data.
  Why one center? Why not 15 centers? Divided by discipline rather than region&amp;hellip;. synthesize the synthesis centers</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>Near the end of another excellent day of discussions we were asked to put pen to paper to outline our own views in writing. My own contribution was the following notebook scribble.
My own vision of what ISEES should be doing for workforce development:
ISEES would employ a multi-scale approach to improve basic computational expertise across the entire workforce necessary to fully participate in the modern software-immersed world of research through the development of training standards, while using a working-group model focused on emerging researcher-developer communities which would simultaneously improve the quality and synthesis of existing software tools while also delivering essential skills to the most transformative members of the research software communities.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>ugh, most journals don&amp;rsquo;t seem to hold to their own data publication policies.
PNAS theoretically endorses UPSIDE, r citet(&amp;quot;10.1073/pnas.0400437101&amp;quot;) principles from the National Academy of Science:
 Principle 1. Authors should include in their publications the data, algorithms, or other information that is central or integral to the publication—that is, whatever is necessary to support the major claims of the paper and would enable one skilled in the art to verify or replicate the claims.</description>
    </item>
    
    <item>
      <title>Adaptive Dynamics</title>
      <link>/1/01/01/adaptive-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/adaptive-dynamics/</guid>
      <description>Wrapping up paper on waiting time to branching in adaptive dynamics. Cleaning up code from summer. Now in a local git repository in adaptiveDynamics/code Moved ensemble parallelization to higher level, so parallelization can be performed in R or C. Adding documentation  Current status  Calculates the waiting time to complete phase 1, phase 2, and phase 3 successfully from individual-based simulation.  Phase 1 ~ Completed when 2 branches exist over threshold Phase 2 ~ Completed when a third population has established over threshold exterior to a stable pair Phase 3 ~ Completed when two populations over threshold are separated in traitspace by more than finishline units</description>
    </item>
    
    <item>
      <title>Adaptive Dynamics</title>
      <link>/1/01/01/adaptive-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/adaptive-dynamics/</guid>
      <description>Public code hosting  Project code now available through Github. Advantages to public hosting: Replication of adaptive dynamics simulations can be difficult, particularly in matching various nuisance parameters. While there is certainly much pedagogical value in individual researchers implementing their own simulations, verification and reproducibility of scientific results would be greatly facilitated by adapting an accepted open source code base and adding branches, etc. Researchers could much more immediately benefit from the work of their colleagues and spend more time implementing novel research and less reproducing it.</description>
    </item>
    
    <item>
      <title>Adaptive Dynamics</title>
      <link>/1/01/01/adaptive-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/adaptive-dynamics/</guid>
      <description>Public code hosting  Project code now available through Github. Advantages to public hosting: Replication of adaptive dynamics simulations can be difficult, particularly in matching various nuisance parameters. While there is certainly much pedagogical value in individual researchers implementing their own simulations, verification and reproducibility of scientific results would be greatly facilitated by adapting an accepted open source code base and adding branches, etc. Researchers could much more immediately benefit from the work of their colleagues and spend more time implementing novel research and less reproducing it.</description>
    </item>
    
    <item>
      <title>Adaptive Dynamics</title>
      <link>/1/01/01/adaptive-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/adaptive-dynamics/</guid>
      <description>Simulation Results (code)  Analytics (Analytics code)
Notebooks and files Looking at a better way to get figures into my online notebook. I often run simulations that generate figures which I look at, make some adjustments to the code based on the figure, and then delete or overwrite the figure without uploading it to my notebook, because it isn&amp;rsquo;t &amp;ldquo;right&amp;rdquo; or &amp;ldquo;finished&amp;rdquo; and uploading takes a lot of clicks, particularly when the code generates 36 figures in a run.</description>
    </item>
    
    <item>
      <title>Deep challenges to dynamic documentation in daily workflows</title>
      <link>/1/01/01/deep-challenges-to-dynamic-documentation-in-daily-workflows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/deep-challenges-to-dynamic-documentation-in-daily-workflows/</guid>
      <description>We often discuss dynamic documents such as Sweave and knitr in reference to final products such as publications or software package vignettes. In this case, all the elements involved are already fixed: external functions, code, text, and so forth. The dynamic documentation engine is really just a tool to combine them (knit them together). Using dynamic documentation on a day-to-day basis on ongoing research presents a compelling opportunity but a rather more complex challenge as well.</description>
    </item>
    
    <item>
      <title>Entry title</title>
      <link>/1/01/01/entry-title/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/entry-title/</guid>
      <description>Automated scripts for image uploads. Tag-based generation of a slideshow:


Running the script:
 ./flickr_slideshow *.png  Creates the html code that can be embedded into the notebook for a slideshow those images. Images only get the date-time tag as a unique id.


This one uses the mediawiki plugin. More formatting options could be incorporated of course. (Creates the thumbnail images down the right-hand side).</description>
    </item>
    
    <item>
      <title>ISEES software lifecycle and components workshop, Day 1</title>
      <link>/1/01/01/isees-software-lifecycle-and-components-workshop-day-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/isees-software-lifecycle-and-components-workshop-day-1/</guid>
      <description>Range of large project to the long tail.
 statistical analyses One-off models community model Services (Blast)  Science Challenges Outcomes of previous community workshops
18 Grand Challenge areas. Highlights: Supermodels. human/biophysical systems shape and be shaped by water availability. (A vision similar to Microsoft&amp;rsquo;s &amp;ldquo;Model all Earth&amp;rdquo; Purves et al)
Functions and services areas: (priority ordering)
 Computational training (early career - all career) Assimilation and QA/QC tools Collaborative environment decisions and workflows software discovery consultants / collaborator community hub to converge on standards merging disparate tools user-friendly interfaces multiscale coupled modeling framework Software vetting (how about paper vetting?</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/1/01/01/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/research/</guid>
      <description>Seem to have successfully fixed the behavior when getting over parameterized models. See diffs against the git log for details.
 Will work on acceleration of code (avoiding refitting), meanwhile a little more trouble shooting and queuing some long runs.
 ace function fails on certain data sets for unknown reasons. Think rewriting this from scratch will be an important step.
  Mixture Models in Bayesian context  Josh sent me this paper regarding our discussion on model choice in Bayesian context last Friday.</description>
    </item>
    
  </channel>
</rss>