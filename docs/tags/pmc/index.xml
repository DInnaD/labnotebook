<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pmc on Boettiger Group</title>
    <link>/tags/pmc/</link>
    <description>Recent content in Pmc on Boettiger Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2013 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/pmc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/2013/04/23/we-need-more-object-oriented-design-in-comparative-methods/</link>
      <pubDate>Tue, 23 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/04/23/we-need-more-object-oriented-design-in-comparative-methods/</guid>
      <description>The bad news is that the latest Geiger version (which is much improved in all respects) breaks my little pmc package, owing to changes in how fitContinuous output is structured. The good news is that the new geiger is much better than the old one, and that the pmc package is pretty trivial. So this should not be read as a complaint, but I think as a scientific developer community as a whole we could learn a few simple lessons here, which can help our code more reproducible and extensible.</description>
    </item>
    
    <item>
      <title></title>
      <link>/2013/04/18/notes/</link>
      <pubDate>Thu, 18 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/04/18/notes/</guid>
      <description>Misc coding  knitcitations handling of html formatting in tooltip: html inside the tag must needs be escaped. knitcitations/#37
 pmc functions
  hmm.. how do we easily wrap a function such that it returns its arguments (given and default) in a named list (appropriate for do.call(function_name, args_list))? match.call() inside the function will give the literal string of how the function was called, which, if evaluated in the same environment with same variables in working space, will return the same results, but doesn&amp;rsquo;t lend itself to updating any one of those arguments.</description>
    </item>
    
    <item>
      <title>Tues: Research, seminar (managing for resilience), talk (future of libraries), etc...</title>
      <link>/2012/02/21/tues-research-seminar-managing-for-resilience-talk-future-of-libraries-etc/</link>
      <pubDate>Tue, 21 Feb 2012 19:20:40 +0000</pubDate>
      
      <guid>/2012/02/21/tues-research-seminar-managing-for-resilience-talk-future-of-libraries-etc/</guid>
      <description>research Stuff warningsignals  Exceeded wall time on NERSC, readjusting and running this again.  treebase  Working on treebase manuscript. Treebase Rewrote handling of server errors, added pauses to reduce load, etc.  PMC  A user pointed out my support for &amp;ldquo;white&amp;rdquo; model was incomplete. Fix is now on github (install instructions for development version are also now on github), will push to CRAN soon.
 Another user points out that pmc support for additional functions would be nice, such as rbrownie, fit.</description>
    </item>
    
    <item>
      <title>Friday</title>
      <link>/2011/12/09/friday-9/</link>
      <pubDate>Fri, 09 Dec 2011 13:42:30 +0000</pubDate>
      
      <guid>/2011/12/09/friday-9/</guid>
      <description>Wrightscape/pmc  Writing up derivation from Thursday properly / for manuscript. Writing up test cases for method2, direct evaluation of covariance matrix for comparisons Minor revisions submitted to Evolution  pdg-control To Do  Implement the NCO. Implement HS SDP Closer read of (Sethi et. al. 2005), (Su &amp;amp; Peterman, 2012) and Woodward &amp;amp; Tomberlin  Warning signals  Allee effects in linear programming with different classes. Timescales of noise using van Kampen expansion.</description>
    </item>
    
    <item>
      <title>Wednesday - wrightscape runs and various other things</title>
      <link>/2011/12/07/wednesday-11/</link>
      <pubDate>Wed, 07 Dec 2011 18:01:07 +0000</pubDate>
      
      <guid>/2011/12/07/wednesday-11/</guid>
      <description>pdg-control  plots for changing costs Report out to Training Problem II group.  ggplot note defining functions that return a ggplot object can be tricky. If you compute some objects in the plot function (stats etc), those stats are not stored by the object (due to lazy evaluation, usually a very nice time-saving feature), so the plot cannot be produced unless the function returns those things to the global environment (or from wherever the plot will be printed/evaluated).</description>
    </item>
    
    <item>
      <title>Wednesday - various logistics, then checking likelihood calculation</title>
      <link>/2011/11/30/wednesday-various-logistics-then-checking-likelihood-calculation/</link>
      <pubDate>Wed, 30 Nov 2011 20:20:42 +0000</pubDate>
      
      <guid>/2011/11/30/wednesday-various-logistics-then-checking-likelihood-calculation/</guid>
      <description>rOpenSci  rOpenSci wins runner up prize in PLoS + Mendeley&amp;rsquo;s Binary Battle! Write replies to interview questions with Mendeley. roxygenizing documentation for RMendeley &amp;ndash; about half done  pdg-control  Call with Jake scheduled for tomorrow Emails with Michael. I&amp;rsquo;ll implement delay intervals, he&amp;rsquo;ll add costs to adjustment. Need to fix/finish delay interval optimization calculation (exponent of matrix &amp;amp; beta)  Warning Signals Meeting with Alan
 Regime shifts Emphasize the RO curve in uncertainty analysis See (Dulvy et.</description>
    </item>
    
    <item>
      <title>Tuesday</title>
      <link>/2011/10/04/tuesday-9/</link>
      <pubDate>Tue, 04 Oct 2011 22:29:29 +0000</pubDate>
      
      <guid>/2011/10/04/tuesday-9/</guid>
      <description>Hastings seminar: Forest Pest Outbreaks Schedule
Prof Dave Rizzo  Reviews major pest outbreaks (Major Forest Insect &amp;amp; Disease Conditions in the United States 2009 Update)  Mentions weekend&amp;rsquo;s NY Times piece, (and see author&amp;rsquo;s commentary), reviews major pest outrbreak trends.   David Kling Allee effects &amp;amp; Management (Gypsy moth). * Review review (Taylor &amp;amp; Hastings, 2005). * Simulation exploration: pmid: 19831081, * Also discuss (Tobin et. al. 2011).</description>
    </item>
    
    <item>
      <title>Friday: ropensci, manuscript edits</title>
      <link>/2011/09/23/friday-ropensci-manuscript-edits/</link>
      <pubDate>Fri, 23 Sep 2011 11:03:43 +0000</pubDate>
      
      <guid>/2011/09/23/friday-ropensci-manuscript-edits/</guid>
      <description>rOpenSci Conference Call Catalyst grant  Reviewed budget. Karthik will merge current proposals, Carl will review. Goal: send out next week. Ask Mark Hahnel to look it over  Blog posts  Carley planning to make guest blog post within the next week.  Mendeley/Plos Binary battle  Carl will file entry form. Karthik push edits to rplos Scott will add rplos tutorial to ropensci.org Carl will add rmendeley tutorial to ropensci.</description>
    </item>
    
    <item>
      <title>Wednesday</title>
      <link>/2011/04/20/wednesday-3/</link>
      <pubDate>Wed, 20 Apr 2011 22:24:25 +0000</pubDate>
      
      <guid>/2011/04/20/wednesday-3/</guid>
      <description>Reading  GrrlScientist disagrees with M. Nielsen (and I add my 2c: Open Science might not give you the credit you need to get a job in today&amp;rsquo;s science culture, but hey, in all probability, neither will the traditional strategy, so have fun). Nature says: Too many PhDs Some useful tools for data visualization: particularly want to try:Fusion Tables, Gephi, DataViz Mendeley groups don&amp;rsquo;t display (or share) stars. Can only sort A-Z or by most recent.</description>
    </item>
    
    <item>
      <title>Likelihood Ratio Paper</title>
      <link>/2010/10/03/likelihood-ratio-paper/</link>
      <pubDate>Sun, 03 Oct 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/10/03/likelihood-ratio-paper/</guid>
      <description>Higher resolution power plot for Anoles tree Figure (below right) shows power for increasing alpha in the Anoles tree. The vertical line shows the estimated value of alpha for the body-size data &amp;ndash; unlikely that a tree of this size can detect selection that is this weak even if it were present. Consequently, this fit should not be taken as evidence that BM is a better fit than the OU, but rather that the data is insufficient to inform this analysis.</description>
    </item>
    
    <item>
      <title>Likelihood Ratio Paper- Power in Trees</title>
      <link>/2010/08/26/likelihood-ratio-paper--power-in-trees/</link>
      <pubDate>Thu, 26 Aug 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/08/26/likelihood-ratio-paper--power-in-trees/</guid>
      <description>Power in Trees tests (Results from single OU simulation per alpha, run yesterday) 2000 replicates gives a cleaner picture of the critical alpha, though with low alpha values the estimates of p are quite variable. Also more variable on the smaller trees. Even arbitrarily large alpha aren&amp;rsquo;t significant on the 5-taxa primate tree. On the small geospiza tree (13 taxa) from the geiger package we find:
0.5735 0.29 0.</description>
    </item>
    
    <item>
      <title>Wainwright Meeting</title>
      <link>/2010/07/19/wainwright-meeting/</link>
      <pubDate>Mon, 19 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/07/19/wainwright-meeting/</guid>
      <description>Discussed goals and appropriate approach. Settled on linear order rather than trying to directly implement the full MCMC over paintings solution directly. Breaks into four steps/manuscripts:
 Phylogenetic signal and model choice in Comparative Methods: Likelihood ratio approach Detecting changing constraints in evolution: varying alpha and sigma across the tree Bayesian approach for all existing continuous trait comparative methods MCMC over possible histories to infer number of niches</description>
    </item>
    
    <item>
      <title></title>
      <link>/2010/04/08/comparative-phylogenetics-notebook/</link>
      <pubDate>Thu, 08 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/04/08/comparative-phylogenetics-notebook/</guid>
      <description>Research  Wrote summary function using bootstrap confidence intervals for the bootstraps of the likelihoods directly. On interpreting the bootstraps of likelihoods directly:   The extent to which the distributions themselves are distinct provides an indication of the ability to distinguish between models on the given phylogenetic tree and the actual data. For instance, if the tree was a star tree and the data produced by either BM or OU1, then all distributions would fall on top of one another.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/04/05/research/</link>
      <pubDate>Mon, 05 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/04/05/research/</guid>
      <description>Bootstrapping procedures should have the option to refit the painting. Currently use the fixed painting but refit parameters. wrote the function peaks_update() which update the painting as well as the OU peak parameters.  20 replicates, bootstraps the painting step as well
Computing / Package  LR_bootstrap and model_bootstrap should be the same function call with an optional second model for the LR bootstrapping. Similarly the LR_bootstrap_all and model_bootstrap_all() should be the same function call.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/04/02/research/</link>
      <pubDate>Fri, 02 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/04/02/research/</guid>
      <description>Graham &amp;amp; Peter Meeting Fig 1
Fig 2
 Decided that bootstrapping against fixed model parameters would be more difficult to interpret, Fig 1. Simulated data results with refitting seem strange (Fig. 2), perhaps related to fitting of the tree. Observed LR for data should at least remain symmetric, so something has gone wrong. Expected pattern should be deducible simply by upper left triangle of graphs, should accept the first model where the data becomes typical.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/30/research/</link>
      <pubDate>Tue, 30 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/30/research/</guid>
      <description>Current Situation &amp;amp; Challenges 2000 bootstraps of models fitted to simulated ou2 data
 A single data set is produced by simulation under a specified ou2 model on the Labrid tree (actually one that was fitted to the original Labrid data). Each model (Brownian motion and 1 to 5 separate OU peaks (assigned by my partition function) is fitted to this dataset. Each fitted model is used to generate 2000 replicate datasets For each replicate dataset, the model used to generate it is compared to each other model in likelihood ratio test 2(log(L~1~) − log(L~2~)), where L~1~ is the likelihood of the model that generated the data.</description>
    </item>
    
    <item>
      <title>Still puzzling on LR</title>
      <link>/2010/03/26/still-puzzling-on-lr/</link>
      <pubDate>Fri, 26 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/26/still-puzzling-on-lr/</guid>
      <description> The likelihood ratios when parameters are not held fixed, using data originally generated from a two peak model. Has 1000 bootstrap replicates per plot.   The likelihood ratios when parameters are held fixed, after being generated from a two-peak model. (2000 replicates each)   parameters held fixed, three peak model (200 replicates each)  </description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/25/research/</link>
      <pubDate>Thu, 25 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/25/research/</guid>
      <description>New Simulations  Now have code to run partitioned models on the Anoles dataset, compare partitioned models to the paintings chosen by Butler and King. Preliminary figures below. Models with _s are partitioned by my algorithm, others are the the original ones from earlier.   Also have examples with simulated data with a known number of partitions, to check the method. Tested with data produced at two peaks on the Labrid tree, the pattern seems similar to that of the Labrids &amp;ndash; comparisons involving any model with more than 2 peaks produce LR ratios that become very unlikely in either pairwise comparison.</description>
    </item>
    
    <item>
      <title>Parallel R</title>
      <link>/2010/03/24/parallel-r/</link>
      <pubDate>Wed, 24 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/24/parallel-r/</guid>
      <description>Parallelized LR_bootstrap_all() and model_bootstrap_all() bootstrapping functions. Lets the bootstrapping of different models (or LRs between models) to be performed on different machines. Using the snowfall package in R for easy parallel computing. The parallel loop is done through the sfLapply() function. Had to get libraries and functions exported to all loops using sfLibrary() and sfExportAll(). R-sig-hpc mailing list was very helpful at getting this up and running at 4am.</description>
    </item>
    
    <item>
      <title>Bootstrapping Likelihood ratios</title>
      <link>/2010/03/23/bootstrapping-likelihood-ratios/</link>
      <pubDate>Tue, 23 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/23/bootstrapping-likelihood-ratios/</guid>
      <description>Bootstraps of the likelihood ratio, holding the parameters fixed, finally completed.  Anoles: Summary Anoles table:
&amp;gt; summary(anoles_boots) [,1] [,2] [,3] [,4] [,5] [1,] 0.500000000 0.981399498 0.26514573 0.06668672 0.018154938 [2,] 0.009580545 0.500000000 0.15610274 0.04819938 0.011616566 [3,] 0.039233893 0.075349644 0.50000000 0.05699877 0.005443493 [4,] 0.004211230 0.004592917 0.01389950 0.50000000 0.006648916 [5,] 0.173457542 0.198303402 0.38893099 0.33891430 0.500000000 &amp;gt;  Labrids: &amp;gt; summary(labrid_boots) [,1] [,2] [,3] [,4] [1,] 0.5000000 6.036842e-05 0.0000000 0.0 [2,] 0.5424613 5.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/20/research/</link>
      <pubDate>Sat, 20 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/20/research/</guid>
      <description>Bootstrap the fitted models (rather than allow the parameters to be refit, simply evaluate the likelihood of the model under each simulated dataset) Directly evaluate Neyman-Pearson lemma for each pairwise model comparison. Also analyze the bootstrapped likelihoods directly (and without refitting) for each model.  To Do  Add a summary function for the likelihood_ratio_bootstrap.R library final wrapper function for infer_niches.R partitioning library More error handling and documentation Example of partitioning Anoles then bootstrapping Bootstrapping likelihoods directly  References on thinking about bootstrapping model comparisons  The original Neyman-Pearson paper Testing Statistical Hypotheses Text by Lehmann &amp;amp; Romano Permutation, Parametric and Bootstrap Tests of Hypotheses by Good Thanks to Peter on these; Bibsonomy collection  ouch2ape.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/19/research/</link>
      <pubDate>Fri, 19 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/19/research/</guid>
      <description>Bootstrapped against 2000 replicate simulations overnight on both the Anoles dataset and the Labrids dataset. Distributions that overlap zero represent replicates that choose the wrong model. The Anoles data below; the first column corresponds to the likelihood ratios shown in Butler &amp;amp; King (2004), comparing each model against BM.   The Labrid dataset:   Find matching figures for the case of parameters held fixed on more recent entry</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/18/research/</link>
      <pubDate>Thu, 18 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/18/research/</guid>
      <description>A some point I&amp;rsquo;ll want to think more seriously about incorporating geographic information into comparative analyses. At some level this data is more available than appropriate continuous traits, as illustrated in the example phylogenetics wiki from Prof. Roderic Page that I just came across.  Today&amp;rsquo;s Goals  Repeating last night&amp;rsquo;s bootstraps on the likelihood ratio. Need to clean up and incorporate my bootstrap codes first. Should also explore the bootstrapping of the partitioning functions from yesterday.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/2010/03/17/research/</link>
      <pubDate>Wed, 17 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/17/research/</guid>
      <description>Maximum likelihood assignment based on arbitrary number of partitions now completed. Working on stochastic assignment. Now have functions for:   partitioning data into k groups, inferring an ancestral state painting based on this partition inferring a continuous model with k OU regimes based on this painting   need to add my function for the proper model-comparison bootstrap of likelihood ratio values rather than the likelihoods themselves. See my original entry on this approach.</description>
    </item>
    
    <item>
      <title>Solution to the strange behavior in ouch</title>
      <link>/2010/03/03/solution-to-the-strange-behavior-in-ouch/</link>
      <pubDate>Wed, 03 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/03/solution-to-the-strange-behavior-in-ouch/</guid>
      <description>Looks like my troubles actually stem from a bug in the code. I summarize the problem here, just as I posted in my query to R sig phylo. Essentially the program erroneously squares alpha at the moment, explaining the pattern I found yesterday. Giving it root alpha preemptively should be a good work around.

data(bimac) tree &amp;lt;- with(bimac,ouchtree(node,ancestor,time/max(time),species)) print(h2 &amp;lt;- hansen(log(bimac[&#39;size&#39;]),tree,bimac[&#39;OU.1&#39;],alpha=1,sigma=1))  The print command says alpha = 0.</description>
    </item>
    
    <item>
      <title>Power in trees</title>
      <link>/2010/03/02/power-in-trees/</link>
      <pubDate>Tue, 02 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/03/02/power-in-trees/</guid>
      <description>Back to Phylogenetics after a week with only population dynamics.  Exploration  data created by brown simulation on the star tree in ouch matches the expected variance, σ^2^t. This variance is slightly reduced when simulated on the Felsenstein tree, as might be intuitively expected due to the increased correlations. The degree of decrease should be analytically attainable, but isn&amp;rsquo;t obvious to me at the moment. Example using functions in felsenstein_tree.</description>
    </item>
    
    <item>
      <title>Implementing an example of low and high power trees</title>
      <link>/2010/02/20/implementing-an-example-of-low-and-high-power-trees/</link>
      <pubDate>Sat, 20 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/20/implementing-an-example-of-low-and-high-power-trees/</guid>
      <description>Code implementation in felsenstein_tree.R
 I consider a star tree and a Felsenstein tree each of N nodes; M = (N+1)/2 tips. I consider an uncorrelated data set of M normal random variables, such as would be generated by either BM or OU models on the star tree. I produce a correlated data set such as would be generated by Brownian motion on the felsenstein tree. I generate 8 model fits &amp;ndash; fitting BM and OU to each tree under each data set.</description>
    </item>
    
    <item>
      <title>Updating manuscript</title>
      <link>/2010/02/16/updating-manuscript/</link>
      <pubDate>Tue, 16 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/16/updating-manuscript/</guid>
      <description>The progress of the past few days has lead to substantial revision proposals in my manuscript draft. Rewriting for discussion with Brian tomorrow. Currently revised outline:
 Intro
 Biological Motivation Comparative Methods and Comparing Models Model choice paradigm: the right direction, but incomplete  Simple Example
 The Star Tree Two hypotheses: stabilizing vs disruptive selection Information Criteria approach is insufficient Likelihood surfaces Likelihood ratio distribution  Anoles Example</description>
    </item>
    
    <item>
      <title>Parametric bootstrapping</title>
      <link>/2010/02/15/parametric-bootstrapping/</link>
      <pubDate>Mon, 15 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/15/parametric-bootstrapping/</guid>
      <description>There are several ways we can frame the bootstrapping question. What is the probability that we see a given likelihood ratio statistic (that the log likelihoods differ by a certain spread) just by chance? We have two versions of chance &amp;ndash; the chance we see this ratio when model a is correct and the chance we see this ratio when model b is correct. We simulate 2000 data sets under each model, generating a distribution of likelihood ratio scores for each data set.</description>
    </item>
    
    <item>
      <title>Follow-up on Parametric Bootstrapping</title>
      <link>/2010/02/10/follow-up-on-parametric-bootstrapping/</link>
      <pubDate>Wed, 10 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/10/follow-up-on-parametric-bootstrapping/</guid>
      <description>Continuing on yesterday&amp;rsquo;s discussion of parametric boostrapping implementation. Peter sent some very helpful comments and agreed to having them here for reference; I&amp;rsquo;ve added some links.  &amp;ldquo;I&amp;rsquo;ve read up a bit on the bootstrap just now. First off, although parametric and nonparametric bootstrapping has always seemed very different to me, they fit in the same natural framework. The general idea is that in reality, there&amp;rsquo;s some real parameters x and some probability distribution P that&amp;rsquo;s given us some data.</description>
    </item>
    
    <item>
      <title>Meeting with Graham and Peter</title>
      <link>/2010/02/09/meeting-with-graham-and-peter/</link>
      <pubDate>Tue, 09 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/09/meeting-with-graham-and-peter/</guid>
      <description>Discussion of Model Comparisons Primary focus of today&amp;rsquo;s discussion was rooted in Sunday&amp;rsquo;s entry
 On plotting quirks, haven&amp;rsquo;t figured out lattice but here&amp;rsquo;s yet another newer plotting engine for R I need to revisit how OUCH computes likelihood ratios and P values. Since some parameters are bounded to the positive real line (and may be small) the standard chi square isn&amp;rsquo;t correct. I should repeat the AIC-style analysis for likelihood ratios, though this is just a shift of where I put the zero line.</description>
    </item>
    
    <item>
      <title>Exploring Difficulties with Existing Methods</title>
      <link>/2010/02/07/exploring-difficulties-with-existing-methods/</link>
      <pubDate>Sun, 07 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/2010/02/07/exploring-difficulties-with-existing-methods/</guid>
      <description>Model choice plays an increasingly dominant role in comparative phylogenetics. The software OUCH of Butler and King (2004) is built around the model comparison framework and returns a variety of information criteria for choosing between different models fitted to the tree.
 I&amp;rsquo;ve written a short R script to explore how robust these inferences are, and the results are rather surprising. None of the inferences holds up to the classical P value standard of 95% probability of being right, and the inference can be wrong more often than it is right &amp;ndash; a coin flip out-performs the model choice method based on these information criteria in certain cases.</description>
    </item>
    
  </channel>
</rss>