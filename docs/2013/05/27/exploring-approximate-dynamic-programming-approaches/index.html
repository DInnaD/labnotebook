<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.20.2" />
  <meta name="author" content="Carl Boettiger">
  

  
  
  
    
    
    <link rel="stylesheet" href="../../../../css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="../../../../css/bootstrap.min.css">
  <link rel="stylesheet" href="../../../../css/font-awesome.min.css">
  <link rel="stylesheet" href="../../../../css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  
  
    <link rel="stylesheet" href="../../../../css/cboettig.css">
  
  
  <link rel="alternate" href="../../../../index.xml" type="application/rss+xml" title="Boettiger Group">
  <link rel="feed" href="../../../../index.xml" type="application/rss+xml" title="Boettiger Group">

  <link rel="icon" type="image/png" href="../../../../img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="../../../../img/apple-touch-icon.png">
  <link rel="canonical" href="../../../../2013/05/27/exploring-approximate-dynamic-programming-approaches/">


  <title> | Boettiger Group</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../../../">Boettiger Group</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="../../../../index.html">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="../../../../members/">
            
            <span>Members</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="../../../../vita/">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="../../../../teaching/">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="../../../../lab-notebook">
            
            <span>Lab Notebook</span>
          </a>
        </li>

        
        
      </ul>

    </div>
  </div>
</nav>


<div class="row">
  <div class="col-md-7 col-md-offset-1">
    <article>
    
      
    

<h2 id="introductory-examples-in-approximate-dynamic-programming">Introductory examples in approximate dynamic programming</h2>

<p><em>Based on Powell 2006, page 97.  I try to conform to that notation throughout</em>.  Haven&rsquo;t really figured this out yet, so this is more a walk through of me thinking this out then a tutorial. My active copy of this analysis can be found in the <a href="https://github.com/cboettig/nonparametric-bayes/blob/4eb6a30441f28e9cbe87690d9e098b0e068cc395/inst/examples/adp-intro.md">adp-intro</a> file of my nonparametric-bayes repo, see there for the <a href="https://github.com/cboettig/nonparametric-bayes/blob/master/inst/examples/adp-intro.md">most recent</a> (or earlier) versions.</p>

<h2 id="setup-my-sample-problem">Setup my sample problem</h2>

<p>First we define the Beverton-Holt stock-recruitment relationship as a function of stock size <code>x</code>, harvest <code>h</code> and parameters <code>p</code></p>

<pre><code class="language-r">f &lt;- function(x, h, p){
    A &lt;- p[1] 
    B &lt;- p[2] 
    s &lt;- pmax(x-h, 0)
    A * s/(1 + B * s)
}
p &lt;- pars &lt;- c(1.5, 0.5)
K &lt;- (p[1] - 1)/p[2]
sigma_g &lt;- 0.2
</code></pre>

<p>We begin with a simulation method $X_{t+1} = f(X_t, Z_t)$.  For illustration, let us consider $f(X_t, Z_t) = Z_t \frac{a X_t}{b + X_t}$ with a = 1.5 and b = 0.5.  We define a statespace $S$</p>

<pre><code class="language-r">S &lt;- seq(0, 1, length=11) 
</code></pre>

<p>as a uniform grid of 11 points from 0 to 1.<br />
We also need a value function on the state space, $C_t(S_t)$.
For simplicity, we set the price of harvest at unity and
the cost of harvesting at zero, so that $C_t(S_t, x_t) = \min(x_t, S_t)$.<br />
($C_t$ is sometimes denoted $\mathbb{\Pi}$).<br />
We also need an action space $\chi_t$ of possible harvest values.<br />
Again for simplicity we assume that harvest can be set to any possible state size, $\chi_t \equiv S_t$,</p>

<pre><code class="language-r">chi &lt;- S
</code></pre>

<pre><code class="language-r">T &lt;- 10
N &lt;- 10
</code></pre>

<p>The approximate dynamic programming algorithm will perform a finite number $N$ = 10 iterations over a window of time $T$ =10 in our example.  The algorithm can then be described as follows:</p>

<h2 id="algorithm-1">Algorithm (1)</h2>

<ul>
<li><p><strong>Step 0</strong></p>

<ul>
<li>Initialize some value $\tilde V_t^0(S_t)$ for all states $S_t$,
where the superscripts denote iterations in the forward approximation.<br />
As we know absolutely nothing yet to base our initial guess on,
we just arbitrarily set this to zero.<br /></li>
</ul></li>
</ul>

<pre><code class="language-r">V &lt;- numeric(length(S))
</code></pre>

<ul>
<li>Choose some initial state $S_0^1$
We start at some initial state for $n = 1$ (superscript) and $t = 0$ (subscript).
The choice of initial condition may come from the problem itself,
otherwise we choose something arbitrarily.<br /></li>
</ul>

<pre><code class="language-r">S_0 &lt;- 0.5
</code></pre>

<ul>
<li><p>Set $n = 1$</p></li>

<li><p><strong>Step 1</strong>: Choose a sample path, $\omega^n$ (a vector of random draws)</p></li>
</ul>

<pre><code class="language-r">sigma &lt;- 0.2
omega_n &lt;- rlnorm(T, 0, sigma)
</code></pre>

<ul>
<li><p>** Step 2**: For $t = 0, 1, 2, \ldots, T$, do:</p>

<ul>
<li>Solve:</li>
</ul></li>
</ul>

<p>$$V_t(S<em>t) = \max</em>{x_t \in \chi_t} \left(C(S_t, x<em>t) + \gamma \sum</em>{s^{\prime} \in \mathcal{S}} \mathbb{P}(s^{\prime} | S_t^n, x<em>t) V</em>{t+1}^{n-1} s^{\prime} \right)$$</p>

<p>That is, choose action $x_t$ that maximizes the value of the next step.</p>

<p>Let&rsquo;s start with $t=0$, $n=1$ and fix an $x_0$ from the set of $\chi$
(allowing the action space to be the same in each period, we can omit
the subscript on $\chi$) to get started.  We first compute $C(S_0, x_0)$.</p>

<p>$S_0 = S_0^1$ which we fixed in step <strong>0b</strong> arbitrarily at 0.5.</p>

<p>The profits/costs $C(S_t, x_t)$ are the value derived by action (harvest) $x_t$
at state (stock) $S_t$.  Assuming a fixed price and no costs to harvesting,
this is just whichever number is smaller (since we cannot harvest more than
the available stock,</p>

<pre><code class="language-r">C &lt;- function(S, X) pmin(S, X)
</code></pre>

<p>(where we have used R&rsquo;s vectorized form of the min function).</p>

<p>This forward dynamic programming will still rely on the one-step transition
matrix, $\mathbb{P}$.</p>

<p>Let&rsquo;s get the transition matrices for this problem, assuming log-normal noise,</p>

<pre><code class="language-r">sdp_matrix &lt;- determine_SDP_matrix(f, p, x_grid=S, h_grid=chi, sigma_g)
</code></pre>

<p>Which is a list of matrices, one for each harvest (action) $x_t$.</p>

<p>Then we want to consider a fixed $S_t^n$ and fixed $x_t$, and take the sum of
$\mathbb{P}(s^{\prime} | S_t^n, x_t)$ over the $s^{\prime}$, which means we want
the $x_t$ element from the list, and then we need sum over the distribution of future
states given the current state $S<em>t^n$, e.g. a row of the matrix, e.g. <code>sdp_matrix[[x]][s,]</code>,
which we (vector) multiply by $V</em>{t+1}^{n-1}(s^{\prime})$.</p>

<p>This value $V$ is of course unknown, other than our initial random guess $V_{t}^0$.<br />
As we step through the iterations $V_t^1$, $V_t^2$, $V_t^3$, etc., this should convgerge to
something meaningful.</p>

<p>Note that the index along $S$ corresponding to $S_t^n$ is given by</p>

<pre><code class="language-r">s &lt;- which.min(abs(S-S_0))
</code></pre>

<p>So our maximization across $x$ just involves:</p>

<pre><code class="language-r">values &lt;- 
  sapply(1:length(chi), function(x)
    C(S[s], chi[x]) + sdp_matrix[[x]][s,] %*% V
)

max_x &lt;- which.max(values)
v_hat &lt;- max(values)
</code></pre>

<p>Trivially, this is just the harvest level that maximizes $C$ so far (which
is just harvesting the $S_0$, since $\bar V^0_t$ begins at zero:</p>

<pre><code class="language-r">chi[max_x]
</code></pre>

<pre><code>[1] 0.5
</code></pre>

<ul>
<li>step <strong>2b</strong> We can now update our $\bar V^0_t$ to get $\bar V^1_t$, using the rule:</li>
</ul>

<p>$$V_t^n(S_t) = \begin{cases}
\hat v_t^n &amp; S_t = S_t^n <br />
\bar V_t^{n-1}(S_t) &amp; \textrm{otherwise}
\end{cases}$$</p>

<p>e.g. use our maximum value for the case of the state we just considered $S_t = S_t^n$, otherwise leave $V_t$ unchanged.  Our new $V$ is thus:</p>

<pre><code class="language-r">V[s] = v_hat
</code></pre>

<ul>
<li>step <strong>2c</strong>  Compute $S^n_{t+1} = S^M(S_t^n, x^n<em>t, W</em>{t+1}(\omega^n))$</li>
</ul>

<p>We compute the next state using our <code>max_x</code> for $x^n_t$, our random samples
and the transition function&hellip;</p>

<pre><code class="language-r">S_1 &lt;- omega_n[1] * f(S_0, chi[hat[&quot;x_nt&quot;]], p)
</code></pre>

<ul>
<li><strong>Step 3</strong> Let $n = n+1$. if $n &lt; N$, go to step 1</li>
</ul>

<h2 id="putting-this-all-together-as-a-recursive-algorithm">Putting this all together as a recursive algorithm</h2>

<pre><code class="language-r">N &lt;- 5000 # iterations
M &lt;- 20 # gridsize 
Tmax &lt;- 5 # Time horizon

gamma &lt;- 0.95 # Discount
# f (defined above) 
# p  (defined above)

sigma_g &lt;- 0.5 # larger variation in random draws helps
chi &lt;- seq(0, 1, length.out = M)
S &lt;- seq(0, 1, length.out = M)

sdp_matrix &lt;- determine_SDP_matrix(f, p, x_grid=S, h_grid=chi, sigma_g)

V &lt;- matrix(1, M, Tmax)  # A* strategy
# Fails to explore at matrix(0, M, Tmax)
# consider: # V &lt;- matrix(rep(chi, Tmax), nrow=M) # 
# V[,1] &lt;- chi   # fails to explore if it doesn't have at least some non-zero values

C &lt;- function(S, X) pmin(S, X)
S_0 &lt;- 0.5 
alpha &lt;- 1 # learning rate


for(n in 1:N){
  
  omega_n &lt;- rlnorm(Tmax, 0, sigma_g)
  S_current &lt;- S_0 #runif(1,0,1) # explores faster when this is random

  for(t in 1:Tmax){
    # index of the state we're considering
    s &lt;- which.min(abs(S-S_current)) 
    
    # Find the action maximizing the value
    values &lt;- sapply(1:length(chi), function(x)
      C(S[s], chi[x]) + gamma * sdp_matrix[[x]][s,] %*% V[,t])
    hat &lt;-  c(x_nt = which.max(values), v_nt = max(values))

    # Update value V as mixture of new value and previous value
    V[hat[&quot;x_nt&quot;], t] &lt;- (1 - alpha) * V[hat[&quot;x_nt&quot;],t] + alpha * hat[&quot;v_nt&quot;] 
    
    # Advance the state in time along random path  
    S_current &lt;- omega_n[t] * f(S_current, chi[hat[&quot;x_nt&quot;]], p)

  }
}
</code></pre>

<p>for comparison: the SDP solution</p>

<pre><code class="language-r">opt &lt;- find_dp_optim(sdp_matrix, S, chi, 70, 0.5, C, 1-gamma, reward=0)
opt$V
</code></pre>

<pre><code> [1] 0.000 7.173 7.375 7.496 7.584 7.653 7.710 7.760 7.810 7.860 7.910
[12] 7.960 8.010 8.060 8.110 8.160 8.210 8.260 8.310 8.360
</code></pre>

<h3 id="problems-arising-from-the-discretization">Problems arising from the discretization</h3>

<p>Note that after the first iteration, $n=1$, the value matrix $V$ is no
longer all zeros.  There is a single state, $S = S_0 =$ 0.5, at which we
have value.  That value is lost if we set harvest $x$ too high, since
we know we will not then end up in that state &ndash; from whence comes the
incentive to consider future value.  Unfortunately, the value exists
only if we hit that state exactly &ndash; all other states are assumed to
have zero value still.</p>

<h3 id="additional-problems">Additional problems</h3>

<p>We no longer have the loop-over-all-states problem, but we face several
new or remaining issues:</p>

<ol>
<li><p>We still require the use of the one-step transition matrix, with
the equally troublesome sum over all states
$\sum_{s^{\prime}\in S} \mathbb{P}(s^{\prime} | S_t^n, x_t)$.<br />
We will fix this by approximating
the transitions in step 2b using random draws as well.</p></li>

<li><p>We only update the values of states we visit.  We still need a way
to estimate the value of states we have not visited.</p></li>

<li><p>Worse, we might not visit states that seem bad relative to states we
have visited. This is particularly atrocious in this example.  Since we
initialize the value of all states at 0, the algorithm prefers to harvest
all stock from the current state rather than risk a transition into a
state starting at 0. There is no convergence guarentee that we will ever
escape this cycle of avoiding states we have not seen. We can alter the
initial guess of the value of course, and we could alter the starting
condition to better explore.</p></li>
</ol>

<h2 id="extensions">Extensions</h2>

<p>The rest of the ADP development is designed to tackle each of these
issues.  This algorithm gives very poor performance, but very flexible
skeleton on which to extend features that have made ADP such a successful
approach for impossibly large problems.</p>

<h3 id="stochastic-value-function-sampling">Stochastic value function sampling</h3>

<p>Dealing with problem 1:</p>

<p>$$V_t(S<em>t) = \max</em>{x_t \in \chi_t} \left(C(S_t, x<em>t) + \gamma \sum</em>{\hat \omega \in \hat \Omega^n_{t+1}} p<em>t+1(\hat \omega) \bar V</em>{t+1}^{n-1} (S_{t+1}) \right)$$</p>

<pre><code class="language-r">    V[hat[&quot;x_nt&quot;], t] &lt;- (1 - alpha) * 
                         V[hat[&quot;x_nt&quot;],t] + 
                          alpha * hat[&quot;v_nt&quot;] 
</code></pre>

<h3 id="decreasing-the-state-space-size">Decreasing the state space size</h3>

<ul>
<li>Aggregation</li>
<li>Continuous Value function approximations</li>
<li>Using the post-decision state variable (improves dealing with the expectation calc)</li>
</ul>

<h3 id="initialization-problem">Initialization problem</h3>

<ul>
<li>We do not explore if we are too pessimistic about value of visiting other states.  Start optimisitc: AI&rsquo;s A* alogrithm (synchronous)</li>
<li>Asynchronous updating &ndash; randomly sampling starting variables</li>
<li>RTDP (Real Time Dynamic Programming &ndash; not necessarily what it sounds like) external rule determines which states we visit</li>
</ul>

<h3 id="learning">Learning</h3>

<ul>
<li>The concepts of learning and the trade-off between exploration and exploitation are already built-in to the forward algorithm.<br /></li>
</ul>

<h3 id="using-non-stochastic-transition-information-only-step-2b-can-be-written-as">Using Non-stochastic transition information only, step <strong>2b</strong> can be written as:</h3>

<p>Taking $x_0$ as the smallest harvest, $\min(\chi)$ = 0 and
evaluating $C(S_0,X_0) = \min(S_0, X_0)$ gives us 0,
rather trivially.<br />
The next terms depend on the value $\tilde V^0_1(s^{\prime})$ for
all $s^{\prime} \in S$, which we have no idea about.  Fortunately we have
assumed a value for each of these in step 0a.</p>

<p>We must also come up with some values for the probability
$\mathbb{P}(s^{\prime} | S_1^0, x_1)$ for each state, given our current
state $S_1^0$ and considered action $x_1$.  This is more straight forward,
since it is determined by our one-step transition function (without
simulation - recall that the single step transition is given exactly).</p>

<p>To do so, we evaluate the argument for each value in our action
space, $x_t \in \chi_t$,</p>

<pre><code class="language-r">s &lt;- S_0
C &lt;- function(S, X) pmin(S, X)
arg &lt;- sapply(chi, function(x) C(s, x) + f(S, x, p) %*% V)
x_nt = which.max(arg)
v_nt = max(arg)
V[x_nt] = v_nt
</code></pre>

    </article>
  </div>
  <div class="col-md-4">
    <div class="sidebar">
      <aside prefix="og:http://ogp.me/ns/article#">
  
  
  
<div class="article-metadata">

  <p>
  <span class="article-date">
    <i class="fa fa-calendar"></i>
    
    <time datetime="2013-05-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Mon, May 27, 2013
    </time>
    
  </span>
  </p>
    

  <p>
  
  
  
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="../../../../categories/ecology">ecology</a
    >
    
  </span>
  
  
  </p>
  
  <p>
  
  
  
  <span class="article-tags">
    <i class="fa fa-tags"></i>
    
    <a href="../../../../tags/nonparametric-bayes">nonparametric-bayes</a
    >, 
    
    <a href="../../../../tags/algorithms">algorithms</a
    >
    
  </span>
  
  
  </p>

</div>

 
  <br />
  
  <p><a class="btn btn-default" rel="prev" href='../../../../2013/05/28/notes/'><i class="fa fa-chevron-left"></i> prev</a>
    <a class="btn btn-default" rel="next" href='../../../../2013/05/24/notes/'>next <i class="fa fa-chevron-right"></i></a></p>

  <br />

  <p> <a class="btn btn-default" 
         href="https://github.com/cboettig/cboettig.github.io/commits/master/content/lab-notebook/2013-05-27-exploring-approximate-dynamic-programming-approaches.md"><i class="fa fa-clock-o"></i> history</a></p>

  <p> <i class="fa fa-barcode"></i> SHA: <a href="https://github.com/cboettig/cboettig.github.io/blob/9d9072923c46d635718a4624d6afff6b1ec2439f/content/lab-notebook/2013-05-27-exploring-approximate-dynamic-programming-approaches.md">9d9072923</a></p> 


</aside>

    </div>
  </div>
</div>


<section id="comments">
  <div id="disqus_thread">
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'cboettig';
    var disqus_identifier = '\/2013\/05\/27\/exploring-approximate-dynamic-programming-approaches\/';
    var disqus_title = '';
    var disqus_url = '\/2013\/05\/27\/exploring-approximate-dynamic-programming-approaches\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>
</section>

<footer class="site-footer">
  <hr />
  <div class="container">

  <div class="row">
    <div class="col-md-3 col-xs-4 socialicons" style="font-size:20px" typeof="foaf:Person" about="https://carlboettiger.info/#me">
      <p>
      <script type="text/javascript" src="../../../../js/obfuscate-email-link.js"></script> 

          <a rel="foaf:account" href="https://twitter.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); 
             return false;"><span class="showtooltip" title="follow me on twitter (reading, discussing)"><i class="fa fa-twitter"></i></span></a> 

          <a rel="foaf:account" href="https://github.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); 
             return false;"><span class="showtooltip" title="follow me on Github (code, research)"><i class="fa fa-github"></i></span></a>
           <a rel="foaf:weblog" type="application/atom+xml" href="../../../../index.xml"  
        class="showtooltip" title="RSS feeds for my blog-style entries." 
         onclick="recordOutboundLink(this, 'Outbound Links', 'RSS'); 
         return false;"><i class="fa fa-rss"></i></a>
       </p>
    </div>

    
    


    <div class="col-md-4 col-md-offset-1 col-xs-4">
	    <p><a onclick="recordOutboundLink(this, 'Outbound Links', 'ONS_claim'); return false;" href="http://onsclaims.wikispaces.com/"><img src="../../../../img/ons-aci2-icon.svg" alt="ONS" class="showtooltip" title="An Open Notebook Science (ONS) project claim: Entry provides all content (AC) immediately (I) or without significant delay.  See link for details"/></a></p>
    </div>


    <div class="col-md-3 col-md-offset-1 col-xs-4">
      <p>
      <a rel="license" property="http://creativecommons.org/ns#license" href="http://creativecommons.org/publicdomain/zero/1.0/" onclick="recordOutboundLink(this, 'Outbound Links', 'CC0'); return false;"><img src="../../../../img/cc-zero.svg" alt="CC0"/></a> 
      </p>
    </div>
  </div>
</footer>


    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="../../../../js/jquery-1.12.3.min.js"></script>
    <script src="../../../../js/bootstrap.min.js"></script>
    <script src="../../../../js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="../../../../js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-18401403-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

 
