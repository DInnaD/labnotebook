<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head prefix="dc: http://purl.org/dc/terms/ og: http://ogp.me/ns#"> <!-- namespaces used in metadata.html -->
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8'/>
  <title>Lab Notebook</title>
  <meta name="author" content="Carl Boettiger" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- HTML5 metadata -->
<meta name="keywords" content="" />
<meta name="description" content="" />
<!-- RDFa Metadata (in DublinCore) -->
<meta property="dc:title" content="Lab Notebook" />
<meta property="dc:creator" content="Carl Boettiger" />
<meta property="dc:date" content="" />
<meta property="dc:format" content="text/html" />
<meta property="dc:language" content="en" />
<meta property="dc:identifier" content="/lab-notebook.html" />
<meta property="dc:rights" content="CC0" />
<meta property="dc:source" content="Lab Notebook" />
<meta property="dc:subject" content="Ecology" /> 
<meta property="dc:type" content="website" /> 
<!-- RDFa Metadata (in OpenGraph) -->
<meta property="og:title" content="Lab Notebook" />
<meta property="og:author" content="http://www.carlboettiger.info/index.html#me" />  <!-- Should be Liquid? URI? -->
<meta property="http://ogp.me/ns/profile#first_name" content="Carl"/>
<meta property="http://ogp.me/ns/profile#last_name" content="Boettiger"/>
<meta property="http://ogp.me/ns/article#published_time" content="" />
<meta property="og:site_name" content="Lab Notebook" /> <!-- Same as dc:source? -->
<meta property="og:url" content="http://www.carlboettiger.info/lab-notebook.html" />
<meta property="og:type" content="website" /> 
<!-- Google Scholar Metadata -->
<!--
<meta name="citation_author" content="Carl Boettiger"/>
<meta name="citation_date" content=""/>
<meta name="citation_title" content="Lab Notebook"/>
<meta name="citation_journal_title" content="Lab Notebook"/>
-->
<!--NOTE: see also the COinS Metadata in span element in footer -->




  <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!-- Help the browser identify the RSS feed automatically -->
  <link rel="alternate" type="application/rss+xml" title="Carl Boettiger's Lab Notebook" href="/blog.xml" />
</head>


  <body prefix="dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/"> 
    <!-- Navbar  ================================================== -->

<nav class="navbar navbar-default" role="navigation">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/README.html"><i class="icon-info-sign"></i></a>
    </div>

 <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

          <li  >
          <a href="/index.html">Home</a></li>
          <li  >
          <a href="/vita.html">Vita</a></li>
          <li  >
          <a href="/research.html">Research</a></li>
          <li  >
          <a href="/teaching.html">Teaching</a></li>
          <li  >
          <a href="/community.html">Community</a></li>
          <li  class="active" >
          <a href="/lab-notebook.html">Lab Notebook</a></li>

        </ul>

      <!-- Search site using Google's index -->
        <form class="navbar-form navbar-right" role="search" method="get" action="http://google.com/search">
          <div class="form-group">
            <input type="hidden" name="q" value="site:carlboettiger.info" />
            <input type="text" class="form-control search-query" name="q" placeholder="Search"/>
          </div>
          <button class="btn btn-mini" type="submit"><i class="icon-search"></i></button> 
       </form>

    </div><!--/.nav-collapse -->
  </div> <!-- /container -->
</nav>



    <div class="container"> <!-- Responsive grid layout, doesn't jump to full-width --> 
      <header>
        <h1 class="entry-title">Lab Notebook</h1>
        <h2>(<a href="http://www.carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html">Introduction</a>)</h2>
      </header>

      <div class="row feed">
  <div class="col-md-3 col-md-offset-1">
    <h4>  <a property="account" href="https://github.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); return false;"><i class="icon-github" alt="github"></i> Coding </a></h4> 
    <div class="excerpt">
      <div class="scroll">
        <ul><li>cboettig pushed to master at cboettig/labnotebook: <em>additional setting of user added</em> <a href="https://github.com/cboettig/labnotebook/compare/2d43ca1b65...76b154b0d0">11:34 2014/09/23</a></li><li>cboettig pushed to gh-pages at cboettig/labnotebook: <em>Updating to cboettig/labnotebook@2d43ca1.</em> <a href="https://github.com/cboettig/labnotebook/compare/81d5e5ebc5...a4774ed9f1">11:10 2014/09/23</a></li><li>cboettig pushed to master at cboettig/labnotebook: <em>new posts</em> <a href="https://github.com/cboettig/labnotebook/compare/471918601d...2d43ca1b65">11:01 2014/09/23</a></li><li>cboettig pushed to master at cboettig/dockerfiles: <em>final edits</em> <a href="https://github.com/cboettig/dockerfiles/compare/0377adb288...92886b2ec8">07:52 2014/09/23</a></li><li>cboettig commented on issue cboettig/knitcitations#66: <em>This is because CRAN likes to add the date automatically rather than rely on a manually specified date. You'll only see this if using the Github co…</em> <a href="https://github.com/cboettig/knitcitations/issues/66#issuecomment-56549923">04:37 2014/09/23</a></li></ul>
      </div>
    </div>
  </div>
  <div class="col-md-3">
    <h4> <a property="account" href="https://twitter.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); return false;"><i class="icon-twitter"></i> Discussing </a></h4> 
     <div class="excerpt">
      <div class="scroll">
       <ul><li><p>@ematsen @xieyihui @recology_ then @phylorich turned me on to <a href="https://t.co/HuvvfFFZBD">https://t.co/HuvvfFFZBD</a> , which is how I think CI should really work</p>
 <a href="http://twitter.com/cboettig/statuses/514243221287751680">02:41 2014/09/23</a> </li><li><p>@ematsen @xieyihui @recology_ Yeah, more-or-less Craig&#39;s r-travis is the way to go. If you&#39;re up for hosting your own system though [1/2]</p>
 <a href="http://twitter.com/cboettig/statuses/514242917490106370">02:40 2014/09/23</a> </li><li><p>@recology_ thanks, G+ ate my link. I think I&#39;m incapable of using G+ successfully, maybe I should just stick to twitter</p>
 <a href="http://twitter.com/cboettig/statuses/514141523848331264">07:57 2014/09/22</a> </li><li><p>&quot;On Age and Species Richness of Higher Taxa&quot; from an all-star author team <a href="http://t.co/XloQARFQYt">http://t.co/XloQARFQYt</a> / (now w/ link) / @recology_</p>
 <a href="http://twitter.com/cboettig/statuses/514141190757707776">07:56 2014/09/22</a> </li><li><p>&quot;On Age and Species Richness of Higher Taxa&quot; from an all-star author team</p>
 <a href="http://twitter.com/cboettig/statuses/514130269738004480">07:13 2014/09/22</a> </li></ul>
      </div>
    </div> 
  </div> 
  <div class="col-md-3">
    <h4> <a href="http://www.mendeley.com/groups/634301/theoretical-ecology/papers/" onClick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); return false;"><i class="icon-book"></i> Reading </a></h4> 
    <div class="excerpt">
      <div class="scroll">
<ul><li>Collapse of an ecological network in Ancient Egypt: Pages: 1-6. Justin D Yeakel, Mathias M Pires, Lars Rudolf, Nathaniel J Dominy, Paul L Koch et al. <a href="http://www.mendeley.com/c/7145752934/g/634301/yeakel-2014-collapse-of-an-ecological-network-in-ancient-egypt/">07:35 2014/09/11</a></li><li>Regime shifts in models of dryland vegetation Regime shifts in models of dryland vegetation: Yuval R Zelnik, Shai Kinast, Hezi Yizhaq, Golan Bel, Ehud Meron, Phil Trans R Soc A et al.Published using Mendeley: The bibliography manager for researchers <a href="http://www.mendeley.com/c/7145752914/g/634301/zelnik-2013-regime-shifts-in-models-of-dryland-vegetation-regime-shifts-in-models-of-dryland-vegetation/">07:35 2014/09/11</a></li><li>Temporal ecology in the Anthropocene: Ecology Letters (2014). Pages: n/a-n/a. E. M. Wolkovich, B. I. Cook, K. K. McLauchlan, T. J. Davies et al. <a href="http://www.mendeley.com/c/7145752874/g/634301/wolkovich-2014-temporal-ecology-in-the-anthropocene/">07:35 2014/09/11</a></li><li>Uncertainty , learning , and the optimal management of wildlife: Environmental and Ecological Statistics (2001). Volume: 8, Issue: 3. Pages: 269-288. Byron K. Williams et al. <a href="/catalog/uncertainty-learning-optimal-management-wildlife/">08:57 2014/08/06</a></li></ul>
      </div>
    </div>
  </div> 
</div>

<hr>
<div class="row postpreview">
  <div class="col-md-11 col-md-offset-1">
    <div class="row">
      <h4> <a href="http://www.carlboettiger.info/atom.xml"
              onClick="recordOutboundLink(this,
              'Outbound Links', 'RSS'); return false;"
              style="color: inherit;"
              ><i class="icon-rss" ></i> Entries</a></h4>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/09/22/containerizing-my-development-environment.html">Containerizing My Development Environment</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 22 Sep 2014</p>

<article>
<div class="excerpt">
<p>A key challenge for reproducible research is developing solutions that integrate easily into a researcher’s existing workflow. Having to move all of one’s development onto remote machines, into a particular piece of workflow software or IDE, or even just constrained to a window running a local virtual machine in an unfamiliar or primitive environment isn’t particularly appealing. In my experience this doesn’t reflect the workflow of even those folks already concerned about reproducibility, and is, I suspect, a major barrier in adoption of such tools.</p>
<p>One reason I find docker particularly attractive for reproducible research it the idea of containerizing my development environment into something I can transport or recreate identically anywhere, particularly on another Linux machine. This also provides a convenient backup system for my development environment, no need to remember each different program or how I installed or configured it when moving to a new machine.</p>
<h2 id="using-aliases">Using aliases</h2>
<p>For me, a convenient way to do this involves creating a simple alias for running a container. This allows me to distinguish between running any software and the container, while managing my files and data through my native operating system tools. I’ve set the following alias in my <code>bashrc</code>.</p>
<pre><code>alias c=&#39;docker run --rm -it -v $(pwd):/home/$USER/`basename $PWD` -w /home/$USER/`basename $PWD` -e HOME=$HOME -e USER=$USER --user=$USER strata&#39;</code></pre>
<p>I can then just do <code>c R</code> (think <code>c</code> for container) to get R running in a container, <code>c bash</code> to drop into a bash shell on the container, <code>c pandoc --version</code> echoes the version of pandoc available on our container (or otherwise execute the container version of pandoc), and so forth.</p>
<h3 id="explanation">explanation</h3>
<p>The break down of each of these arguments is as follows:</p>
<ul>
<li><code>--rm</code> remove this container when we quit, we don’t need to let it persist as a stopped container we could later return to.</li>
<li><code>-it</code> Interactive terminal</li>
<li><code>-v</code> binds a host volume to the container. Files on the host working directory (<code>pwd</code>) will be available on the container, and changes made on the container are immediately written to the host directory:</li>
</ul>
<pre><code>-v $(pwd):/home/$USER/`basename $PWD`</code></pre>
<p>The path after the colon specifies where this directory should live on the container: we specify in a directory that has the same name as the current working directory <code>basename $PWD</code>, located in the home directory of the user (e.g. where the user has write permissions).</p>
<ul>
<li><code>-w</code> specifies the working directory we should drop into when our session on the container starts. We set this to match the path where we have just mounted our current working directory:</li>
</ul>
<pre><code>-w /home/$USER/`basename $PWD`</code></pre>
<ul>
<li><p><code>-e HOME=$HOME</code> sets the value of the environmental variable <code>HOME</code> to whatever it is on the host machine (e.g. <code>/home/username</code>), so that when R tries to access <code>~/</code>, it gets the user’s directory and not the root directory.</p></li>
<li><p><code>-e USER=$USER</code> though this seems redundant, we set the user environmental variable by default in the cboettig/rstudio image, so this overrides that environmental variable with the current user.</p></li>
<li><p><code>--user=$USER</code> Specifies the user we log in as. This is rather important, otherwise the we find that we are the root (or whatever user has been set in the Dockerfile). That would cause any files we generate from the container to be owned by the root user, not our local user. Note that this only works if the specific user has already been created (e.g. by <code>adduser</code>) on the container, otherwise this will fail.</p></li>
<li><p><code>strata</code> the name of the container (could be <code>cboettig/ropensci</code>, but my <code>strata</code> image provides a few additional customizations, created by <a href="">it’s own Dockerfile</a>. That Dockerfile (and its FROM dependencies) specify all the software available on this container. Importantly, it also already creates my username in it’s Dockerfile. Otherwise, the argument given above should use <code>--user=rstudio</code>, since the <code>rstudio</code> user is already created by the base image <code>cboettig/rstudio</code>, and thus available in <code>cboettig/ropensci</code> and <code>strata</code>. Note that this user can be created interactively by passing the environmental variable <code>-e USER=$USER</code> when running in deamon mode, since the user is then created by the start-up script. However, when we provide a custom command (like <code>/usr/bin/R</code> in this example, the <code>CMD</code> from the Dockerfile is overriden and the user isn’t created.</p></li>
</ul>
<p>A stricter alias I considered first enforces running R as a container rather than a local operation:</p>
<pre><code>alias R=&#39;docker run --rm -it -v $(pwd):/home/$USER/`basename $PWD` -w /home/$USER/`basename $PWD` -e HOME=$HOME --user=$USER strata /usr/bin/R&#39;</code></pre>
<h2 id="why-not-separate-containers-per-application">Why not separate containers per application?</h2>
<p>A more natural / more docker-esque approach might simply be to have separate containers for each application (R, pandoc, etc). This idealism belies the fact that I already need many of these tools installed on the same container, as they regularly interact in a deep way (e.g. R packages like <code>rmarkdown</code> already depend on pandoc), so these should really be thought of as a single development environment.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/09/22/containerizing-my-development-environment.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/09/09/server-backups.html">Server Backups</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 09 Sep 2014</p>

<article>
<div class="excerpt">
<h2 id="digital-ocean-snapshots">Digital Ocean Snapshots</h2>
<p>At $0.02 per gig per month, this looks like this is the cheapest way to make complete backups.</p>
<p>The process is rather manual: we have to <code>sudo poweroff</code> the droplet and then trigger the snapshot (the container will come back online after that, though we have to restart the services / active docker containers). We also have to delete old snapshots manually. Some of this can be automated from the API. DigitalOcean uses redundant storage for these (paying $0.01/month/gigabyte to Amazon Glacier), but at the moment we can’t export these images. Snapshots are also handy to deploy to a larger (but not smaller) droplet.</p>
<h3 id="digital-ocean-backups">Digital Ocean Backups</h3>
<p>These backups are an automated, always-online alternative to snapshots but must me initialized when the droplet is created and cost more (20% of server cost).</p>
<h2 id="manually-configuring-backups">Manually configuring backups</h2>
<p>To have the flexibility to restore individual pieces, to move between machines, etc we need a different approach.</p>
<h3 id="container-backups">Container backups</h3>
<p>Docker containers, including running containers, should be effectively backed up by either of these approaches to the state we would be in after a power cycle (e.g. we may need to start stopped containers, but not rebuild them from scratch).</p>
<p>Nevertheless we may want to back up containers themselves. For many containers this is trivial (e.g. our ssh container): we can just commit the running container to an image and save that as a tar archive (or equivalently, just export the container to a tarball).</p>
<p>If the containers have a <code>VOLUME</code> command in their dockerfile or in their execution however, this is insufficient. Containers using volumes (such as <code>sameersbn/gitlab</code> and <code>mattgruter/drone</code>) need <a href="http://serverfault.com/questions/576490/docker-volume-backup-and-restore">four things</a> to be backed up:</p>
<ul>
<li>Dockerfile (or container image, from <code>save</code> or <code>commit</code>)</li>
<li>volume</li>
<li>volume path in container</li>
<li>name of the container the volume belongs to</li>
</ul>
<p>A <a href="https://github.com/discordianfish/docker-backup">utility</a> makes this easier.</p>
<h3 id="sparkleshare">Sparkleshare</h3>
<p>Sparkleshare is a git-backed dropbox alterantive. With binaries for most major platforms (Windows, Mac, Ubuntu/Linux) it’s pretty easy to set up and acts in much the same way, with automated synch and notifications. The backend just needs a server running git – Gitlab is a great way to set this up to permit relatively easy sharing / user management. (Ignore the information about setting up separately on a server, Gitlab is much easier. Also ignore advice about building from source on Ubuntu, installing the binary is far more straight forward: <code>apt-get install sparkleshare</code>. Certainly it is not as feature rich as dropbox (e.g. email links to add users, web links to share individual files), but easy sharing over the server at no extra cost. The Sparkleshare directory is also a fully functional git repo.</p>
<h3 id="encrypted-backup-of-filesystem-with-duplicity">Encrypted backup of filesystem with duplicity</h3>
<p>See <a href="https://www.digitalocean.com/community/tutorials/how-to-use-duplicity-with-gpg-to-securely-automate-backups-on-ubuntu">Duplicity setup</a></p>
<p>Good for backing up to another host for which we have ssh access, or to an Amazon S3 bucket, etc. (Unclear if this works with Glacier due to upload-only et-up).</p>
<h3 id="some-other-rates-for-data-storage">Some other rates for data storage:</h3>
<ul>
<li>Compare to S3 ($0.03 /gig/month)</li>
<li>EBS ($0.12 /gig/month) (really for computing I/O, not storage).</li>
<li>Remarkably, Google Drive and Dropbox now offer 1 TB at $10 / mo. Clearly a lot can be saved by ‘overselling’ (most users will not use their capacity) and by shared files (counting against the space for all users but requiring no more storage capacity). Nonetheless, impressive, on par with Glacier (without the bandwidth charges or delay).</li>
<li>For comparison, (non-redundant, non-enterprise, disk-based) storage is roughly $100/TB, or on order of that annual cost.</li>
</ul>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/09/09/server-backups.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/09/08/server-security-basics.html">Server Security Basics</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 08 Sep 2014</p>

<article>
<div class="excerpt">
<h2 id="security-configuration">Security configuration</h2>
<p>We set up SSH key-only login on non-standard port, with root login forbidden. We then set up <code>ufw</code> firewall, <code>fail2ban</code>, and <code>tripwire</code>.</p>
<ol type="1">
<li>Configure an <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2">SSH key login</a>. Next, <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-12-04">Create a user, add to sudoers, and then disable root login.</a>. Edits <code>/etc/ssh/sshd_config</code>:</li>
</ol>
<ul>
<li>Disabling root logins. (We’ll need to add ourselves to sudo first: (<code>adduser</code>, edit <code>/etc/sudoers</code>)</li>
<li>Change ssh port from default to something else.</li>
<li>Whitelist user login ids</li>
</ul>
<p>Additionally, let’s be sure to disable password authentication: Add <code>PasswordAuthentication no</code> to <code>/etc/ssh/sshd_config</code>. (editing PermitRootLogin only doesn’t do this).</p>
<p>Locally add an entry in <code>~/.ssh/config</code> to alias the host and port to avoid having to remember these numbers for login. Run <code>ssh-copy-id &lt;droplet-ip&gt;</code> to enable key-based login for the user.</p>
<ol start="2" type="1">
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-setup-a-firewall-with-ufw-on-an-ubuntu-and-debian-cloud-server">Install and configure</a> <code>ufw</code> firewall. As we’re not using the default <code>ssh</code> port, we need to explicitly tell <code>ufw</code> which ssh port to allow.</li>
</ol>
<pre><code>sudo ufw allow &lt;PORT&gt;/tcp</code></pre>
<p>(The <code>/tcp</code> part is optional, saying only allow <code>tcp</code> protocol over that port, not other protocols.)</p>
<p>We must also tell ufw to <a href="http://docs.docker.com/installation/ubuntulinux/#docker-and-ufw">allow Docker</a>: In <code>/etc/default/ufw</code> change <code>DEFAULT_FORWARD_POLICY</code> to ACCEPT, then:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> ufw reload
<span class="kw">sudo</span> ufw allow 2375/tcp</code></pre>
<p>and similarly allow any ports we export for our various services (Gitlab, Drone, etc).</p>
<ol start="3" type="1">
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-protect-ssh-with-fail2ban-on-ubuntu-12-04">Install and configure</a> <code>fail2ban</code>. Prevents brute force password attacks. Be sure to assign the config to match chosen ssh port.</p></li>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-use-tripwire-to-detect-server-intrusions-on-an-ubuntu-vps">Install and configure</a> <code>tripwire</code> (intrusion detection).</p></li>
<li><p>Update software:</p></li>
</ol>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> apt-get -q update <span class="kw">&amp;&amp;</span> <span class="kw">sudo</span> apt-get -qy dist-upgrade</code></pre>
<p>and then also update tripwire log:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> tripwire --check --interactive</code></pre>
<p>Note: Clearly all these steps need to be running on the server itself, not merely in a container image deployed on server so that they are securing access to the actual host.</p>
<h2 id="additional-configuration">Additional configuration</h2>
<p>While we’re doing things, add user to the docker group for convenience: <code>sudo addgroup cboettig docker</code></p>
<p><a href="https://www.digitalocean.com/community/tutorials/how-to-add-swap-on-ubuntu-12-04">Enable swap</a> on a small instance. Here we set up 1GB of swap (setting swap at twice the available RAM is the recommended rule-of-thumb, though makes less sense once RAM is large)</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> fallocate -l 1G /swapfile
<span class="kw">sudo</span> chmod 600 /swapfile
<span class="kw">sudo</span> mkswap /swapfile
<span class="kw">sudo</span> swapon /swapfile</code></pre>
<p>To make this persistant on reboot edit <code>/etc/fstab</code>:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> echo <span class="st">&quot;/swapfile       none    swap    sw      0       0&quot;</span> <span class="kw">&gt;&gt;</span> /etc/fstab</code></pre>
<p>For better performance, we might tweak swappiness to 10 (default is 60 out of 100, where 0 is never swap and 1 is swap frequently):</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">echo</span> 10 <span class="kw">|</span> <span class="kw">sudo</span> tee /proc/sys/vm/swappiness
<span class="kw">echo</span> vm.swappiness = 10 <span class="kw">|</span> <span class="kw">sudo</span> tee -a /etc/sysctl.conf</code></pre>
<p>Set ownership</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> chown root:root /swapfile
<span class="kw">sudo</span> chmod 0600 /swapfile</code></pre>
<h2 id="server-modules">Server modules</h2>
<p>Running different services as their own docker containers offers serveral advantages:</p>
<ul>
<li><p>Containers often make it easier to install and deploy existing services, since the necessary configuration is scripted in the Dockerfile and we can often find Dockerfiles already made on <a href="http://hub.docker.com">Docker Hub</a> for common services. This note illustrates several examples.</p></li>
<li><p>Containers may provide an added level of stability, since they run largely in isolation from each other.</p></li>
<li><p>Containers can be <a href="http://stackoverflow.com/questions/16084741/how-do-i-set-resources-allocated-to-a-container-using-docker">resource limited</a>, e.g.</p></li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -it -m 100m -c 100 ubuntu /bin/bash</code></pre>
<p>would provide the container with 100 MB of RAM and 100 “shares” of CPU (acts kind of like a niceness, where the default share of a container is 1024. On multicore machines you can also pass <code>--cpuset &quot;0&quot;</code> or <code>--cpuset &quot;0,1&quot;</code> etc, which is a list of which cpus (numbered 0 to n-1, as in <code>/proc/cpuinfo</code>) the container is permitted to use.</p>
<p>As noted in the link, restricting disk space is more tricky, though might become easier down the road.</p>
<h3 id="ssh-server">ssh server:</h3>
<p>Permit users to ssh directly into a container rather than access the server itself. Despite its simplicity, I found this a bit tricky to set up correctly, particularly in managing users.</p>
<p>Here’s the basic Dockerfile for an image we’ll call <code>ssh</code>. This creates a user given by the environmental variable. A few tricks:</p>
<ul>
<li>We use <code>adduser</code> instead of <code>useradd</code> so that we get the home directory for the user created and granted the correct permissions automaticalliy. We need the <code>--gecos</code> information so that we’re not prompted to enter the user’s full name etc. We use <code>--disabled-password</code> rather than set a password here.</li>
<li>Login is still possible through ssh key (as well as through nsenter on the host machine). We go ahead and add the ssh key now, though this could be done after the container is running by using nsenter.</li>
<li>In this dockerfile, we’ve added the user to <code>sudo</code>ers group for root access on the container (installing software, etc). This won’t be active until the user has a password.</li>
</ul>
<pre><code>FROM     ubuntu:14.04
ENV USER cboettig
RUN apt-get update &amp;&amp; apt-get install -y openssh-server
RUN mkdir /var/run/sshd
RUN adduser --disabled-password --gecos &quot;&quot; $USER
RUN adduser $USER sudo
ADD authorized_keys /home/$USER/.ssh/authorized_keys
RUN chown $USER /home/$USER/.ssh/authorized_keys
EXPOSE 22
CMD    [&quot;/usr/sbin/sshd&quot;, &quot;-D&quot;]</code></pre>
<p>When building the image, note that a copy of <code>authorized_keys</code> (contains the contents of the <code>id_rda.pub</code> public key) file must be found in the same directory as the Dockerfile so that it can be added to the image.</p>
<p>Start the ssh server on port 2200:</p>
<pre><code>docker run -d -p 2200:22 --name=&quot;ssh&quot; ssh</code></pre>
<p>Add to the firewall permissions</p>
<pre><code>sudo ufw add 2200/tcp</code></pre>
<p>From here I can now ssh in from the computer housing the private key pair to the public key that is added to the image here. However, that user doesn’t have root access since we haven’t provided a password.</p>
<p>Use <code>nsenter</code> to enter the instance:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -v /usr/local/bin:/target jpetazzo/nsenter
<span class="kw">nsenter</span> -m -u -n -i -p -t <span class="kw">`docker</span> inspect --format <span class="st">&#39;&#39;</span> ssh<span class="kw">`</span> /bin/bash</code></pre>
<p>Create a password for the user to enable root access:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&#39;$USER:&lt;password&gt;&#39;</span> <span class="kw">|</span> <span class="kw">chpasswd</span></code></pre>
<p>We can create other users and add them to sudoers or not as desired, e.g. add interactively using:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">useradd</span> <span class="kw">&lt;</span>username<span class="kw">&gt;</span></code></pre>
<p>users can later change their passwords once they log in.</p>
<h2 id="restoring-containers-when-restarting">Restoring containers when restarting</h2>
<p>A certain times we need to power cycle the server (e.g. after certain software updates), using the <code>sudo reboot now</code> command or the DigitalOcean console. Any running containers will be automatically stopped. Once the machine is back up and we log back in, these containers can be brought back up with <code>docker restart &lt;container_id&gt;</code> (or <code>&lt;container_name&gt;</code>), and then everything is back to normal.</p>
<p>Note that while we can stop and restart a container, it seems we cannot simply save a container (e.g. with <code>docker commit</code> or <code>docker save</code> and re-run it and expect the server functionality to be restored after the container is destroyed (e.g. by <code>docker rm -f</code>). (See previous notes 2014-09-05) for an illustration of this problem. This occurs because the container image does not include the volume where it writes its data, and that volume address is generated uniquely each time a container is run.</p>
<p>Consequently, a different (probably more traditional) approach is needed to backup the configuration of a server such as Gitlab or Drone-CI even when running in a container. Will explore this later.</p>
<p>Meanwhile, remember remove unneeded containers with</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> rm <span class="ot">$(</span><span class="kw">docker</span> ps -a <span class="kw">|</span> <span class="kw">grep</span> Exited <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $1}&#39;</span><span class="ot">)</span></code></pre>
<p>and not with <code>-f</code> (destroying running containers!)</p>
<h2 id="key-security">Key security</h2>
<p>We can largely avoid needing a private ssh key for the server, though may use https authentication to let us use git (rather than, say, rsync) to develop and save changes made remotely (say, through RStudio server).</p>
<h3 id="backing-up-keys">Backing up keys</h3>
<p>Probably unnecessary to have a backup of the ssh private RSA key, as we can access the DigitalOcean Server or Github through the web consoles and add a new public key and replace our private key.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/09/08/server-security-basics.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/09/05/drone-ci-and-docker.html">Drone Ci And Docker</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 05 Sep 2014</p>

<article>
<div class="excerpt">
<h2 id="drone-ci-continous-integration-in-custom-docker-environments">Drone CI: Continous integration in custom docker environments</h2>
<p>Having gotten accustomed to Docker, configuring the appropriate build environment for a Continuous Integration system like Travis CI or Shippable CI starts to feel incredibly tedious and archaic (particularly if you work primarily in a language like R or haskell that usually isn’t supported out of the box).</p>
<ul>
<li>We do not have to hack together a custom image environment</li>
<li>We can build and test our environment locally instead of having to rely on trial-and-error pushes to the CI server</li>
<li>We do not have to download, compile and install the development environment each time, (which frequently takes longer than the CI checks themselves and can break)</li>
</ul>
<p>(Shippable provides a persistent environment too, by preserving the state of your ‘minion’. But unlike Shippable, I believe the Drone approach is unlikely that you can create troublesome side-effects in your environment, such as removing a necessary dependency from the <code>shippable.yml</code> and yet not catching it since the dependency is still available on the minion from before. In the Drone approach, we start on the same docker image each time, but merely avoid the few minutes it might take to download that image).</p>
<p>Unfortunately, custom images are not available on the fully hosted <a href="http://drone.io">drone.io</a> system. (Though perhaps they’d accept pull requests that would add an R environment to <a href="https://github.com/drone/images">their image library</a>). Fortunately, the Drone team kindly provides an <a href="https://github.com/drone.drone">open source version</a> of their platform that can be hosted on a self-hosted / private server (such as the new web darling DigitalOcean or Amazon’s EC2). This has other advantages as well – such as using privately hosted repositories (it also integrates with BitBucket and GitLab) or running very long tests / simulations (since we’re now paying for the server time ourselves, after all).</p>
<h2 id="the-easy-way-use-docker">The easy way: use docker</h2>
<p>We can deploy the Drone CI server somewhat more seamlessly by running it in a container itself. Rather than worry about the above configuration, we can simply launch an existing docker image for Drone, rather cleverly created by Matt Gruter:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run --name=<span class="st">&#39;drone&#39;</span> -d -p 8080:80 --privileged mattgruter/drone</code></pre>
<ul>
<li>Now we can follow the <a href="http://drone.readthedocs.org/en/latest/setup.html">setup instructions</a>. Be sure to use the matching case in the application name (<code>Drone</code> not <code>drone</code>) and the appropriate URLs for the authorization call back.</li>
</ul>
<p>Note that we must use a different port than 80, and that we must give this port explicitly in the Authorization callback URL: <code>http://localhost:8080/auth/login/github</code> in order to authenticate.</p>
<p>Also note that in this approach, the Drone CI’s docker image library will be separate from the docker image library. To manage or update the images available, we have to first <code>nsenter</code> into the Drone CI container.</p>
<p>This runs rather nicely on a tiny DigitalOcean droplet. Bare in mind that the tiny droplet has only 20 GB of disk space though, which can be consumed rather quickly with too many images. If many of the images use the same base templates, the total disk space required will fortunately be much lower than the sum of their virtual sizes.</p>
<h3 id="experimenting-with-saving-images">experimenting with saving images</h3>
<p>Being a docker image, we can snapshot and export it for later use, and meanwhile can even destroy our server instance.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> export drone <span class="kw">&gt;</span> dronedroplet.tar</code></pre>
<p>Not clear that this works. Consider saving an image instead? Save container named drone as image named drone:droplet</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> commit drone drone:droplet
<span class="kw">docker</span> save drone:droplet <span class="kw">&gt;</span> dronedroplet.tar</code></pre>
<p>are these identical?</p>
<p>Hmm, doesn’t seem to store configuration, login is no longer valid. Starting a stopped container maintains the configuration of course, but not launching from scratch (e.g. the sqlite database is local to the container, not accessible through an externally linked volume).</p>
<p>Note that this tarball does not include the Drone CI image library itself, which is not part of the container but rather connected as a volume. This makes it quite a bit smaller, and that library can presumably be reconstructed from the docker hub.</p>
<h2 id="configuring-drone-ci-the-hard-way">Configuring Drone CI: the hard way</h2>
<ul>
<li>Install and launch drone: (see <a href="https://github.com/drone/drone">drone/README</a>)</li>
<li>Add <code>DOCKER_OPTS=&quot;-H 127.0.0.1:4243 -d&quot;</code> to <code>/etc/default/docker</code></li>
<li>Kill the docker deamon and restart docker. Or run docker with the explicit binding:</li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> docker -d -H 127.0.0.1:4243 <span class="kw">&amp;</span></code></pre>
<ul>
<li>You may need to <a href="http://docs.docker.com/installation/ubuntulinux/#docker-and-ufw">configure firewall</a>, if ufw is running.</li>
<li>References: <a href="https://github.com/drone/drone/issues/149">drone/issues/149</a>, <a href="https://github.com/drone/drone/issues/24">drone/issues/24</a></li>
<li>Now we can follow the <a href="http://drone.readthedocs.org/en/latest/setup.html">setup instructions</a>. Be sure to use the matching case in the application name (<code>Drone</code> not <code>drone</code>) and the appropriate URLs for the authorization call back.</li>
</ul>
<h2 id="configuring-an-already-running-docker-session">Configuring an already-running docker session</h2>
<p>Launch a named repository in deamon mode:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -d -p 8787:8787 --name=<span class="st">&#39;drone&#39;</span> mattgruter/drone</code></pre>
<p>Use a docker-based install to add <code>nsenter</code> into your executable path:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -v /usr/local/bin:/target jpetazzo/nsenter</code></pre>
<p>Run <code>nsenter</code> to log into the docker image:</p>
<pre><code>nsenter -m -u -n -i -p -t `docker inspect --format &#39;{{ .State.Pid }}&#39; drone` /bin/bash</code></pre>
<p>Now we can update or delete images with <code>docker pull</code>, <code>docker rmi</code>, etc.</p>
<p>This is useful with many containers, for instance, with our ssh container or rstudio container we may want to modify usernames and passwords, etc:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">useradd</span> -m <span class="ot">$USER</span> <span class="kw">&amp;&amp;</span> <span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">$USER</span><span class="st">:</span><span class="ot">$PASSWORD</span><span class="st">&quot;</span> <span class="kw">|</span> <span class="kw">chpasswd</span></code></pre>
<h3 id="making-this-easier">Making this easier:</h3>
<p>Add to <code>.bashrc</code>:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">function</span><span class="fu"> dock</span> <span class="kw">{</span> <span class="kw">sudo</span> nsenter -m -u -n -i -p -t <span class="kw">`docker</span> inspect --format  <span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span><span class="kw">`</span> /bin/bash<span class="kw">;</span> <span class="kw">}</span></code></pre>
<p>This defines the function <code>dock</code> such that <code>dock &lt;name&gt;</code> will enter a running container named <code>&lt;name&gt;</code>. Note that we have to have <code>nsenter</code> bound to the executable path as indicated above. Yay less typing.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/09/05/drone-ci-and-docker.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/08/29/docker-notes.html">Docker tricks of the trade and best practices thoughts</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 29 Aug 2014</p>

<article>
<div class="excerpt">
<h2 id="best-practices-questions">Best practices questions</h2>
<p><em>Here are some tricks that may or may not be in keeping with best practices, input would be appreciated.</em></p>
<ul>
<li>Keep images small: use the <code>--no-install-recommends</code> option for <code>apt-get</code>, install true dependencies rather than big metapackages (like <code>texlive-full</code>).</li>
<li>Avoid creating additional AUFS layers by combining RUN commands, etc? (limit was once 42, but is now at least 127).</li>
<li><p>Can use <code>RUN git clone ...</code> to add data to a container in place of ADD, which invalidates caching.</p></li>
<li><p>Use automated builds linked to Github-based Dockerfiles rather than pushing local image builds. Not only does this make the Dockerfile transparently available and provide a link to the repository where one can file issues, but it also helps ensure that the image available on the hub gets its base image (<code>FROM</code> entry) from the hub instead of whatever was available locally. This can help avoid various out-of-sync errors that might otherwise emerge.</p></li>
</ul>
<h3 id="dockers-use-of-tags">Docker’s use of tags</h3>
<p>Unfortunately, Docker seems to use the term <code>tag</code> to refer both to the label applied to an image (e.g. in <code>docker build -t imagelabel .</code> the <code>-t</code> argument “tags” the image as ‘imagelabel’ so we need not remember its hash), but also uses <code>tag</code> to refer to the string applied to the end of an image name after a colon, e.g. <code>latest</code> in <code>ubuntu:latest</code>. The latter is the definition of “tags” as listed under the “tags” tab on the Docker Hub. Best practices for this kind of tag (which I’ll arbitrarily refer to as a ‘version tag’ to distinguish it) are unclear.</p>
<p>One case that is clear is tagging specific versions. Docker’s automated builds lets a user link a “version tag” to either to a branch or a tag in the git history. A “branch” in this case can refer either to a different git branch or merely a different sub-directory. Matching to a Git tag provides the most clear-cut use of the docker version-tag; providing a relatively static version stable link. (I say “relatively” static because even when we do not change the Dockerfile, if we re-build the Dockerfile we may get a new image due the presence of newer versions of the software included. This can be good with respect to fixing security vulnerabilities, but may also break a previously valid environment).</p>
<p>The use case that is less clear is the practice of using these “version tags” in Docker to indicate other differences between related images, such as <code>eddelbuettel/docker-ubuntu-r:add-r</code> and <code>eddelbuettel/docker-ubuntu-r:add-r-devel</code>. Why these are different tags instead of different roots is unclear, unless it is for the convenience of multiple docker files in a single Github repository. Still, it is perfectly possible to configure completely separate docker hub automated builds pointing at the same Github repo, rather than adding additional builds as tags in the same docker hub repo.</p>
<p>Docker linguistics borrow from git terminology, but it’s rather dangerous to interpret these too literally.</p>
<h2 id="keeping-a-clean-docker-environment">Keeping a clean docker environment</h2>
<ul>
<li><p>run interactive containers with <code>--rm</code> flag to avoid having to remove them later.</p></li>
<li><p>Remove all stopped containers:</p></li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> rm <span class="ot">$(</span><span class="kw">docker</span> ps -a <span class="kw">|</span> <span class="kw">grep</span> Exited <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $1}&#39;</span><span class="ot">)</span></code></pre>
<ul>
<li>Clean up un-tagged docker images:</li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> rmi <span class="ot">$(</span><span class="kw">docker</span> images -q --filter <span class="st">&quot;dangling=true&quot;</span><span class="ot">)</span></code></pre>
<ul>
<li>Stop and remove all containers (including running containers!)</li>
</ul>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> rm -f <span class="ot">$(</span><span class="kw">docker</span> ps -a -q<span class="ot">)</span></code></pre>
<h2 id="docker-and-continuous-integration">Docker and Continuous Integration</h2>
<ul>
<li><p>We can install but cannot run Docker on <a href="http://travis.org">Travis-CI</a> at this time. It appears the linux kernel available there is much too old. Maybe when they upgrade to Ubuntu 14:04 images…</p></li>
<li><p>We cannot run Docker on the docker-based <a href="http://shippable.com">Shippable-CI</a> (at least without a vagrant/virtualbox layer in-between). Docker on Docker is not possible (see below).</p></li>
<li><p>For the same reason, we cannot run Docker on <a href="http://drone.io">drone.io</a> CI. However, Drone provides an open-source version of it’s system that can be run on your own server, which unlike the fully hosted offering, permits custom images. Unfortunately I <a href="https://github.com/drone/drone/issues/54">cannot get it working</a> at this time.</p></li>
</ul>
<h2 id="docker-inside-docker">Docker inside docker:</h2>
<p>We cannot directly install docker inside a docker container. We can get around this by adding a complete virtualization layer – e.g. docker running in vagrant/virtualbox running in docker.</p>
<p>Alternatively, we can be somewhat more clever and tell our docker to simply use a different volume to store its AUFS layers. Matt Gruter has a <a href="https://github.com/mattgruter/dockerfile-doubledocker">very clever example</a> of this, which can be used, e.g. to run a Drone server (which runs docker) inside a Docker container (<a href="http://registry.hub.docker.com/u/mattgruter/drone/">mattgruter/drone</a>).</p>
<p>I believe this only works if we run the outer docker image with <code>--privileged</code> permissions, e.g. we cannot use this approach on a server like Shippable that is dropping us into a prebuilt docker container.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/08/29/docker-notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/08/14/pdg-controlfest-notes.html">Pdg Controlfest Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 14 Aug 2014</p>

<article>
<div class="excerpt">
<p>Just wanted to give a quick update on stuff relevant to our adjustment costs paper in events of this week.</p>
<p>I think the talk on Tuesday went all right, (though thanks to a technology snafu going from reveal.js to pdf my most useful figure actually showing the bluefin tuna didn’t display – I tried not to let on). I tried to keep the focus pretty big-picture throughout (we ignore these costs when we model, they matter) and avoid being too bold / prescriptive (e.g. not suggesting we found the ‘right’ way to model these costs). I also could not stop myself from calling the adjustment cost models L1 L2 L3 instead of “linear” “quadratic” and “fixed”, or _1,2,3. whoops.</p>
<p>One question asked about asymmetric costs. You may recall we started off doing but ran into some unexpected results where they just looked like the cost free case, possibly due to problems with the code. We should probably at least say this is an area for further study.</p>
<p>Another question asked about just restricting the period of adjustment, say, once every 5 years or so. I answered that we hoped to see what cost structures “induced” that behavior rather than enforcing it explicitly; but I should probably add some mention of this to the text as well.</p>
<p>I think the other questions were more straight forward but don’t remember any particulars.</p>
<p>The Monday meeting was very helpful for me in framing the kind of big questions around the paper:</p>
<ol type="1">
<li><p>Can we make this story about more than TAC-managed fisheries? My ideal paper would be something people could cite to show that simply using profit functions with diminishing returns is not a sufficient way to reflect this reality (could be the opposite if reality is more like a transaction fee), and that this mistake can be large. But all our examples are in the fisheries context, so this may take some finesse. (Since we’re aiming for Eco Apps rather than, say, Can Jor Fisheries)</p></li>
<li><p>Emphasizing the “Pretty Darn Good” angle – thinking of the policies we derive with adjustment costs not as the “True optimum” but as a “Pretty Darn Good” policy that can be more robust to adjustment costs – (Provided you have intuition to know if those costs are more like a fixed transaction fee or some proportional cost). The last two figures help with this, since they show using policies under different cost regimes than those under which they were computed to be optimal.</p></li>
<li><p>Need to figure out what to say about policies that can ‘self-adjust’, e.g. when you don’t have to change the law to respond to the fluctuations. (Jim pointed out that Salmon are the best/only case where you can actually manage by “escapement” since you get a complete population census from the annual runs).</p></li>
</ol>
<ul>
<li>Stripping down the complexity of the charts</li>
<li>Conversely, may need to show some examples of the fish stock dynamics (In search for simplicity I’ve focused almost all the graphs on harvest dynamics).</li>
<li>Calibrating and running the case of quadratic control term for comparison</li>
</ul>
<p>As a bonus, I quickly ran the tipping point models, and it looks like these stay really close to the Reed solution – e.g. relative to the safer Beverton Holt world, they are much happier to pay whatever the adjustment cost might be to stick with the optimal than they are to risk total collapse. Not sure but maybe should add this into the paper…</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/08/14/pdg-controlfest-notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/08/14/docker-notes.html">Docker Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 14 Aug 2014</p>

<article>
<div class="excerpt">
<p>Ticking through a few more of the challenges I raised in my <a href="http://www.carlboettiger.info/2014/08/07/too-much-fun-with-docker.html">first post on docker</a>; here I explore some of the issues about facilitating interaction with a docker container so that a user’s experience is more similar to working in their own environment and less like working on a remote terminal over ssh. While technically minor, these issues are probably the first stumbling blocks in making this a valid platform for new users.</p>
<h2 id="sharing-a-local-directory">Sharing a local directory</h2>
<p>Launch a bash shell on the container that shares the current working directory of the host machine (from <code>pwd</code>) with the <code>/host</code> directory on the container (thanks to Dirk for this solution):</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -it -v <span class="ot">$(</span><span class="kw">pwd</span><span class="ot">)</span>:/host cboettig/ropensci-docker /bin/bash</code></pre>
<p>This allows a user to move files on and off the container, use a familiar editor and even handle things like git commits / pulls / pushes in their working directory as before. Then the code can be executed in the containerized environment which handles all the dependencies. From the terminal docker opens, we just <code>cd /host</code> where we find our working directory files, and can launch R and run the scripts. A rather clean way of maintaining the local development experience but containerizing execution.</p>
<p>In particular, this frees us from having to pass our git credentials etc to the container, though is not so useful if we’re wanting to interact with the container via the RStudio server instead of R running in the terminal. (More on getting around this below).</p>
<p>Unfortunately, Mac and Windows users have to run Docker inside an already-virualized environment such as provided by <code>boot2docker</code> or <code>vagrant</code>. This means that it is only the directories on the virtualized environment, not those on the native OS, can be shared in this way. While one could presumably keep a directory synced between this virtual environment and the native OS, (standard in in <code>vagrant</code>), this is a problem for the easier-to-use <code>boot2docker</code> at this time: (<a href="https://github.com/docker/docker/issues/7249">docker/issues/7249</a>).</p>
<h2 id="a-docker-desktop">A Docker Desktop</h2>
<p>Dirk brought this <a href="http://blog.docker.com/2013/07/docker-desktop-your-desktop-over-ssh-running-inside-of-a-docker-container">docker-desktop</a> to my attention; which uses Xpra (in place of X11 forwarding) to provide a window with fluxbox running on Ubuntu along with common applications like libreoffce, firefox, and rox file manager. Pretty clever, and worked just fine for me, but needs Xpra on the client machine and requires some extra steps (run the container, query for passwords and ports, run ssh to connect, then run Xpra to launch the window). The result is reasonably responsive but still slower than virtualbox, and probably too slow for real work.</p>
<h2 id="base-images">Base images?</h2>
<p>The basic Ubuntu:14.04 seems like a good lightweight base image (at 192 MB), but other images try to give more useful building blocks, like <a href="https://github.com/phusion/baseimage-docker#contents">phusion/baseimage</a> (423 MB). Their <code>docker-bash</code> script and other utilities provide some handy features for managing / debugging containers.</p>
<h2 id="other-ways-to-share-files">Other ways to share files?</h2>
<p>Took a quick look at this <a href="https://github.com/gfjardim/docker-dropbox/blob/master/Dockerfile">Dockerfile for running dropbox</a>, which works rather well (at least on a linux machine, since it requires local directory sharing). Could probably be done without explicit linking to local directories to faciliate moving files on and off the container. Of course one can always scp/rsync files on and off containers if ssh is set up, but that is unlikely to be a popular solution for students.</p>
<p>While we have rstudio server running nicely in a Docker container for local or cloud use, it’s still an issue getting Github ssh keys set up to be able to push changes to a repo. We can get around this by linking to our keys directory with the same <code>-v</code> option shown above. We still need a few more steps: setting the Git username and email, and running <code>ssh-add</code> for the key. Presumably we could do this with environmental variables and some adjustment to the Dockerfile:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">docker</span> run -it -v /path/to/keys:/home/rstudio/.ssh/ -e <span class="st">&quot;USERNAME=Carl Boettiger&quot;</span> -e <span class="st">&quot;EMAIL=cboettig@example.org&quot;</span> cboettig/ropensci-docker</code></pre>
<p>which would prevent storing these secure values on the image itself.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/08/14/docker-notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/08/08/an-appropriate-amount-of-fun-with-docker.html">An appropriate amount of fun with docker?</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 08 Aug 2014</p>

<article>
<div class="excerpt">
<p><em>An update on my exploration with Docker. Title courtesy of <a href="https://twitter.com/DistribEcology/status/497523435371638784">Ted</a>, with my hopes that this really does move us in a direction where we can spend less time thinking about the tools and computational environments. Not there yet though</em></p>
<p>I’ve gotten RStudio Server working in the <a href="https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile">ropensci-docker</a> image (Issues/pull requests welcome!).</p>
<pre><code>docker run -d -p 8787:8787 cboettig/ropensci-docker</code></pre>
<p>will make an RStudio server instance available to you in your browser at localhost:8787. (Change the first number after the -p to have a different address). You can log in with username:pw rstudio:rstudio and have fun.</p>
<p>One thing I like about this is the ease with which I can now get an RStudio server up and running in the cloud (e.g. I took this for sail on DigitalOcean.com today). This means in few minutes and 1 penny you have a URL that you and any collaborators could use to interact with R using the familiar RStudio interface, already provisioned with your data and dependencies in place.</p>
<hr />
<p>For me this is a pretty key development. It replaces a lot of command-line only interaction with probably the most familiar R environment out there, online or off. For more widespread use or teaching this probably needs to get simpler still. I’m still skeptical that this will make it out beyond the crazies, but I’m less skeptical than I was when starting this out.</p>
<p>The ropensci-docker image could no doubt be more modular (and better documented). I’d be curious to hear if anyone has had success or problems running docker on windows / mac platforms. Issues or pull requests on the repo would be welcome! https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile (maybe the repo needs to be renamed from it’s original fork now too…)</p>
<p>Rich et al highlighted several “remaining challenges” in their original post. Here’s my take on where those stand in the Docker framework, though I’d welcome other impressions:</p>
<ol type="1">
<li>dependencies could still be missed by incompletely documentation</li>
</ol>
<p>I think this one is largely addressed, at least assuming a user loads the Docker image. I’m still concerned that later builds of the docker image could simply break the build (though earlier images may still be available). Does anyone know how to roll back to earlier images in docker?</p>
<ol start="2" type="1">
<li>The set of scripts for managing reproducibility are at least as complex as the analysis itself</li>
</ol>
<p>I think a lot of that size is due to the lack of an R image for Travis and the need to install many common tools from scratch. Because docker is both modular and easily shared via docker hub, it’s much easier to write a really small script that builds on existing images, (as I show in cboettig/rnexml)</p>
<ol start="3" type="1">
<li>Travis.org CI constraints: public/open github repository with analyses that run in under 50 minutes.</li>
</ol>
<p>Docker has two advantages and also some weaknesses here: (1) it should be easy to run locally, while accomplishing much of the same thing as running on travis (though clearly that’s not as nice as running automatically &amp; in the cloud on every push). (2) It’s easier to take advantage of caching – for instance, cboettig/rnexml provides the knitr cache files in the image so that a user can start exploring without waiting for all the data to download and code to run.</p>
<p>It seems that Travis CI doesn’t currently support docker since the linux kernel they use is too old. (Presumably they’ll update one day. Anyone try Shippable CI? (which supports docker))</p>
<ol start="4" type="1">
<li>The learning curve is still prohibitive</li>
</ol>
<p>I think that’s still true. But what surprised me is that I’m not sure that it’s gotten any worse by adding docker than it was to begin with using Travis CI. Because the approach can be used both locally and for scaling up in the cloud, I think it offers some more immediate payoffs to users than learning a Github+CI approach does. (Notably it doesn’t require any git just to deploy something ‘reproducible’, though of course it works nicely with git.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/08/08/an-appropriate-amount-of-fun-with-docker.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/08/07/too-much-fun-with-docker.html">Too Much Fun With Docker</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 07 Aug 2014</p>

<article>
<div class="excerpt">
<p><strong>NOTE:</strong> This post was originally drafted as a set of questions to the revived <a href="https://groups.google.com/forum/#!forum/ropensci-discuss">ropensci-discuss list</a>, hopefully readers might join the discussion from there.</p>
<p>Been thinking about Docker and the discussion about reproducible research in the comments of Rich et al’s recent post on the <a href="ropensci.org/blog/2014/06/09/reproducibility/">rOpenSci blog</a> where quite a few of people mentioned the potential for Docker as a way to facilitate this.</p>
<p>I’ve only just started playing around with Docker, and though I’m quite impressed, I’m still rather skeptical that non-crazies would ever use it productively. Nevertheless, I’ve worked up some Dockerfiles to explore how one might use this approach to transparently document and manage a computational environment, and I was hoping to get some feedback from all of you.</p>
<p>For those of you who are already much more familiar with Docker than me (or are looking for an excuse to explore!), I’d love to get your feedback on some of the particulars. For everyone, I’d be curious what you think about the general concept.</p>
<p>So far I’ve created a <a href="https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile">dockerfile</a> and <a href="https://registry.hub.docker.com/u/cboettig/ropensci-docker/">image</a></p>
<p>If you have docker up and running, perhaps you can give it a test drive:</p>
<pre><code>docker run -it cboettig/ropensci-docker /bin/bash</code></pre>
<p>You should find R installed with some common packages. This image builds on Dirk Eddelbuettel’s R docker images and serves as a starting point to test individual R packages or projects.</p>
<p>For instance, my RNeXML manuscript draft is a bit more of a bear then usual to run, since it needs <code>rJava</code> (requires external libs), <code>Sxslt</code> (only available on Omegahat and requires extra libs) and latest <code>phytools</code> (a tar.gz file from Liam’s website), along with the usual mess of pandoc/latex environment to compile the manuscript itself. By building on ropensci-docker, we need a <a href="https://github.com/ropensci/RNeXML/tree/master/manuscripts/Dockerfile">pretty minimal docker file</a> to compile this environment:</p>
<p>You can test drive it (<a href="https://registry.hub.docker.com/u/cboettig/rnexml">docker image here</a>):</p>
<pre><code>docker run -it cboettig/rnexml /bin/bash</code></pre>
<p>Once in bash, launch R and run <code>rmarkdown::render(&quot;manuscript.Rmd&quot;)</code>. This will recompile the manuscript from cache and leave you to interactively explore any of the R code shown.</p>
<h2 id="advantages-goals">Advantages / Goals</h2>
<p>Being able to download a pre-compiled image means a user can run the code without dependency hell (often not as much an R problem as it is in Python, but nevertheless one that I hit frequently, particularly as my projects age), and also without altering their personal R environment. Third (in principle) this makes it easy to run the code on a cloud server, scaling the computing resources appropriately.</p>
<p>I think the real acid test for this is not merely that it recreates the results, but that others can build and extend on the work (with fewer rather than more barriers than usual). I believe most of that has nothing to do with this whole software image thing – providing the methods you use as general-purpose functions in an R package, or publishing the raw (&amp; processed) data to Dryad with good documentation will always make work more modular and easier to re-use than cracking open someone’s virtual machine. But that is really a separate issue.</p>
<p>In this context, we look for an easy way to package up whatever a researcher or group is already doing into something portable and extensible. So, is this really portable and extensible?</p>
<h2 id="concerns">Concerns:</h2>
<ol type="1">
<li><p>This presupposes someone can run docker on their OS – and from the command line at that. Perhaps that’s the biggest barrier to entry right now, (though given docker’s virulent popularity, maybe something smart people with big money might soon solve).</p></li>
<li><p>The only way to interact with thing is through a bash shell running on the container. An RStudio server might be much nicer, but I haven’t been able to get that running. <em>Anyone know how to run RStudio server from docker?</em></p></li>
</ol>
<p>(I tried &amp; <a href="https://github.com/mingfang/docker-druid/issues/2">failed</a>)</p>
<ol start="3" type="1">
<li>I don’t see how users can move local files on and off the docker container. In some ways this is a great virtue – forcing all code to use fully resolved paths like pulling data from Dryad instead of their hard-drive, and pushing results to a (possibly private) online site to view them. But obviously a barrier to entry. <em>Is there a better way to do this?</em></li>
</ol>
<h2 id="alternative-strategies">Alternative strategies</h2>
<ol type="1">
<li><p>Docker is just one of many ways to do this (particularly if you’re not concerned about maximum performance speed), and quite probably not the easiest. Our friends at Berkeley D-Lab opted for a GUI-driven virtual machine instead, built with Packer and run in Virtualbox, after their experience proved that students were much more comfortable with the mouse-driven installation and a pixel-identical environment to the instructor’s (see their excellent <a href="https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl">paper</a> on this).</p></li>
<li><p>Will/should researchers be willing to work and develop in virtual environments? In some cases, the virtual environment can be closely coupled to the native one – you use your own editors etc to do all the writing, and then execute in the virtual environment (seems this is easier in docker/vagrant approach than in the BCE.</p></li>
</ol>
<!--

I would like a way for a collaborator who knows a little R to be able to open my .Rmd manuscript on his/her own computer, edit some parameters, recompile and view the pdf. RStudio w/ rmarkdown has gone a long way to making that happen.
-->




 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/08/07/too-much-fun-with-docker.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

  </div>
</div> <!--end row -->

<div class="row socialicons">
  <div class="col-md-11 col-md-offset-1">
      <p> <a href="/archive.html"><i class="icon-calendar"></i> All entries by date</a></p> 
      <p> <a href="/categories.html"><i class="icon-list"></i> All entries by category</a> </p>
      <p> <a href="/tags.html"><i class="icon-tags"></i> All entries by tag</a> </p>
  </div> <!--end col-md-9 -->
</div> <!--end row -->




      <footer class="footer">

<!--************** FOAF information to social networks ***************************** -->
  <div class="row">
    <div class="col-md-3 col-xs-4 socialicons" style="font-size:20px" typeof="foaf:Person" about="http://www.carlboettiger.info#me">
      <p>
          <script type="text/javascript" src="/assets/js/obfuscate-email-link.js"></script> 

          <a rel="foaf:account" href="https://twitter.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); 
             return false;"><span class="showtooltip" title="follow me on twitter (reading, discussing)"><i class="fa fa-twitter"></i></span></a> 

          <a rel="foaf:account" href="https://github.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); 
             return false;"><span class="showtooltip" title="follow me on Github (code, research)"><i class="fa fa-github"></i></span></a>
      <!--
          <a rel="foaf:account" href="https://plus.google.com/" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'GPlus'); 
             return false;"><i class="fa fa-google-plus"></i></a>

          <a rel="foaf:account" href="http://www.mendeley.com/profiles/carl-boettiger" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); 
             return false;"><img src="/assets/img/icon-mendeley.png" /></a> 

           citations on google-scholar

           stackoverflow
      -->
      <a rel="foaf:weblog" type="application/atom+xml" href="/blog.xml"  
         class="showtooltip" title="RSS feeds for my blog-style entries. See the feed on my lab notebook (/atom.xml) to follow all entries instead." 
         onclick="recordOutboundLink(this, 'Outbound Links', 'RSS'); 
         return false;"><i class="fa fa-rss"></i></a>
       </p>
    </div>

    
    <!--**************** End social links **************************** -->


    <div class="col-md-4 col-md-offset-1 col-xs-4">
      <p><a onclick="recordOutboundLink(this, 'Outbound Links', 'ONS_claim'); return false;" href="http://onsclaims.wikispaces.com/"><img src="/assets/img/ons-aci2-icon.svg" alt="ONS" class="showtooltip" title="An Open Notebook Science (ONS) project claim: Entry provides all content (AC) immediately (I) or without significant delay.  See link for details"/></a></p>
    </div>


    <div class="col-md-3 col-md-offset-1 col-xs-4">
      <p>
      <a rel="license" property="http://creativecommons.org/ns#license" href="http://creativecommons.org/publicdomain/zero/1.0/" onclick="recordOutboundLink(this, 'Outbound Links', 'CC0'); return false;"><img src="/assets/img/cc-zero.svg" alt="CC0"/></a> 
      </p>
    </div>
  </div>


  
<!-- COinS metadata (for citation managers like Zotero etc), goes in body text -->
  <span
      class="Z3988" 
      title="ctx_ver=Z39.88-2004
      &amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc
      &amp;rfr_id=info%3Asid%2Focoins.info%3Agenerator
      &amp;rft.title=Lab Notebook
      &amp;rft.creator=Carl Boettiger
      &amp;rft.date=
      &amp;rft.language=EN
      &amp;rft.rights=CC0
      &amp;rft_id=http://www.carlboettiger.info/lab-notebook.html">
  </span>


</footer>




          <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

    <!-- JQuery, used on a few pages (still?) -->
    <!-- <script type="text/javascript" src="/assets/js/jquery.js"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Equations using MathJax -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });       </script>
    <!-- Twitter Bootstrap Javascript -->
    <!--  <script src="/assets/js/bootstrap.min.js"></script> -->
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>


    

        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-18401403-1']);
          _gaq.push(['_trackPageview']);
          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
  </script>



<script type="text/javascript">
function recordOutboundLink(link, category, action) {
  try {
    var pageTracker=_gat._getTracker("UA-18401403-1");
    pageTracker._trackEvent(category, action);
    setTimeout('document.location = "' + link.href + '"', 100)
  }catch(err){}
}
</script>




    </div>
  </body>
</html>
   
