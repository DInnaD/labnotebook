<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head prefix="dc: http://purl.org/dc/terms/ og: http://ogp.me/ns#"> <!-- namespaces used in metadata.html -->
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8'/>
  <title>Lab Notebook</title>
  <meta name="author" content="Carl Boettiger" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- HTML5 metadata -->
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="resource_type" content="website"/> 
<!-- RDFa Metadata (in DublinCore) -->
<meta property="dc:title" content="Lab Notebook" />
<meta property="dc:creator" content="Carl Boettiger" />
<meta property="dc:date" content="" />
<meta property="dc:format" content="text/html" />
<meta property="dc:language" content="en" />
<meta property="dc:identifier" content="/lab-notebook.html" />
<meta property="dc:rights" content="CC0" />
<meta property="dc:source" content="Lab Notebook" />
<meta property="dc:subject" content="Ecology" /> 
<meta property="dc:type" content="website" /> 
<!-- RDFa Metadata (in OpenGraph) -->
<meta property="og:title" content="Lab Notebook" />
<meta property="og:author" content="http://www.carlboettiger.info/index.html#me" />  <!-- Should be Liquid? URI? -->
<meta property="http://ogp.me/ns/profile#first_name" content="Carl"/>
<meta property="http://ogp.me/ns/profile#last_name" content="Boettiger"/>
<meta property="http://ogp.me/ns/article#published_time" content="" />
<meta property="og:site_name" content="Lab Notebook" /> <!-- Same as dc:source? -->
<meta property="og:url" content="http://www.carlboettiger.info/lab-notebook.html" />
<meta property="og:type" content="website" /> 
<!-- Google Scholar Metadata -->
<!--
<meta name="citation_author" content="Carl Boettiger"/>
<meta name="citation_date" content=""/>
<meta name="citation_title" content="Lab Notebook"/>
<meta name="citation_journal_title" content="Lab Notebook"/>
-->
<!--NOTE: see also the COinS Metadata in span element in footer -->




  <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!-- Help the browser identify the RSS feed automatically -->
  <link rel="alternate" type="application/rss+xml" title="Carl Boettiger's Lab Notebook" href="/blog.xml" />
</head>


  <body prefix="dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/"> 
    <!-- Navbar  ================================================== -->

<nav class="navbar navbar-default" role="navigation">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/README.html"><i class="icon-info-sign"></i></a>
    </div>

 <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

          <li  >
          <a href="/index.html">Home</a></li>
          <li  >
          <a href="/vita.html">Vita</a></li>
          <li  >
          <a href="/research.html">Research</a></li>
          <li  >
          <a href="/teaching.html">Teaching</a></li>
          <li  >
          <a href="/community.html">Community</a></li>
          <li  class="active" >
          <a href="/lab-notebook.html">Lab Notebook</a></li>

        </ul>

      <!-- Search site using Google's index -->
        <form class="navbar-form navbar-right" role="search" method="get" action="http://google.com/search">
          <div class="form-group">
            <input type="hidden" name="q" value="site:carlboettiger.info" />
            <input type="text" class="form-control search-query" name="q" placeholder="Search"/>
          </div>
          <button class="btn btn-mini" type="submit"><i class="icon-search"></i></button> 
       </form>

    </div><!--/.nav-collapse -->
  </div> <!-- /container -->
</nav>



    <div class="container"> <!-- Responsive grid layout, doesn't jump to full-width --> 
      <header>
        <h1 class="entry-title">Lab Notebook</h1>
        <h2>(<a href="http://www.carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html">Introduction</a>)</h2>
      </header>

      <div class="row feed">
  <div class="col-md-3 col-md-offset-1">
    <h4>  <a property="account" href="https://github.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); return false;"><i class="icon-github" alt="github"></i> Coding </a></h4> 
    <div class="excerpt">
      <div class="scroll">
        <ul><li>cboettig pushed to master at cboettig/labnotebook: <em>fixing a few validation errors</em> <a href="https://github.com/cboettig/labnotebook/compare/3c40508df4...7016a1e601">10:34 2014/08/04</a></li><li>cboettig pushed to devel at ropensci/rfigshare: <em>bump version</em> <a href="https://github.com/ropensci/rfigshare/compare/9d7350b9cb...ace94f6bd5">10:24 2014/08/04</a></li><li>cboettig pushed to devel at ropensci/rfigshare: <em>bump version for CRAN patch make sure README.md is XHTML1.0 and HTML5 compliant</em> <a href="https://github.com/ropensci/rfigshare/compare/11519787b0...9d7350b9cb">09:52 2014/08/04</a></li><li>cboettig commented on issue tpitale/legato#80: <em>Very strange. The command line tool works fine for me. I get a token, I can store it in yaml, I can use the token to execute all the steps for the …</em> <a href="https://github.com/tpitale/legato/issues/80#issuecomment-51121205">09:36 2014/08/04</a></li><li>cboettig pushed to gh-pages at cboettig/labnotebook: <em>Updating to cboettig/labnotebook@3c40508.</em> <a href="https://github.com/cboettig/labnotebook/compare/f7eac94789...5f3d67275b">09:23 2014/08/04</a></li></ul>
      </div>
    </div>
  </div>
  <div class="col-md-3">
    <h4> <a property="account" href="https://twitter.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); return false;"><i class="icon-twitter"></i> Discussing </a></h4> 
     <div class="excerpt">
      <div class="scroll">
       <ul><li><p>.@noamross Check it out: @RStudio now supports markdown -&gt; markdown <a href="http://t.co/nmEk9dS47K">http://t.co/nmEk9dS47K</a> #rstats (not listed on documentation homepage)</p>
 <a href="http://twitter.com/cboettig/statuses/496335501108854785">04:42 2014/08/04</a> </li><li><p>@eastonrwhite nothing much that year, maybe some seminars &amp; Monte carlos</p>
 <a href="http://twitter.com/cboettig/statuses/495994203655401472">06:06 2014/08/03</a> </li><li><p>RT @srsupp: Look! The new @ESA<em>EarlyCareer Section is on twitter! Check out what they&#39;re doing and share ideas for moving forward. @ESA</em>org…</p>
 <a href="http://twitter.com/cboettig/statuses/494586545655926784">08:53 2014/07/30</a> </li><li><p>Wow. Excellent intro to the DevOpts appr to #reproducibleresearch and instruction from @davclark + @DLabAtBerkeley  <a href="https://t.co/TJ9ObbL8i8">https://t.co/TJ9ObbL8i8</a></p>
 <a href="http://twitter.com/cboettig/statuses/494547562569793536">06:18 2014/07/30</a> </li><li><p>.@xieyihui on the future of #rstats mailing lists <a href="http://t.co/vlGJeT3u4G">http://t.co/vlGJeT3u4G</a> &quot;The past was good.  and the future can be better&quot;. Read: use SO</p>
 <a href="http://twitter.com/cboettig/statuses/494529651801194496">05:07 2014/07/30</a> </li></ul>
      </div>
    </div> 
  </div> 
  <div class="col-md-3">
    <h4> <a href="http://www.mendeley.com/groups/634301/theoretical-ecology/papers/" onClick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); return false;"><i class="icon-book"></i> Reading </a></h4> 
    <div class="excerpt">
      <div class="scroll">
<ul><li>A generalized perturbation approach for exploring stock recruitment relationships: Theoretical Ecology (2014). Justin D. Yeakel, Marc Mangel et al. <a href="http://www.mendeley.com/c/6971620324/g/634301/yeakel-2014-a-generalized-perturbation-approach-for-exploring-stock-recruitment-relationships/">10:44 2014/07/09</a></li><li>Temporal variability of forest communities: empirical estimates of population change in 4000 tree species: Ecology Letters (2014). Pages: n/a-n/a. Ryan a. Chisholm, Richard Condit, K. Abd. Rahman, Patrick J. Baker, Sarayudh Bunyavejchewin, Yu-Yun Chen, George Chuyong, H. S. Dattaraja, Stuart Davies, Corneille E. N. Ewango, C. V. S. Gunatilleke, I. a. U. Nimal Gunatilleke, Stephen Hubbell, David Kenfack, Somboon Kiratiprayoon, Yiching Lin, Jean-Remy Makana, Nantachai Pongpattananurak, Sandeep Pulla, Ruwan Punchi-Manage, Raman Sukumar, Sheng-Hsin Su, I-Fang Sun, H. S. Suresh, Sylvester Tan, Duncan Thomas, Sandra Yap et al. <a href="http://www.mendeley.com/c/6971620304/g/634301/chisholm-2014-temporal-variability-of-forest-communities-empirical-estimates-of-population-change-in-4000-tree-species/">09:51 2014/07/09</a></li><li>The importance of individual developmental variation in stage-structured population models: Ecology Letters (2014). Pages: n/a-n/a. Perry de Valpine, Katherine Scranton, Jonas Knape, Karthik Ram, Nicholas J. Mills et al. <a href="http://www.mendeley.com/c/6971620314/g/634301/de-valpine-2014-the-importance-of-individual-developmental-variation-in-stage-structured-population-models/">09:51 2014/07/09</a></li><li>A semiparametric Bayesian method for detecting Allee effects: Ecology (2013). Volume: 94, Issue: 5. Pages: 1196-1204. Masatoshi Sugeno, Stephan B Munch et al. <a href="http://www.mendeley.com/c/6971620294/g/634301/sugeno-2013-a-semiparametric-bayesian-method-for-detecting-allee-effects/">09:51 2014/07/09</a></li></ul>
      </div>
    </div>
  </div> 
</div>

<hr>
<div class="row postpreview">
  <div class="col-md-11 col-md-offset-1">
    <div class="row">
      <h4> <a href="http://www.carlboettiger.info/atom.xml"
              onClick="recordOutboundLink(this,
              'Outbound Links', 'RSS'); return false;"
              style="color: inherit;"
              ><i class="icon-rss" ></i> Entries</a></h4>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/07/31/notes.html">Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 31 Jul 2014</p>

<article>
<div class="excerpt">
<h2 id="berkeley-collaborative-environment">Berkeley Collaborative Environment</h2>
<p>Trying out the Berkeley image (Essentially ubuntu 14.04 XFCE with ipython and RStudio installed, but finely tuned to improve user experience; e.g. solid colors for faster remote window connections.)</p>
<p>Highly recommend <a href="https://t.co/TJ9ObbL8i8">their paper</a> for the first explanation I’ve actually been able to follow that provides a definition of “DevOpts” (essentially, using scripts rather than documentation to manage consistent cross-platform installation) and explains the differences and similarities between the various programs operating in this sphere, sometimes as alternatives and simultaneously.</p>
<ul>
<li><p>Virtual machines. Their approach focuses on running on top of a complete virtual machine. Primarily considers Oracle’s <code>virtualbox</code> for local use, Amazon’s AMI for cloud use. (For an emerging alternative to the full virtual machine approach, they discuss Docker).</p></li>
<li><p><strong>Ansible</strong> (“playbooks”) used by the BCE to specify the complete software environment. Compares to: <strong>Chef</strong> (Ruby-based, “recipes”), <strong>Salt</strong> (Python based, “states”), and <strong>Puppet</strong> (“manifests”).</p></li>
<li><p><strong>Packer</strong> Used at build time to create a machine image for the VM. Packer can use Ansible/Chef/Salt/Puppet files to do this. Results in a nice AMI for Amazon web console or an image for the virtualbox GUI.</p></li>
<li><p><strong>Vagrant</strong> Offers a different approach. Rather than the developer creating a VM image using Packer and Ansible script that is ready to run on virtualbox or Amazon, the end user installs vagrant instead of virtualbox. Vagrant also handles the job of Packer, in preparing an environment to run (on Vagrant’s virtual machine, rather than on virtualbox). (Note that conversely, Packer can create a Vagrant virtual machine just as easily as it can create the Oracle virtualbox or Amazon AMI). Vagrant feels a lot more native, as the user works within their familiar OS tools for editing, etc, while vagrant makes sure that the executation of the software happens in a controlled, identical environment.</p></li>
<li><p><strong>Docker</strong> offers a more modular alternative to a full virtual machine, with performance that is more like running on ‘bare metal’, sharing the kernel of the native OS; though at the moment it requires that be a linux kernel. Docker is typically deployed using Vagrant, (though stand-alone setup is emerging).</p></li>
</ul>
<p>From first glance, Vagrant sounds like a more elegant approach than having end users install Oracle’s virtual machine and then learn to live in a virtual window emulating the Ubuntu-XFCE desktop (particularly if those users are developers!). However, it seems that the BCE team found that Vagrant was harder for students to work with, since it required knowledge of the commandline.</p>
<p>The paper also provides a fabulous case study of a major scientific software project using the “DevOpts” philosophy but without any of these new and emerging tools, relying only on scripts, makefiles, and Linux distribution package managers.</p>
<h3 id="test-drive-impressions">Test drive impressions</h3>
<p>Unfortunately, no luck getting virtualization running on my laptop (due to <a href="ttps://github.com/dlab-berkeley/collaboratool/issues/created_by/cboettig">BIOS issues</a>). Testing on Ubuntu desktop required a newer version of virtualbox than what’s in the 12.04 repos, but otherwise just worked. It’s straight forward to install additional needed software, and virtualbox gives the option of perserving the machine state on exit, presumably with the software. Not clear how that should be managed to avoid re-creating the problems that using a consistent image set out to avoid in the first place; perhaps requires re-provisioning a divergent image?</p>
<p>Nice experience testing out the Amazon machine image, but I think the workflow would be improved if it provided RStudio server for a more interactive interface.</p>
<h2 id="misc-tasks">Misc tasks</h2>
<ul>
<li><p>Working on reveal.js slides</p></li>
<li><p>RNeXML manuscript edits from Francois’s comments</p></li>
</ul>
<h2 id="thoughts-on-namespaces-in-r">Thoughts on namespaces (in R)</h2>
<p><em>Chatting with Scott about namespace practices, thought I’d put some of this down.</em></p>
<p>I go back and forth on having a small namespace (particularly vs convenience functions). It is certainly easier to maintain, but from the user’s perspective I think it’s pretty easy to just ignore extra functions; as long as it’s well documented what functions they need to know to get started.</p>
<p>I guess if a user needs to know too many functions to do anything, than it becomes hard to keep track of. That’s partly why I started wrapping exposed functions. For instance, you can just use <code>nexml_write</code> all the time and never use add_characters, add_trees, etc, since <code>nexml_write</code> takes additional trees and characters as arguments.</p>
<p>But I like function calls to be as semantic as possible so the code is more self documenting. Sometimes <code>add_characters(nex)</code> is more self explanatory than <code>nexml_write(nex, characters = characters)</code>.</p>
<p>The XML package really got me started on this. There’s usually like 4 or 5 ways to do the same thing. Sometimes that’s really annoying, but sometimes it helps write more transparent code or less verbose code (e.g. adding child nodes with <code>addChildren</code> vs passing as <code>.children</code> argument to <code>newXMLNode</code> – the former tends to be more semantic, the latter often more consise).</p>
<h2 id="code-tricks-vim-pandoc">Code tricks: vim pandoc</h2>
<p>Vim pandoc syntax highlighting</p>
<ul>
<li>Recognize <code>.Rmd</code> as a pandoc-syntax file extension. In <code>.vimrc</code> do:</li>
</ul>
<pre class="vim"><code>au BufRead,BufNewFile *.Rmd set filetype=pandoc</code></pre>
<ul>
<li>Enable sytnax highlighting inside code blocks, by language. In vim session, do: <code>PandocHighlight r</code>. Unfortunately doesn’t recognize the default <code>.Rmd</code> format used by RStudio. This then enables syntax highlighting, folding, etc.</li>
</ul>
<p>If I recall correctly, knitr’s default markdown syntax,</p>
<pre><code>```{r}</code></pre>
<p>was intended to be a valid markdown syntax. It seems Github Flavored markdown is happy to recognize this as markdown syntax for a code block in the R language, but while pandoc recognizes this as a code block, it does not recognize the language.</p>
<p>I believe Pandoc does recognize the format</p>
<pre><code>```{.r options}</code></pre>
<p>as the notation to specify the R language in this notation. I like this notation because it means that pandoc-aware syntax highlighters will highlight my code chunks as R code, which does not happen in the default syntax.</p>
<p>I realize I could define this as an input hook for knitr, but am reluctant to do so as it makes my code slightly less familiar/less portable to other users (e.g. RStudio expects and integrates with only the standard notation.)</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/07/31/notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/07/21/notes.html">Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 21 Jul 2014</p>

<article>
<div class="excerpt">
<h2 id="reading">Reading</h2>
<p>Looking for this in my notes and couldn’t find it: An excellent paper from the Software Sustainability Institute and friends outlining the need and possible structure for sustaining career paths for software developers. “<a href="http://dirkgorissen.com/2012/09/13/the-research-software-engineer/">The research software engineer</a>”, Dirk Gorissen. Provides a good response to the <a href="http://www.timeshighereducation.co.uk/news/save-your-work-give-software-engineers-a-career-track/2006431.article">software issues highlighted by climategate</a>, etc.</p>
<h2 id="remote-conferencing">Remote conferencing</h2>
<p>(Based on earlier unposted notes). With so much going on, it’s nice to be able to follow highlights from some conferences remotely</p>
<ul>
<li><p>From ISIC: Ben Bolker’s <a href="http://t.co/ft2sJjRNFp">slides on statistial machismo</a></p></li>
<li><p>From BOSC: C. Titus Brown’s keynote <a href="http://ivory.idyll.org/blog/2014-bosc-keynote.html">A history of bioinformatics (in 2039)</a> (see links to slides, storify).</p></li>
<li><p>From PyCon: Greg Wilson’s talk (<a href="https://www.youtube.com/watch?v=1e26rp6qPbA">youtube recording</a>) is fantastic, ending with compelling thesis for large-scale collaboration via diff/merge as the foundation of transformational change / open science, and the puzzling case of why this model has not yet been more widely adopted in generating teaching materials. Greg mentions several excellent resources in the talk as well: <a href="http://www.slideshare.net/richardcookau/john-hattie-effect-sizes-on-achievement">John Hattie’s slides on effect sizes of different classroom methods</a>, Carnegie Mellon’s summary text on decades of research, “How Learning Works” (<a href="http://c4ed.lib.kmutt.ac.th/sites/default/files/HowLearningWorks-Ambrose.pdf">pdf</a>), and Mark Guzdial’s <a href="http://computinged.wordpress.com/about/">blog on CS education</a>.</p></li>
<li><p>From ESIP: Daniel Katz’s slides on <a href="http://t.co/1fUkycXAMP">Working towards Sustainable Software for Science (an NSF and community view)</a></p></li>
</ul>
<h2 id="misc-code-tricks">Misc code-tricks</h2>
<p>For a question raised during the Mozilla sprint: had to remember how to write custom hooks for knitr (e.g. for kramdown compatibility):</p>
<pre class="sourceCode r"><code class="sourceCode r">hook.t &lt;-<span class="st"> </span>function(x, options) <span class="kw">paste0</span>(<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">~~~</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">paste0</span>(x, <span class="dt">collapse=</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>), <span class="st">&quot;~~~</span><span class="ch">\n\n</span><span class="st">&quot;</span>)
hook.r &lt;-<span class="st"> </span>function(x, options) {
       <span class="kw">paste0</span>(<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">~~~ &quot;</span>, <span class="kw">tolower</span>(options$engine), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">paste0</span>(x, <span class="dt">collapse=</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">~~~</span><span class="ch">\n\n</span><span class="st">&quot;</span>)
  }
knitr::knit_hooks$<span class="kw">set</span>(<span class="dt">source=</span>hook.r, <span class="dt">output=</span>hook.t, <span class="dt">warning=</span>hook.t,
                              <span class="dt">error=</span>hook.t, <span class="dt">message=</span>hook.t)</code></pre>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/07/21/notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/07/21/UPS-and-data-vs-optimal-control.html">UPS and data vs optimal control</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 21 Jul 2014</p>

<article>
<div class="excerpt">
<p><em>Random idea for possible further exploration:</em></p>
<p>The use of ‘big data’ by UPS to perform lots of small efficiency gains seems to be everybody’s favorite example (<a href="http://www.npr.org/blogs/money/2014/05/02/308640135/episode-536-the-future-of-work-looks-like-a-ups-truck">NPR</a>, <a href="http://www.economist.com/news/business/21607816-businesses-should-aim-lots-small-wins-big-data-add-up-something-big-little">The Economist</a>). During a typical applications of optimal control for ecological conservation talk yesterday I couldn’t help thinking back to that story. The paradigm shift is not so much the kind or amount of the data being used as it is the control levers themselves. As the Economist (rightly) argues, everyone typically assumes that a few principle actions are responsible for 80% of possible improvement.</p>
<p>Optimal control tends to focus on these big things, which are also usually particularly thorny optimizations. Most of the classic textbook hard optimization problems could have come right from the UPS case: the traveling salesman, the inventory packing/set cover problems, and so forth. Impossible to solve exactly on large networks, approximate dynamic programming approaches have since been the work-around. Yet the “Big Data” approach takes a rather different strategy all together, tackling many small problems instead of one big one. Our typical approach of theoretical abstractions to simple models is designed to focus on these big overarching problems. In abstracting the problem, we focus on the big picture stuff that should matter most – stuff like figuring out the optimal route to travel, and so forth. But when the gains through increasing optimization of these things are marginal, focusing on the “other 20%” can make more sense. However, that means abandoning the abstraction and going back to the original messy problem. It means knowing about all the other little levers and switches we can control. In the UPS context, this means thinking about how many times a truck backs up, or idles at a stop light, or what hand the deliveryman holds the pen in. Given both the data and the ability to control so many of these little things, optimizing each one in the first place can be more valuable than focusing on the big abstract optimizations.</p>
<p>So, does this work only once the heuristic solutions to the big problems are nearly optimal, so improved approximations have very limited gains? Or can this also be a route forward when the big problems are primarily intractable as well? The former certainly seems the more likely, but if the latter is true, it could prove very interesting.</p>
<p>So this got me thinking – if we accept the latter premise we find a case closely analogous to the very messy optimizations we face in conservation decision-making. Could the many little levers be an alternative? It’s unlikely given both the need for the kind of arbitrarily detailed data at almost no cost available to the UPS problem, and also the kind of totalitarian control UPS can apply to control all the little levers, while the conservation problem more frequently has nothing bit a scrawny blunt stick to toggle in the first place. Nevertheless, it’s hard to know what possible gains we have already excluded when we focus only on the big abstractions and the controls relevant to them. Could conservation decision-making think more outside the box about the many little things we might be able to more effectively influence?</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/07/21/UPS-and-data-vs-optimal-control.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/07/18/notes.html">Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 18 Jul 2014</p>

<article>
<div class="excerpt">
<h2 id="cran-trivia">CRAN trivia</h2>
<h3 id="when-should-you-bump-version-for-a-rejected-resubmission">When should you bump version for a (rejected) resubmission?</h3>
<blockquote>
<p>Once accepted, any change other than to the metadata (essentially the DESCRIPTION file) needs an increased version. For submissions, we prefer (but do not insist on) a new number of each attempt.</p>
</blockquote>
<p>–Prof Brian Ripley</p>
<h3 id="using-non-cran-repositories-in-suggests-or-enahances">Using non-CRAN repositories in SUGGESTS or ENAHANCES</h3>
<p>More dubious tricks, from Yihui:</p>
<blockquote>
<p>FYI, here is how R core checks dependencies: https://github.com/wch/r-source/blob/trunk/src/library/tools/R/QC.R#L5195</p>
</blockquote>
<blockquote>
<p>Because I know this, sometimes I intentionally use something like (function(pkg) library(pkg, character.only = TRUE))(“foo”) to silence R CMD check and cheat when I (optionally) need a package but do not want CRAN maintainers to know it</p>
</blockquote>
<h2 id="package-maintainence">Package maintainence</h2>
<ul>
<li><p>knitcitations 0.1.1 on CRAN now.</p></li>
<li><p>See RNeXML check results: http://cran.r-project.org/web/checks/check_results_RNeXML.html</p></li>
<li><p>RNeXML Submitted a series of patches that allow tests not to fail when external resources (packages, web APIs) that are not available.</p></li>
<li><p>rfigshare updated <a href="https://github.com/ropensci/rfigshare/issues/84">#84</a>. Triggered occassional errors from API failing, so most tests now skipped if authentication call fails. (Reworked authentication a bit)</p></li>
<li><p>bugfix: <a href="https://github.com/cboettig/knitcitations/issues/63#issuecomment-49459723">knitcitations/#63</a></p></li>
</ul>
<h2 id="misc">Misc</h2>
<p>Taking a look at <a href="">auto</a> for bifurcation diagrams (ht Noam, who is using the XPP wrapper).</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/07/18/notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/07/10/notes-on-tricks-in-manuscript-submission-and-collaboration.html">Notes On Tricks In Manuscript Submission And Collaboration</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 10 Jul 2014</p>

<article>
<div class="excerpt">
<h2 id="some-thoughts-on-collaborating-with-markdown-dynamic-document-workflow">Some thoughts on collaborating with markdown / dynamic document workflow</h2>
<p>Collaborating on manuscripts with other researchers when not writing in MS Word has been a perpetual nuisance for those of us not using Word (no doubt the others might say the same). When I first began writing papers I worked in LaTeX, and at that time I collaborated largely with others who already knew TeX (e.g. my mentors), so this wasn’t much of a problem. While moving my workflow into markdown has simplified collaborations with (often junior) researchers who know markdown better than tex, it has managed to make the potential for mismatches even greater, as it creates a barrier for both co-authors working in LaTeX as well as those working in Word.</p>
<p>This has always been more of a nuisance than a real problem. I’ve usually just sent co-authors some derived copy (e.g. a pdf, or sometimes creating a Word or TeX document from the markdown using pandoc and sending that). This means I have to transcribe or at least copy and paste the edits, though that’s never all that time consuming a process.</p>
<p>I still have high hopes that RStudio’s <code>rmarkdown</code> format will make it practical for co-authors to edit and compile my <code>.Rmd</code> files directly. Meanwhile, a mentor who frequently uses LaTeX in collaborating with Word users suggested a much simpler solution that has proven very pratical for me so far.</p>
<h3 id="a-simple-solution">A simple solution</h3>
<p>Based on his suggestion, I just paste the contents of the <code>.Rmd</code> file into a Word (well, LibreOffice) document and send that. Most collaborators can just ignore the code blocks and LaTeX equations, etc, and edit the text directly. I send the compiled pdf as well for the figures and rendered equations. A collaborator cannot easily re-compile their changes, but I can by copy-pasting back into the <code>.Rmd</code> file. They can track changes via Word, and I can track the same changes through the version control when I paste their changes back in. It’s not perfect, but it’s simple.</p>
<h1 id="challenges-in-submission-systems">Challenges in submission systems</h1>
<h2 id="transparent-figures">transparent figures</h2>
<p>I use semi-transparent graphs to show ensemble distributions of stochastic trajectories. First, a quick example as to why:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sims_data) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(time, fishstock, <span class="dt">group=</span><span class="kw">interaction</span>(reps,method), <span class="dt">color=</span>method), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values=</span>colorkey, <span class="dt">guide =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>))) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~method) +<span class="st"> </span><span class="kw">guides</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</code></pre>
<figure>
<img src="http://io.carlboettiger.info/nonparametric-bayes/replicates.svg" />
</figure>
<p>Statistical summaries can get around this approach, but don’t really reflect the true rather binary nature of the ensemble (that some trajectories go to zero while others remain around a constant level):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sims_data) +

<span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(time, fishstock), <span class="dt">fun.data =</span> <span class="st">&quot;mean_sdl&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;ribbon&quot;</span>, <span class="dt">fill=</span><span class="st">&#39;grey80&#39;</span>, <span class="dt">col=</span><span class="st">&#39;grey80&#39;</span>) +
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(time, fishstock, <span class="dt">col=</span>method), <span class="dt">fun.y =</span> <span class="st">&quot;mean&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;line&quot;</span>) +

<span class="co">#  geom_line(aes(time, fishstock, group=interaction(reps,method), color=method), alpha=0.1) +</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values=</span>colorkey, <span class="dt">guide =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>))) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~method) +<span class="st"> </span><span class="kw">guides</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</code></pre>
<figure>
<img src="http://io.carlboettiger.info/nonparametric-bayes/replicates-stat-summary-sdl.svg" />
</figure>
<p>Tweaking the statistical definitions can remove the more obvious errors from this, but still give the wrong impression:</p>
<pre class="sourceCode r"><code class="sourceCode r">mymin &lt;-<span class="st"> </span>function(x) <span class="kw">mean</span>(x) -<span class="st"> </span><span class="kw">sd</span>(x)
<span class="kw">ggplot</span>(sims_data) +

<span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(time, fishstock), <span class="dt">fun.y =</span> <span class="st">&quot;mean&quot;</span>, <span class="dt">fun.ymin =</span> <span class="st">&quot;mymin&quot;</span>, <span class="dt">fun.ymax=</span><span class="st">&quot;max&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;ribbon&quot;</span>, <span class="dt">fill=</span><span class="st">&#39;grey80&#39;</span>, <span class="dt">col=</span><span class="st">&#39;grey80&#39;</span>) +
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="kw">aes</span>(time, fishstock, <span class="dt">col=</span>method), <span class="dt">fun.y =</span> <span class="st">&quot;mean&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;line&quot;</span>) +

<span class="co">#  geom_line(aes(time, fishstock, group=interaction(reps,method), color=method), alpha=0.1) +</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values=</span>colorkey, <span class="dt">guide =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>))) +
<span class="st">    </span><span class="kw">facet_wrap</span>(~method) +<span class="st"> </span><span class="kw">guides</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</code></pre>
<figure>
<img src="http://io.carlboettiger.info/nonparametric-bayes/replicates-stat-summary.svg" />
</figure>
<h3 id="technical-challenges-in-submitting-transparent-figures">Technical challenges in submitting transparent figures</h3>
<p>All this works great with R+ggplot2, generating svg versions shown here or in generating pdfs for the final manuscript. Try and upload those pdfs to manuscriptcentral though (or arXiv, actually) and they will render improperly or not at all. What ever happened to the “portable” in “portable document format”? (Note that submission systems can take EPS/PS, though we’d need to change our LaTeX flavor for it, but R’s graphic devices for those formats don’t seem to support transparency.</p>
<p>It seems the problem for pdfs arises from different versions (thanks to Rich FitzJohn for figuring this out, I would never have managed). Transparency is natively supported in pdf &gt;= 1.4, while in earlier versions it is just emulated. R can generate pdfs in 1.3 (using <code>dev.args = list(version=&quot;1.3&quot;)</code> as the knitr chunk option), but unfortunately, ggplot promotes pdfs to version 1.4:</p>
<pre><code>Saving 7 x 6.99 in image
Warning message:
In grid.Call.graphics(L_lines, x$x, x$y, index, x$arrow) :
  increasing the PDF version to 1.4</code></pre>
<p>I’m not quite clear what part of the ggplot command triggers this, as some ggplot figures do render in version 1.3. To add one more gotcha, RStudio’s <code>rmarkdown</code> by default runs pdfcrop which also promotes the pdfs to version 1.4.</p>
<p>It seems that pdf 1.5 works however – opening the v1.4 pdf in inkscape and saving as v1.5 seems to do the trick. (the current R version seems to be 1.7, though R supports up to 1.4). This is the route I took for the time being with mansucriptcentral, though frustrating that it requires a step external to the <code>rmarkdown::render</code> process.</p>
<p>They can also take TIFF for graphics (though (pdf)LaTeX can’t). I suppose one could submit jpg/png images as supplementary files for the tex compilation, which would have been a workable solution if annoying to use rasters when a vector graphic is preferred (and much smaller – I can’t understand why manuscriptcentral takes 10^4 times as long to upload a document as arXiv or other platforms).</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/07/10/notes-on-tricks-in-manuscript-submission-and-collaboration.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/06/12/knitcitations-updates.html">Knitcitations Updates</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 12 Jun 2014</p>

<article>
<div class="excerpt">
<p>Used some down-time while traveling to hammer out a long overdue update to my <a href="https://github.com/cboettig/knitcitations">knitcitations</a> package.</p>
<p>My first task inovled a backwards-compatible update fixing a few minor issues (see <a href="https://github.com/cboettig/knitcitations/blob/master/NEWS">NEWS</a>) and providing pandoc style inline citations <a href="https://github.com/cboettig/knitcitations/releases/tag/v0.6-2"><code>v0.6-2</code></a>, on CRAN.</p>
<p>I followed this with a ground-up rewrite, as I summarize in NEWS:</p>
<h2 id="v1.0-1">v1.0-1</h2>
<p>This version is a ground-up rewrite of knitcitations, providing a more powerful interface while also streamlining the back end, mostly by relying more on external libraries for knitty gritty. While an effort has been made to preserve the most common uses, some lesser-used functions or function arguments have been significantly altered or removed. Bug reports greatly appreciated.</p>
<ul>
<li><p><code>citet</code>/<code>citep</code> now accept more options. In addition to the four previously supported options (DOI, URL, bibentry or bibkey (of a previously cited work)), these now accept a plain text query (used in a CrossRef Search), or a path to a PDF file (which attempts metadata extraction).</p></li>
<li><p>Citation key generation is now handled internally, and cannot be configured just by providing a named argument to <code>citet</code>/<code>citep</code>.</p></li>
<li><p>The <code>cite</code> function is replaced by <code>bib_metadata</code>. This function takes any argument to <code>citet</code>/<code>citep</code> as before (including the new arguments), see docs.</p></li>
<li><p>Linked inline citations now use the configuration: <code>cite_options(style=&quot;markdown&quot;, hyperlink=&quot;to.doc&quot;)</code> provides a link to the DOI or URL of the document, using markdown format.</p></li>
<li><p>Support for cito and tooltip have been removed. These may be restored at a later date. (The earlier implementation did not appropriately abstract the use of these features from the style/formatting of printing the citation, making generalization hard.</p></li>
<li><p><code>bibliography</code> now includes CSL support directly for entries with a DOI using the <code>style=</code> argument. No need to provide a CSL file itself, just the name of the journal (or rather, the name of the corresponding csl file: full journal name, all lower case, spaces as dashes). See https://github.com/cboettig/knitcitations/issues/38</p></li>
<li><p><code>bibliography</code> formatting has otherwise been completely rewritten, and no longer uses <code>print_markdown</code>, <code>print_html</code>, and <code>print_rdfa</code> methods. rdfa is no longer available, and other formats are controlled through <code>cite_options</code>. For formal publication pandoc mode is recommended instead of <code>bibliography</code>.</p></li>
</ul>
<p>This version was developed on a separate branch (<code>v1</code>), and has only just been merged back into master. CRAN doesn’t like getting multiple updates in the same month or so, but hopefully waiting a bit longer will give users and I a chance to shake out bugs anway. Meanwhile grab it from github with:</p>
<pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">install_github</span>(<span class="st">&quot;cboettig/knitcitations@v1&quot;</span>)</code></pre>
<p>You can see this package in use, for instance, in providing dynamic citations for my <code>RNeXML</code> <a href="https://github.com/ropensci/RNeXML/blob/7a6be7bd0106bc91a5586ee614b3cf5249627692/manuscripts/manuscript.Rmd">mansucript draft</a>.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/06/12/knitcitations-updates.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/06/04/is-statistical-software-harmful.html">Is statistical software harmful?</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 04 Jun 2014</p>

<article>
<div class="excerpt">
<p>Ben Bolker has an excellent post on this complex issue over <a href="http://dynamicecology.wordpress.com/2014/06/04/guest-post-is-statistical-software-harmful">at Dynamic Ecology</a>, which got me thinking about writing my own thoughts on the topic in reply.</p>
<hr />
<p>Google recently announced that it will be making it’s own self-driving cars, rather than modifying those of others. <a href="http://www.automotive.com/news/1405-google-envisions-self-driving-cars-with-no-steering-wheel/">Cars that won’t have steering wheels and pedals</a>. Just a button that says “stop.” What does this tell us about the future of user-friendly complex statistical software?</p>
<p>Ben quotes prominent statisticians voicing fears that echo common concerns about self-driving cars:</p>
<blockquote>
<p>Andrew Gelman attributes to Brad Efron the idea that “recommending that scientists use Bayes’ theorem is like giving the neighbourhood kids the key to your F-16″.</p>
</blockquote>
<p>I think it is particularly interesting and instructive that the quote Gelman attributes to Efron is about a mathematical theorem rather than about software (e.g. Bayes Theorem, not WinBUGS). Even relatively simple statistical concepts like <span class="math">\(p\)</span> values can cause plenty of confusion, statistical package or no. The concerns are not unique to software, so the solutions cannot come through limiting access to software.</p>
<p>I am very wary of the suggestion that we should address concerns of appropriate application by raising barriers to access. Those arguments have been made about knowledge of all forms, from access to publications, to raw data, to things as basic as education and democratic voting.</p>
<p>There are many good reasons for not creating a statistical software implementation of a new method, but I argue here that fear of misuse just is not one of them.</p>
<ol type="1">
<li><em>The barriers created by not having a convenient software implementation are not an appropriate filter to keep out people who can miss-interpret or miss-use the software. As you know, a fundamentally different skillset is required to program a published algorithm (say, MCMC), than to correctly interpret the statistical consequences.</em></li>
</ol>
<p>We must be wary of a different kind of statistical machismo, in which we use the ability to implement a method by one’s self as a proxy for interpreting it correctly.</p>
<p>1a) One immediate corollary of (1) is that: <em>Like it or not, someone is going to build a method that is “easy to use”, e.g. remove the programming barriers.</em></p>
<p>1b) The second corollary is that: <em>individuals with excellent understanding of the proper interpretation / statistics will frequently make mistakes in the computational implementation.</em></p>
<p>Both mistakes will happen. And both are much more formidable problems in the complex methodology of today than when computer was a job description.</p>
<p>So, what do we do? I think we should abandon the <a href="http://www.r-bloggers.com/what-is-correctness-for-statistical-software/">false dichotomy between “usability” and “correctness.”</a>. Just because software that is easy to use is easy to misuse, does not imply that decreasing usability increases correctness. I think that is a dangerous fallacy.</p>
<p>A software implementation should aim first to remove the programming barriers rather than statistical knowledge barriers. Best practices such as modularity and documentation should make it easy for users and developers to understand and build upon it. I agree with Ben that software error messages are poor teachers. I agree that a tool cannot be foolproof, no tool ever has been.</p>
<p>Someone does not misuse a piece of software merely because they do not understand it. Misuse comes from mistakenly thinking you understand it. The premise that most researchers will use something they do not understand just because it is easy to use is distasteful.</p>
<p>Kevin Slavin gives <a href="http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world">a fantastic Ted talk</a> on the ubiquitous role of algorithms in today’s world. His conclusion is neither one of panacea or doom, but rather that we seek to understand and characterize them, learn their strengths and weaknesses like a naturalist studies a new species.</p>
<p>More widespread adoption of software such as BUGS &amp; relatives has indeed increased the amount of misuse and false conclusions. But it has also dramatically increased awareness of issues ranging from computational aspects peculiar to particular implementations to general understanding and discourse about Bayesian methods. Like Kevin, I don’t think we can escape the algorithms, but I do think we can learn to understand and live with them.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/06/04/is-statistical-software-harmful.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/30/PLoS-data-sharing-policy-reflections.html">Plos Data Sharing Policy Reflections</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 30 May 2014</p>

<article>
<div class="excerpt">
<p>PLOS has posted an <a href="http://blogs.plos.org/biologue/2014/05/30/plos-data-policy-update/">excellent update</a> reflecting on their experiences a few months in to their new data sharing policy, which requires authors to include a statement of where the data can be obtained rather than providing it upon request. They do a rather excellent job of highlighting common concerns and offering well justified and explained replies where appropriate.</p>
<p>At the end of the piece they pose several excellent questions, which I reflect on here (mostly as a way of figuring out my own thoughts on these issues).</p>
<hr />
<ul>
<li><strong>When should an author choose Supplementary Files vs a repository vs figures and tables?</strong></li>
</ul>
<p>To me, repositories should always be the default. Academic repositories provide robust permanent archiving (such as <a href="http://clockss.org">CLOCKSS</a> backup), independent DOIs to content, often tracking of use metrics, enhanced discoverability, clear and appropriate licenses, richer metadata, as well as frequently providing things like API access and easy-to-use interfaces. They are the Silicon Valley of publishing innovation today.</p>
<p>Today I think it is much more likely that some material is not appropriate for a ‘journal supplement’ rather than not being able to find an appropriate repository (enough are free, subject agnostic and accept almost any file types). In my opinion the primary challenge is for publishers to tightly integrate the repository contents with their own website, something that the repositories themselves can support with good APIs and embedding tools (many do, PLOS’s coordination with figshare for individual figures being a great example).</p>
<p>I’m not clear on “vs figures and tables”, as this seems like a content question of “What” should be archived rather than “Where” (unless it is referring to separately archiving the figures and tables of the main text, which sounds like a great idea to me).</p>
<ul>
<li><strong>Should software/code be treated any differently from ‘data’? How should materials-sharing differ?</strong></li>
</ul>
<p>At the highest level I think it is possible to see software as a ‘type’ of data. Like other data, it is in need of appropriate licensing, a management plan, documentation/metadata, and conforming to appropriate standards and placed in appropriate repositories. Of course what is meant by “appropriate” differs, but that is also true between other types of data. The same motivations for requiring data sharing (understanding and replicating the work, facilitating future work, increasing impact) apply.</p>
<p>I think we as a scientific community (or rather, many loosely federated communities) are still working out just how best to share scientific code and the unique challenges that it raises. Traditional scientific data repositories are well ahead in establishing best practices for other data, but are rapidly working out approaches to code. The <a href="http://openresearchsoftware.metajnl.com/about/editorialPolicies">guidelines</a> from the Journal of Open Research Software from the UK Software Sustainability Institute are a great example. (I’ve written on this topic before, such as <a href="http://www.carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html">what I look for in software papers</a> and on the topic of the <a href="www.carlboettiger.info/2013/09/25/mozilla-software-review.html">Mozilla Science Code review pilot</a></p>
<p>I’m not informed enough to speak to sharing of non-digital material.</p>
<ul>
<li><strong>What does peer review of data mean, and should reviewers and editors be paying more attention to data than they did previously, now that they can do so?</strong></li>
</ul>
<p>In as much as we are satisfied with the current definition of peer review for journal articles I think this is a false dichotomy. Landmark papers, at least in my field, five or six decades ago (e.g. about as old as the current peer review system) frequently contained all the data in the paper (papers were longer and data was smaller). Somehow the data outgrew the paper and it just became okay to omit it, just as methods have gotten more complex and papers today frequently gloss over methodological details. The problem, then, is not one of type but one of scale: how do you review data when it takes up more than half a page of printed text.</p>
<p>The problem of scale is of course not limited to data. Papers frequently have many more authors than reviewers, often representing disparate and highly specialized expertise over possibly years of work, depend upon more than 100 citations and be accompanied by as many pages of supplemental material. To the extent that we’re satisfied with how reviewers and editors have coped with these trends, we can hope for the same for data.</p>
<p>Meanwhile, data transparency and data reuse may be more effective safe guards. Yes, errors in the data may cause trouble before they can be brought to light, just like bugs in software. But in this way they do eventually come to light, and that is somewhat less worrying if we view data the way we currently build publications (e.g. as fundamental building blocks of research) and publications as we currently view data (e.g. as a means to an ends, illustrated in the idea that it is okay to have mistakes in the data as long as they don’t change the conclusions). Jonathan Eisen has some <a href="http://www.slideshare.net/phylogenomics/jonathan-eisen-talk-on-open-science-at-bosc2012-ismb" title="see slide 13">excellent</a> <a href="https://www.youtube.com/watch?v=oWZzUe3Kxeo">examples</a> in which openly sharing the data led to rapid discovery and correction of errors that might have been difficult to detect otherwise.</p>
<ul>
<li><strong>And getting at the reason why we encourage data sharing: how much data, metadata, and explanation is necessary for replication?</strong></li>
</ul>
<p>I agree that the “What” question is a crux issue, and one we are still figuring out by community. There are really two issues here: what data to include, and what metadata (which to me includes any explanation or other documentation of the data) to provide for whatever data is included.</p>
<p>On appropriate metadata, we’ll never have a one-size-fits-all answer, but I think the key is to at least uphold current community best-practices (best != mode), whatever they may be. Parts of this are easy: scholarly archives everywhere include basic <a href="http://en.wikipedia.org/wiki/Dublin_Core">Dublin Core Elements</a> metadata like title, author, date, subject and unique identifier, and most data repositories will attach this information in a machine-readable metadata format with minimal burden on the author (e.g. <a href="http://datadryad.org">Dryad</a>, or to lesser extent, <a href="http://figshare.org">figshare</a>). Many fields already have well-established and tested standards for data documentation, such as the [Ecological Metadata Langauge], which helps ecologists document things like column names and units in an appropriate and consistent way without constraining how the data is collected or structured.</p>
<p>What data we include in the first place is more challenging, particularly as there is no good definition of ‘raw data’ (one person’s raw data being another person’s highly processed data). I think a useful minimum might be to provide any data shown in a figure or used in a statistical test that appears in the paper.</p>
<p>Journal policies can help most in each of these cases by pointing authors to the policies of repositories and to subject-specific publications on these best practices.</p>
<ul>
<li><strong>A crucial issue that is much wider than PLOS is how to cite data and give academic credit for data reuse, to encourage researchers to make data sharing part of their everyday routine.</strong></li>
</ul>
<p>Again I agree that credit for data reuse is an important and largely cultural issue. Certainly editors can play there part as they already do in encouraging authors to cite the corresponding papers on the methods used, etc.</p>
<p>I think the cultural challenge is much greater for the “long tail” content than it is for the most impactful data. I think most of the top-cited papers over the last two decades have been methods papers (or are cited for the use of a method that has become the standard of a field; often as software). As with any citation, there’s a positive feedback as more people are aware of it. I suspect that the papers announcing the first full genomes of commonly studied organisms (essentially data papers, though published by the most exclusive journals) did not lack citations. For data (or methods for that matter) that do not anticipate that level of reuse, the concern of appropriate credit is more real. Even if a researcher can assume they will be cited by future reuse of their data, they may not feel that sufficient compensation if it means one less paper to their name.</p>
<p>Unfortunately I think these are not issues unique to data publication but germane to academic credit in general. Citations, journal names, and so forth are not meaningless metrics, but very noisy ones. I think it is too easy to fool ourselves by looking only at cases where statistical averages are large enough to see the signal – datasets like the human genome and algorithms like BLAST we know are impactful, and the citation record bears this out. Really well cited papers or well-cited journals tend to coincide with our notions of impact, so it is easy to overestimate the fidelity of citation statistics when the sample size is much smaller. Besides, academic work is a high-dimensional creature not easily reduced to a few scalar metrics. <!--(I think that is why, at least in the US, we tend
to place more trust in the opinions of people over current metrics.)--></p>
<ul>
<li><strong>And for long-term preservation, we must ask who funds the costs of data sharing? What file formats should be acceptable and what will happen in the future with data in obsolete file formats? Is there likely to be universal agreement on how long researchers should store data, given the different current requirements of institutions and funders?</strong></li>
</ul>
<p>I think these are questions for the scientific data repositories and the fields they serve, rather than the journals, and for the most part they are handling them well.</p>
<p>Repositories like <a href="http://datadryad.org">Dryad</a> have clear pricing schemes closely integrated with other publication costs, and standing at least an order of magnitude less than most journal publication fees look like a bargain. (Not so if you publish in subscription journals I hear you say. Well, I would not be surprised if we start seeing such repositories negotiate institutional subscriptions to cover the costs of their authors).</p>
<p>I think the question of data formats is closely tied to that of metadata, as they are all topics of best-practices in archiving. Many scientific data repositories have usually put a lot of thought into these issues and also weigh them against the needs and ease-of-use of the communities they serve. Journal data archiving policies can play their part by encouraging best practices by pointing authors to repository guidelines as well as published articles from their community (such as the <a href="http://library.queensu.ca/ojs/index.php/IEE/article/view/4608">Nine Simple Ways</a> paper by White et al.)</p>
<p>I feel the almost rhetorical question about ‘universal agreement’ is unnecessarily pessimistic. I suspect that much of the variance in recommendations for the duration a researcher should archive their own work predates the widespread emergence of data repositories, which have vastly simplified the issue from when it was left up to each individual lab. Do we ask this question of the scientific literature? No, largely because many major journals have already provided robust long term archiving with <a href="http://clockss.org">CLOCKSS</a>/LOCKSS backup agreements. Likewise scientific data repositories seem to have settled for indefinite archiving. It seems both reasonable and practical that data archiving can be held to the same standard as the journal article itself. (Sure there are lots of challenging issues to be worked out here, the key is only to leave it in the hands of those already leading the way and not re-invent the wheel).</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/30/PLoS-data-sharing-policy-reflections.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/28/notes.html">packrat and rmarkdown</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 28 May 2014</p>

<article>
<div class="excerpt">
<p>I’m pretty happy with the way <code>rmarkdown</code> looks like it can pretty much replace my <a href="">Makefile approach</a> with a simple R command to <code>rmarkdown::render()</code>. Notably, a lot of the pandoc configuration can already go into the document’s <code>yaml</code> header (bibliography, csl, template, documentclass, etc), avoiding any messing around with the Makefile, etc.</p>
<p>Even more exciting is the pending RStudio integration with pandoc. This exposes the features of the <code>rmarkdown</code> package to the RStudio IDE buttons, but more importantly, seems like it will simplify the pandoc/latex dependency issues cross-platform.</p>
<p>In light of these developments, I wonder if I should separate my manuscripts from their corresponding R packages entirely (and/or treat them as vignettes?) I think it would be ideal to point people to a single <code>.Rmd</code> file and say “load this in RStudio” rather than passing along a whole working directory.</p>
<p>The <code>rmarkdown::render</code> workflow doesn’t cover installing the dependencies, or downloading the a pre-built cache. I’ve been relying on the R package mechanism itself to handle dependencies, though I list all packages loaded by the manuscript but not needed by the package functions themselves as <code>SUGGESTS</code>, as one would do with a vignette. Consequently, I’ve had to add an <a href="">install.R</a> script to my template, to make sure these packages are installed before a user attempts to run the document. The install script feels like a bit of a hack, and makes me think that RStudio’s packrat may be what I actually want for this. So I finally got around to playing with <a href="http://rstudio.github.io/packrat/">packrat</a>.</p>
<h2 id="packrat">packrat</h2>
<p>Packrat isn’t yet on CRAN, and for an RStudio package I admit that it feels a bit clunky still. Having a single <code>packrat.lock</code> file (think <code>Gemfile.lock</code> I suppose) seems like a great idea. Carting around the hidden files <code>.Rprofile</code>, <code>.Renviron</code>, and the <code>tar.gz</code> sources for all the dependences (in <code>packrat.sources</code>) seems heavy and clunky, and logging in and out all the time feels like a hack.</p>
<ul>
<li><p>Am I really supposed to commit the <code>.tar.gz</code> files? <a href="https://github.com/rstudio/packrat/issues/59">packrat/issues/59</a> (Summary: option coming)</p></li>
<li><p>Do I really need to restart R <a href="https://github.com/rstudio/packrat/issues/60">packrat/issues/60</a> (Summary: yes).</p></li>
</ul>
<p>The first discussion led to an interesting question about just how big are CRAN packages these days anyhow? Thanks to this clever <code>rsync</code> trick from Duncan, I could quickly explore this:</p>
<pre class="sourceCode r"><code class="sourceCode r">txt =<span class="st"> </span><span class="kw">system</span>(<span class="st">&quot;rsync --list-only cran.r-project.org::CRAN/src/contrib/ | grep .tar.gz&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>)
<span class="kw">setAs</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;num.with.commas&quot;</span>, function(from) <span class="kw">as.numeric</span>(<span class="kw">gsub</span>(<span class="st">&quot;,&quot;</span>, <span class="st">&quot;&quot;</span>, from) ) )
ans =<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">textConnection</span>(txt), <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;num.with.commas&quot;</span>, <span class="st">&quot;Date&quot;</span>, <span class="st">&quot;character&quot;</span>))
<span class="kw">ggplot</span>(ans, <span class="kw">aes</span>(V3, V2)) +<span class="st"> </span><span class="kw">geom_point</span>()
<span class="kw">sum</span>(ans$V2&gt;<span class="fl">1e6</span>)
<span class="kw">sum</span>(ans$V2/<span class="fl">1e6</span>)</code></pre>
<figure>
<img src="https://cloud.githubusercontent.com/assets/222586/3122393/cbe422b8-e766-11e3-9048-016dc21c55e9.png" alt="cran" /><figcaption>cran</figcaption>
</figure>
<p>Note that there are 711 packages over 1 MB, for a total weight of over 2.8 GB. Not huge but more than you might want in a Git repo all the same.</p>
<p>Nevertheless, packrat works pretty well. Using a bit of a hack <a href="https://groups.google.com/forum/#!topic/packrat-discuss/sm46dsvLxSk">we can</a> just version manage/ship the <code>packrat.lock</code> file and let packrat try and restore the rest.</p>
<pre class="sourceCode r"><code class="sourceCode r">packrat::<span class="kw">packify</span>()
<span class="kw">source</span>(<span class="st">&quot;.Rprofile&quot;</span>); <span class="kw">readRenviron</span>(<span class="st">&quot;.Renviron&quot;</span>)
packrat::<span class="kw">restore</span>()
<span class="kw">source</span>(<span class="st">&quot;.Rprofile&quot;</span>); <span class="kw">readRenviron</span>(<span class="st">&quot;.Renviron&quot;</span>)</code></pre>
<p>The <code>source</code>/<code>readRenviron</code> calls should really be restarts to R. Tried replacing this with calls to <code>Rscript -e &quot;packrat::packify()</code> etc. but that fails to find <code>packrat</code> on the second call. (Attempting to reinstall it doesn’t work either).</p>
<p>Provided the sources haven’t disappeared from their locations on Github, CRAN, etc., I think this strategy should work just fine. More long-term, we would want to archive a tarball with the <code>packrat.sources</code>, perhaps downloading it from a script as I currently do with the cache archive.</p>
<h2 id="knitcitations">knitcitations</h2>
<p>Debugging check reveals some pretty tricky behavior on R’s part: it wants to check the R code in my vignette even though it’s not building the vignette. It does this by tangling out the code chunks, which ignores in-line code. Not sure if this should be a bug in knitr or R, but it can’t be my fault ;-). See <a href="https://github.com/yihui/knitr/issues/784">knitr/issues/784</a></p>
<p>With checks passing, have sent v0.6 to CRAN. Fingers crossed…</p>
<p>Milestones for <a href="https://github.com/cboettig/knitcitations/issues?milestone=4&amp;state=open">version 0.7</a> should be able to address the print formatting issues, hopefully as a new <code>citation_format</code> option and without breaking backwards compatibility.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/28/notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

  </div>
</div> <!--end row -->

<div class="row socialicons">
  <div class="col-md-11 col-md-offset-1">
      <p> <a href="/archive.html"><i class="icon-calendar"></i> All entries by date</a></p> 
      <p> <a href="/categories.html"><i class="icon-list"></i> All entries by category</a> </p>
      <p> <a href="/tags.html"><i class="icon-tags"></i> All entries by tag</a> </p>
  </div> <!--end col-md-9 -->
</div> <!--end row -->




      <footer class="footer">

<!--************** FOAF information to social networks ***************************** -->
  <div class="row">
    <div class="col-md-3 col-xs-4 socialicons" style="font-size:20px" typeof="foaf:Person" about="http://www.carlboettiger.info#me">
      <p>
          <script type="text/javascript" src="/assets/js/obfuscate-email-link.js" language="javascript"></script> 

          <a rel="foaf:account" alt="twitter" href="https://twitter.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); 
             return false;"><span class="showtooltip" title="follow me on twitter (reading, discussing)"><i class="fa fa-twitter"></i></span></a> 

          <a rel="foaf:account" alt="github" href="https://github.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); 
             return false;"><span class="showtooltip" title="follow me on Github (code, research)"><i class="fa fa-github"></i></span></a>
      <!--
          <a rel="foaf:account" href="https://plus.google.com/" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'GPlus'); 
             return false;"><i class="fa fa-google-plus"></i></a>

          <a rel="foaf:account" href="http://www.mendeley.com/profiles/carl-boettiger" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); 
             return false;"><img src="/assets/img/icon-mendeley.png" alt="mendeley" /></a> 

           citations on google-scholar

           stackoverflow
      -->
      <a alt="rss" rel="foaf:weblog", type="application/atom+xml" href="/blog.xml"  
         class="showtooltip" title="RSS feeds for my blog-style entries. See the feed on my lab notebook (/atom.xml) to follow all entries instead." 
         onclick="recordOutboundLink(this, 'Outbound Links', 'RSS'); 
         return false;"><i class="fa fa-rss"></i></a>
       </p>
    </div>

    
    <!--**************** End social links **************************** -->


    <div class="col-md-4 col-md-offset-1 col-xs-4">
      <p><a onclick="recordOutboundLink(this, 'Outbound Links', 'ONS_claim'); return false;" href="http://onsclaims.wikispaces.com/"><img src="/assets/img/ons-aci2-icon.svg" alt="ONS" class="showtooltip" title="An Open Notebook Science (ONS) project claim: Entry provides all content (AC) immediately (I) or without significant delay.  See link for details"/></a></p>
    </div>


    <div class="col-md-3 col-md-offset-1 col-xs-4">
      <p>
      <a rel="license" property="http://creativecommons.org/ns#license" href="http://creativecommons.org/publicdomain/zero/1.0/" onclick="recordOutboundLink(this, 'Outbound Links', 'CC0'); return false;"><img src="/assets/img/cc-zero.svg" alt="CC0"/></a> 
      </p>
    </div>
  </div>


  
<!-- COinS metadata (for citation managers like Zotero etc), goes in body text -->
  <span
      class="Z3988" 
      title="ctx_ver=Z39.88-2004
      &amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc
      &amp;rfr_id=info%3Asid%2Focoins.info%3Agenerator
      &amp;rft.title=Lab Notebook
      &amp;rft.creator=Carl Boettiger
      &amp;rft.date=
      &amp;rft.language=EN
      &amp;rft.rights=CC0
      &amp;rft_id=http://www.carlboettiger.info/lab-notebook.html">
  </span>


</footer>




          <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

    <!-- JQuery, used on a few pages (still?) -->
    <!-- <script type="text/javascript" src="/assets/js/jquery.js"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Equations using MathJax -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });       </script>
    <!-- Twitter Bootstrap Javascript -->
    <!--  <script src="/assets/js/bootstrap.min.js"></script> -->
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>


    

        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-18401403-1']);
          _gaq.push(['_trackPageview']);
          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
  </script>



<script type="text/javascript">
function recordOutboundLink(link, category, action) {
  try {
    var pageTracker=_gat._getTracker("UA-18401403-1");
    pageTracker._trackEvent(category, action);
    setTimeout('document.location = "' + link.href + '"', 100)
  }catch(err){}
}
</script>




    </div>
  </body>
</html>
   
