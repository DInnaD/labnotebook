<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head prefix="dc: http://purl.org/dc/terms/ og: http://ogp.me/ns#"> <!-- namespaces used in metadata.html -->
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8'/>
  <title>Lab Notebook</title>
  <meta name="author" content="Carl Boettiger" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- HTML5 metadata -->
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="resource_type" content="website"/> 
<!-- RDFa Metadata (in DublinCore) -->
<meta property="dc:title" content="Lab Notebook" />
<meta property="dc:creator" content="Carl Boettiger" />
<meta property="dc:date" content="" />
<meta property="dc:format" content="text/html" />
<meta property="dc:language" content="en" />
<meta property="dc:identifier" content="/lab-notebook.html" />
<meta property="dc:rights" content="CC0" />
<meta property="dc:source" content="Lab Notebook" />
<meta property="dc:subject" content="Ecology" /> 
<meta property="dc:type" content="website" /> 
<!-- RDFa Metadata (in OpenGraph) -->
<meta property="og:title" content="Lab Notebook" />
<meta property="og:author" content="http://www.carlboettiger.info/index.html#me" />  <!-- Should be Liquid? URI? -->
<meta property="http://ogp.me/ns/profile#first_name" content="Carl"/>
<meta property="http://ogp.me/ns/profile#last_name" content="Boettiger"/>
<meta property="http://ogp.me/ns/article#published_time" content="" />
<meta property="og:site_name" content="Lab Notebook" /> <!-- Same as dc:source? -->
<meta property="og:url" content="http://www.carlboettiger.info/lab-notebook.html" />
<meta property="og:type" content="website" /> 
<!-- Google Scholar Metadata -->
<meta name="citation_author" content="Carl Boettiger"/>
<meta name="citation_date" content=""/>
<meta name="citation_title" content="Lab Notebook"/>
<meta name="citation_journal_title" content="Lab Notebook"/>
<!--NOTE: see also the COinS Metadata in span element in footer -->




  <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!-- Help the browser identify the RSS feed automatically -->
  <link rel="alternate" type="application/rss+xml" title="Carl Boettiger's Lab Notebook" href="/blog.xml" />
</head>


  <body prefix="dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/"> 
    <!-- Navbar  ================================================== -->

<nav class="navbar navbar-default" role="navigation">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/README.html"><i class="icon-info-sign" alt="info"></i></span></a>
    </div>

 <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

          <li  >
          <a href="/index.html">Home</a></li>
          <li  >
          <a href="/vita.html">Vita</a></li>
          <li  >
          <a href="/research.html">Research</a></li>
          <li  >
          <a href="/teaching.html">Teaching</a></li>
          <li  >
          <a href="/community.html">Community</a></li>
          <li  class="active" >
          <a href="/lab-notebook.html">Lab Notebook</a></li>

        </ul>

      <!-- Search site using Google's index -->
        <form class="navbar-form navbar-right" role="search" method="get" action="http://google.com/search">
          <div class="form-group">
            <input type="hidden" name="q" value="site:carlboettiger.info" />
            <input type="text" class="form-control search-query" name="q" placeholder="Search"/>
          </div>
          <button class="btn btn-mini" type="submit"><i class="icon-search"></i></button> 
       </form>

    </div><!--/.nav-collapse -->
  </div> <!-- /container -->
</nav>



    <div class="container"> <!-- Responsive grid layout, doesn't jump to full-width --> 
      <header>
        <h1 class="entry-title">Lab Notebook</h1>
        <h2>(<a href="http://www.carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html">Introduction</a>)</h3>
      </header>

      <div class="row feed">
  <div class="col-md-3 col-md-offset-1">
    <h4>  <a property="account" href="https://github.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); return false;"><i class="icon-github" alt="github"></i> Coding </a></h4> 
    <div class="excerpt">
      <div class="scroll">
        <ul><li>cboettig pushed to master at cboettig/labnotebook: <em>tweak category layout fix tag page layout</em> <a href="https://github.com/cboettig/labnotebook/compare/2629df716f...de63372b42">10:42 2014/06/18</a></li><li>cboettig pushed to master at cboettig/labnotebook: <em>new post add ? in title</em> <a href="https://github.com/cboettig/labnotebook/compare/bd9c84a4a4...2629df716f">10:34 2014/06/18</a></li><li>cboettig created tag v1.0-1 at cboettig/knitcitations: <em></em> <a href="https://github.com/cboettig/knitcitations/tree/v1.0-1">10:24 2014/06/18</a></li><li>cboettig pushed to master at cboettig/knitcitations: <em>Merge branch 'v1' updated options documentation</em> <a href="https://github.com/cboettig/knitcitations/compare/e5b25c34fb...0aed93ab61">10:23 2014/06/18</a></li><li>cboettig pushed to master at cboettig/knitcitations: <em>merge use external libs for crossref, update docs, remove deprecated functions</em> <a href="https://github.com/cboettig/knitcitations/compare/a10b601545...e5b25c34fb">10:23 2014/06/18</a></li></ul>
      </div>
    </div>
  </div>
  <div class="col-md-3">
    <h4> <a property="account" href="https://twitter.com/cboettig" onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); return false;"><i class="icon-twitter"></i> Discussing </a></h4> 
     <div class="excerpt">
      <div class="scroll">
       <ul><li><p>Looks like the rest of this issue of IAOS is also dedicated to open data and  #reproducibleresearch   <a href="http://t.co/bWCwBFuy09">http://t.co/bWCwBFuy09</a></p>
 <a href="http://twitter.com/cboettig/statuses/479372335653195776">09:17 2014/06/18</a> </li><li><p>&quot;The reproducible research movement in statistics&quot; in StatJ of the IAOS, by @victoriastodden (pdf) <a href="http://t.co/g6nT4HbIbV">http://t.co/g6nT4HbIbV</a></p>
 <a href="http://twitter.com/cboettig/statuses/479372296872677377">09:17 2014/06/18</a> </li><li><p>From @NatureMagazine&quot;Open access: Sharing your data is easier than you think&quot; <a href="http://t.co/dT4GpRaheB">http://t.co/dT4GpRaheB</a></p>
 <a href="http://twitter.com/cboettig/statuses/479321989182541824">05:57 2014/06/18</a> </li><li><p>discussing use of @github issues tracker in academic workflows <a href="http://t.co/InSpijaoL2">http://t.co/InSpijaoL2</a></p>
 <a href="http://twitter.com/cboettig/statuses/479302896077385728">04:41 2014/06/18</a> </li><li><p>@phylorich @github Agreed, but can&#39;t see them allowing javascript.  but maybe mathml at least?</p>
 <a href="http://twitter.com/cboettig/statuses/478688625643618304">12:00 2014/06/17</a> </li></ul>
      </div>
    </div> 
  </div> 
  <div class="col-md-3">
    <h4> <a href="http://www.mendeley.com/groups/634301/theoretical-ecology/papers/" onClick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); return false;"><i class="icon-book"></i> Reading </a></h4> 
    <div class="excerpt">
      <div class="scroll">
<ul><li>Coevolution can reverse predator-prey cycles: Proceedings of the National Academy of Sciences (2014). M. H. Cortez, J. S. Weitz et al. <a href="/research/coevolution-reverse-predatorprey-cycles/">08:54 2014/05/06</a></li><li>Early-Warning Signs for Pattern-Formation in Stochastic Partial Differential Equations: Pages: 26. Karna Gowda, Christian Kuehn et al. <a href="/research/earlywarning-signs-patternformation-stochastic-partial-differential-equations/">08:17 2014/05/01</a></li><li>Structural asymmetry and the stability of diverse food webs.: Nature (2006). Volume: 442, Issue: 7100. Pages: 265-9. Neil Rooney, Kevin McCann, Gabriel Gellner, John C Moore et al. <a href="/catalog/structural-asymmetry-stability-diverse-food-webs/">08:16 2014/05/01</a></li><li>Foraging adaptation and the relationship between food-web complexity and stability.: Science (New York, N.Y.) (2003). Volume: 299, Issue: 5611. Pages: 1388-91. Michio Kondoh et al. <a href="/catalog/foraging-adaptation-relationship-between-foodweb-complexity-stability/">08:16 2014/05/01</a></li></ul>
      </div>
    </div>
  </div> 
</div>

<hr>
<div class="row postpreview">
  <div class="col-md-11 col-md-offset-1">
    <div class="row">
      <h4> <a href="http://www.carlboettiger.info/atom.xml"
              onClick="recordOutboundLink(this,
              'Outbound Links', 'RSS'); return false;"
              style="color: inherit;"
              ><i class="icon-rss" ></i> Entries</a></h4>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/06/12/knitcitations-updates.html">Knitcitations Updates</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 12 Jun 2014</p>

<article>
<div class="excerpt">
<p>Used some down-time while traveling to hammer out a long overdue update to my <a href="https://github.com/cboettig/knitcitations">knitcitations</a> package.</p>
<p>My first task inovled a backwards-compatible update fixing a few minor issues (see <a href="https://github.com/cboettig/knitcitations/blob/master/NEWS">NEWS</a>) and providing pandoc style inline citations <a href="https://github.com/cboettig/knitcitations/releases/tag/v0.6-2"><code>v0.6-2</code></a>, on CRAN.</p>
<p>I followed this with a ground-up rewrite, as I summarize in NEWS:</p>
<h2 id="v1.0-1">v1.0-1</h2>
<p>This version is a ground-up rewrite of knitcitations, providing a more powerful interface while also streamlining the back end, mostly by relying more on external libraries for knitty gritty. While an effort has been made to preserve the most common uses, some lesser-used functions or function arguments have been significantly altered or removed. Bug reports greatly appreciated.</p>
<ul>
<li><p><code>citet</code>/<code>citep</code> now accept more options. In addition to the four previously supported options (DOI, URL, bibentry or bibkey (of a previously cited work)), these now accept a plain text query (used in a CrossRef Search), or a path to a PDF file (which attempts metadata extraction).</p></li>
<li><p>Citation key generation is now handled internally, and cannot be configured just by providing a named argument to <code>citet</code>/<code>citep</code>.</p></li>
<li><p>The <code>cite</code> function is replaced by <code>bib_metadata</code>. This function takes any argument to <code>citet</code>/<code>citep</code> as before (including the new arguments), see docs.</p></li>
<li><p>Linked inline citations now use the configuration: <code>cite_options(style=&quot;markdown&quot;, hyperlink=&quot;to.doc&quot;)</code> provides a link to the DOI or URL of the document, using markdown format.</p></li>
<li><p>Support for cito and tooltip have been removed. These may be restored at a later date. (The earlier implementation did not appropriately abstract the use of these features from the style/formatting of printing the citation, making generalization hard.</p></li>
<li><p><code>bibliography</code> now includes CSL support directly for entries with a DOI using the <code>style=</code> argument. No need to provide a CSL file itself, just the name of the journal (or rather, the name of the corresponding csl file: full journal name, all lower case, spaces as dashes). See https://github.com/cboettig/knitcitations/issues/38</p></li>
<li><p><code>bibliography</code> formatting has otherwise been completely rewritten, and no longer uses <code>print_markdown</code>, <code>print_html</code>, and <code>print_rdfa</code> methods. rdfa is no longer available, and other formats are controlled through <code>cite_options</code>. For formal publication pandoc mode is recommended instead of <code>bibliography</code>.</p></li>
</ul>
<p>This version was developed on a separate branch (<code>v1</code>), and has only just been merged back into master. CRAN doesn’t like getting multiple updates in the same month or so, but hopefully waiting a bit longer will give users and I a chance to shake out bugs anway. Meanwhile grab it from github with:</p>
<pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">install_github</span>(<span class="st">&quot;cboettig/knitcitations@v1&quot;</span>)</code></pre>
<p>You can see this package in use, for instance, in providing dynamic citations for my <code>RNeXML</code> <a href="https://github.com/ropensci/RNeXML/blob/7a6be7bd0106bc91a5586ee614b3cf5249627692/manuscripts/manuscript.Rmd">mansucript draft</a>.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/06/12/knitcitations-updates.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/06/04/is-statistical-software-harmful.html">Is statistical software harmful?</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 04 Jun 2014</p>

<article>
<div class="excerpt">
<p>Ben Bolker has an excellent post on this complex issue over <a href="http://dynamicecology.wordpress.com/2014/06/04/guest-post-is-statistical-software-harmful">at Dynamic Ecology</a>, which got me thinking about writing my own thoughts on the topic in reply.</p>
<hr />
<p>Google recently announced that it will be making it’s own self-driving cars, rather than modifying those of others. <a href="http://www.automotive.com/news/1405-google-envisions-self-driving-cars-with-no-steering-wheel/">Cars that won’t have steering wheels and pedals</a>. Just a button that says “stop.” What does this tell us about the future of user-friendly complex statistical software?</p>
<p>Ben quotes prominent statisticians voicing fears that echo common concerns about self-driving cars:</p>
<blockquote>
<p>Andrew Gelman attributes to Brad Efron the idea that “recommending that scientists use Bayes’ theorem is like giving the neighbourhood kids the key to your F-16″.</p>
</blockquote>
<p>I think it is particularly interesting and instructive that the quote Gelman attributes to Efron is about a mathematical theorem rather than about software (e.g. Bayes Theorem, not WinBUGS). Even relatively simple statistical concepts like <span class="math">\(p\)</span> values can cause plenty of confusion, statistical package or no. The concerns are not unique to software, so the solutions cannot come through limiting access to software.</p>
<p>I am very wary of the suggestion that we should address concerns of appropriate application by raising barriers to access. Those arguments have been made about knowledge of all forms, from access to publications, to raw data, to things as basic as education and democratic voting.</p>
<p>There are many good reasons for not creating a statistical software implementation of a new method, but I argue here that fear of misuse just is not one of them.</p>
<ol type="1">
<li><em>The barriers created by not having a convenient software implementation are not an appropriate filter to keep out people who can miss-interpret or miss-use the software. As you know, a fundamentally different skillset is required to program a published algorithm (say, MCMC), than to correctly interpret the statistical consequences.</em></li>
</ol>
<p>We must be wary of a different kind of statistical machismo, in which we use the ability to implement a method by one’s self as a proxy for interpreting it correctly.</p>
<p>1a) One immediate corollary of (1) is that: <em>Like it or not, someone is going to build a method that is “easy to use”, e.g. remove the programming barriers.</em></p>
<p>1b) The second corollary is that: <em>individuals with excellent understanding of the proper interpretation / statistics will frequently make mistakes in the computational implementation.</em></p>
<p>Both mistakes will happen. And both are much more formidable problems in the complex methodology of today than when computer was a job description.</p>
<p>So, what do we do? I think we should abandon the <a href="http://www.r-bloggers.com/what-is-correctness-for-statistical-software/">false dichotomy between “usability” and “correctness.”</a>. Just because software that is easy to use is easy to misuse, does not imply that decreasing usability increases correctness. I think that is a dangerous fallacy.</p>
<p>A software implementation should aim first to remove the programming barriers rather than statistical knowledge barriers. Best practices such as modularity and documentation should make it easy for users and developers to understand and build upon it. I agree with Ben that software error messages are poor teachers. I agree that a tool cannot be foolproof, no tool ever has been.</p>
<p>Someone does not misuse a piece of software merely because they do not understand it. Misuse comes from mistakenly thinking you understand it. The premise that most researchers will use something they do not understand just because it is easy to use is distasteful.</p>
<p>Kevin Slavin gives <a href="http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world">a fantastic Ted talk</a> on the ubiquitous role of algorithms in today’s world. His conclusion is neither one of panacea or doom, but rather that we seek to understand and characterize them, learn their strengths and weaknesses like a naturalist studies a new species.</p>
<p>More widespread adoption of software such as BUGS &amp; relatives has indeed increased the amount of misuse and false conclusions. But it has also dramatically increased awareness of issues ranging from computational aspects peculiar to particular implementations to general understanding and discourse about Bayesian methods. Like Kevin, I don’t think we can escape the algorithms, but I do think we can learn to understand and live with them.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/06/04/is-statistical-software-harmful.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/30/PLoS-data-sharing-policy-reflections.html">Plos Data Sharing Policy Reflections</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 30 May 2014</p>

<article>
<div class="excerpt">
<p>PLOS has posted an <a href="http://blogs.plos.org/biologue/2014/05/30/plos-data-policy-update/">excellent update</a> reflecting on their experiences a few months in to their new data sharing policy, which requires authors to include a statement of where the data can be obtained rather than providing it upon request. They do a rather excellent job of highlighting common concerns and offering well justified and explained replies where appropriate.</p>
<p>At the end of the piece they pose several excellent questions, which I reflect on here (mostly as a way of figuring out my own thoughts on these issues).</p>
<hr />
<ul>
<li><strong>When should an author choose Supplementary Files vs a repository vs figures and tables?</strong></li>
</ul>
<p>To me, repositories should always be the default. Academic repositories provide robust permanent archiving (such as <a href="http://clockss.org">CLOCKSS</a> backup), independent DOIs to content, often tracking of use metrics, enhanced discoverability, clear and appropriate licenses, richer metadata, as well as frequently providing things like API access and easy-to-use interfaces. They are the Silicon Valley of publishing innovation today.</p>
<p>Today I think it is much more likely that some material is not appropriate for a ‘journal supplement’ rather than not being able to find an appropriate repository (enough are free, subject agnostic and accept almost any file types). In my opinion the primary challenge is for publishers to tightly integrate the repository contents with their own website, something that the repositories themselves can support with good APIs and embedding tools (many do, PLOS’s coordination with figshare for individual figures being a great example).</p>
<p>I’m not clear on “vs figures and tables”, as this seems like a content question of “What” should be archived rather than “Where” (unless it is referring to separately archiving the figures and tables of the main text, which sounds like a great idea to me).</p>
<ul>
<li><strong>Should software/code be treated any differently from ‘data’? How should materials-sharing differ?</strong></li>
</ul>
<p>At the highest level I think it is possible to see software as a ‘type’ of data. Like other data, it is in need of appropriate licensing, a management plan, documentation/metadata, and conforming to appropriate standards and placed in appropriate repositories. Of course what is meant by “appropriate” differs, but that is also true between other types of data. The same motivations for requiring data sharing (understanding and replicating the work, facilitating future work, increasing impact) apply.</p>
<p>I think we as a scientific community (or rather, many loosely federated communities) are still working out just how best to share scientific code and the unique challenges that it raises. Traditional scientific data repositories are well ahead in establishing best practices for other data, but are rapidly working out approaches to code. The <a href="http://openresearchsoftware.metajnl.com/about/editorialPolicies">guidelines</a> from the Journal of Open Research Software from the UK Software Sustainability Institute are a great example. (I’ve written on this topic before, such as <a href="http://www.carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html">what I look for in software papers</a> and on the topic of the <a href="www.carlboettiger.info/2013/09/25/mozilla-software-review.html">Mozilla Science Code review pilot</a></p>
<p>I’m not informed enough to speak to sharing of non-digital material.</p>
<ul>
<li><strong>What does peer review of data mean, and should reviewers and editors be paying more attention to data than they did previously, now that they can do so?</strong></li>
</ul>
<p>In as much as we are satisfied with the current definition of peer review for journal articles I think this is a false dichotomy. Landmark papers, at least in my field, five or six decades ago (e.g. about as old as the current peer review system) frequently contained all the data in the paper (papers were longer and data was smaller). Somehow the data outgrew the paper and it just became okay to omit it, just as methods have gotten more complex and papers today frequently gloss over methodological details. The problem, then, is not one of type but one of scale: how do you review data when it takes up more than half a page of printed text.</p>
<p>The problem of scale is of course not limited to data. Papers frequently have many more authors than reviewers, often representing disparate and highly specialized expertise over possibly years of work, depend upon more than 100 citations and be accompanied by as many pages of supplemental material. To the extent that we’re satisfied with how reviewers and editors have coped with these trends, we can hope for the same for data.</p>
<p>Meanwhile, data transparency and data reuse may be more effective safe guards. Yes, errors in the data may cause trouble before they can be brought to light, just like bugs in software. But in this way they do eventually come to light, and that is somewhat less worrying if we view data the way we currently build publications (e.g. as fundamental building blocks of research) and publications as we currently view data (e.g. as a means to an ends, illustrated in the idea that it is okay to have mistakes in the data as long as they don’t change the conclusions). Jonathan Eisen has some <a href="http://www.slideshare.net/phylogenomics/jonathan-eisen-talk-on-open-science-at-bosc2012-ismb" title="see slide 13">excellent</a> <a href="https://www.youtube.com/watch?v=oWZzUe3Kxeo">examples</a> in which openly sharing the data led to rapid discovery and correction of errors that might have been difficult to detect otherwise.</p>
<ul>
<li><strong>And getting at the reason why we encourage data sharing: how much data, metadata, and explanation is necessary for replication?</strong></li>
</ul>
<p>I agree that the “What” question is a crux issue, and one we are still figuring out by community. There are really two issues here: what data to include, and what metadata (which to me includes any explanation or other documentation of the data) to provide for whatever data is included.</p>
<p>On appropriate metadata, we’ll never have a one-size-fits-all answer, but I think the key is to at least uphold current community best-practices (best != mode), whatever they may be. Parts of this are easy: scholarly archives everywhere include basic <a href="http://en.wikipedia.org/wiki/Dublin_Core">Dublin Core Elements</a> metadata like title, author, date, subject and unique identifier, and most data repositories will attach this information in a machine-readable metadata format with minimal burden on the author (e.g. <a href="http://datadryad.org">Dryad</a>, or to lesser extent, <a href="http://figshare.org">figshare</a>). Many fields already have well-established and tested standards for data documentation, such as the [Ecological Metadata Langauge], which helps ecologists document things like column names and units in an appropriate and consistent way without constraining how the data is collected or structured.</p>
<p>What data we include in the first place is more challenging, particularly as there is no good definition of ‘raw data’ (one person’s raw data being another person’s highly processed data). I think a useful minimum might be to provide any data shown in a figure or used in a statistical test that appears in the paper.</p>
<p>Journal policies can help most in each of these cases by pointing authors to the policies of repositories and to subject-specific publications on these best practices.</p>
<ul>
<li><strong>A crucial issue that is much wider than PLOS is how to cite data and give academic credit for data reuse, to encourage researchers to make data sharing part of their everyday routine.</strong></li>
</ul>
<p>Again I agree that credit for data reuse is an important and largely cultural issue. Certainly editors can play there part as they already do in encouraging authors to cite the corresponding papers on the methods used, etc.</p>
<p>I think the cultural challenge is much greater for the “long tail” content than it is for the most impactful data. I think most of the top-cited papers over the last two decades have been methods papers (or are cited for the use of a method that has become the standard of a field; often as software). As with any citation, there’s a positive feedback as more people are aware of it. I suspect that the papers announcing the first full genomes of commonly studied organisms (essentially data papers, though published by the most exclusive journals) did not lack citations. For data (or methods for that matter) that do not anticipate that level of reuse, the concern of appropriate credit is more real. Even if a researcher can assume they will be cited by future reuse of their data, they may not feel that sufficient compensation if it means one less paper to their name.</p>
<p>Unfortunately I think these are not issues unique to data publication but germane to academic credit in general. Citations, journal names, and so forth are not meaningless metrics, but very noisy ones. I think it is too easy to fool ourselves by looking only at cases where statistical averages are large enough to see the signal – datasets like the human genome and algorithms like BLAST we know are impactful, and the citation record bears this out. Really well cited papers or well-cited journals tend to coincide with our notions of impact, so it is easy to overestimate the fidelity of citation statistics when the sample size is much smaller. Besides, academic work is a high-dimensional creature not easily reduced to a few scalar metrics. <!--(I think that is why, at least in the US, we tend
to place more trust in the opinions of people over current metrics.)--></p>
<ul>
<li><strong>And for long-term preservation, we must ask who funds the costs of data sharing? What file formats should be acceptable and what will happen in the future with data in obsolete file formats? Is there likely to be universal agreement on how long researchers should store data, given the different current requirements of institutions and funders?</strong></li>
</ul>
<p>I think these are questions for the scientific data repositories and the fields they serve, rather than the journals, and for the most part they are handling them well.</p>
<p>Repositories like <a href="http://datadryad.org">Dryad</a> have clear pricing schemes closely integrated with other publication costs, and standing at least an order of magnitude less than most journal publication fees look like a bargain. (Not so if you publish in subscription journals I hear you say. Well, I would not be surprised if we start seeing such repositories negotiate institutional subscriptions to cover the costs of their authors).</p>
<p>I think the question of data formats is closely tied to that of metadata, as they are all topics of best-practices in archiving. Many scientific data repositories have usually put a lot of thought into these issues and also weigh them against the needs and ease-of-use of the communities they serve. Journal data archiving policies can play their part by encouraging best practices by pointing authors to repository guidelines as well as published articles from their community (such as the <a href="http://library.queensu.ca/ojs/index.php/IEE/article/view/4608">Nine Simple Ways</a> paper by White et al.)</p>
<p>I feel the almost rhetorical question about ‘universal agreement’ is unnecessarily pessimistic. I suspect that much of the variance in recommendations for the duration a researcher should archive their own work predates the widespread emergence of data repositories, which have vastly simplified the issue from when it was left up to each individual lab. Do we ask this question of the scientific literature? No, largely because many major journals have already provided robust long term archiving with <a href="http://clockss.org">CLOCKSS</a>/LOCKSS backup agreements. Likewise scientific data repositories seem to have settled for indefinite archiving. It seems both reasonable and practical that data archiving can be held to the same standard as the journal article itself. (Sure there are lots of challenging issues to be worked out here, the key is only to leave it in the hands of those already leading the way and not re-invent the wheel).</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/30/PLoS-data-sharing-policy-reflections.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/28/notes.html">packrat and rmarkdown</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 28 May 2014</p>

<article>
<div class="excerpt">
<p>I’m pretty happy with the way <code>rmarkdown</code> looks like it can pretty much replace my <a href="">Makefile approach</a> with a simple R command to <code>rmarkdown::render()</code>. Notably, a lot of the pandoc configuration can already go into the document’s <code>yaml</code> header (bibliography, csl, template, documentclass, etc), avoiding any messing around with the Makefile, etc.</p>
<p>Even more exciting is the pending RStudio integration with pandoc. This exposes the features of the <code>rmarkdown</code> package to the RStudio IDE buttons, but more importantly, seems like it will simplify the pandoc/latex dependency issues cross-platform.</p>
<p>In light of these developments, I wonder if I should separate my manuscripts from their corresponding R packages entirely (and/or treat them as vignettes?) I think it would be ideal to point people to a single <code>.Rmd</code> file and say “load this in RStudio” rather than passing along a whole working directory.</p>
<p>The <code>rmarkdown::render</code> workflow doesn’t cover installing the dependencies, or downloading the a pre-built cache. I’ve been relying on the R package mechanism itself to handle dependencies, though I list all packages loaded by the manuscript but not needed by the package functions themselves as <code>SUGGESTS</code>, as one would do with a vignette. Consequently, I’ve had to add an <a href="">install.R</a> script to my template, to make sure these packages are installed before a user attempts to run the document. The install script feels like a bit of a hack, and makes me think that RStudio’s packrat may be what I actually want for this. So I finally got around to playing with <a href="http://rstudio.github.io/packrat/">packrat</a>.</p>
<h2 id="packrat">packrat</h2>
<p>Packrat isn’t yet on CRAN, and for an RStudio package I admit that it feels a bit clunky still. Having a single <code>packrat.lock</code> file (think <code>Gemfile.lock</code> I suppose) seems like a great idea. Carting around the hidden files <code>.Rprofile</code>, <code>.Renviron</code>, and the <code>tar.gz</code> sources for all the dependences (in <code>packrat.sources</code>) seems heavy and clunky, and logging in and out all the time feels like a hack.</p>
<ul>
<li><p>Am I really supposed to commit the <code>.tar.gz</code> files? <a href="https://github.com/rstudio/packrat/issues/59">packrat/issues/59</a> (Summary: option coming)</p></li>
<li><p>Do I really need to restart R <a href="https://github.com/rstudio/packrat/issues/60">packrat/issues/60</a> (Summary: yes).</p></li>
</ul>
<p>The first discussion led to an interesting question about just how big are CRAN packages these days anyhow? Thanks to this clever <code>rsync</code> trick from Duncan, I could quickly explore this:</p>
<pre class="sourceCode r"><code class="sourceCode r">txt =<span class="st"> </span><span class="kw">system</span>(<span class="st">&quot;rsync --list-only cran.r-project.org::CRAN/src/contrib/ | grep .tar.gz&quot;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>)
<span class="kw">setAs</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;num.with.commas&quot;</span>, function(from) <span class="kw">as.numeric</span>(<span class="kw">gsub</span>(<span class="st">&quot;,&quot;</span>, <span class="st">&quot;&quot;</span>, from) ) )
ans =<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">textConnection</span>(txt), <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;num.with.commas&quot;</span>, <span class="st">&quot;Date&quot;</span>, <span class="st">&quot;character&quot;</span>))
<span class="kw">ggplot</span>(ans, <span class="kw">aes</span>(V3, V2)) +<span class="st"> </span><span class="kw">geom_point</span>()
<span class="kw">sum</span>(ans$V2&gt;<span class="fl">1e6</span>)
<span class="kw">sum</span>(ans$V2/<span class="fl">1e6</span>)</code></pre>
<figure>
<img src="https://cloud.githubusercontent.com/assets/222586/3122393/cbe422b8-e766-11e3-9048-016dc21c55e9.png" alt="cran" /><figcaption>cran</figcaption>
</figure>
<p>Note that there are 711 packages over 1 MB, for a total weight of over 2.8 GB. Not huge but more than you might want in a Git repo all the same.</p>
<p>Nevertheless, packrat works pretty well. Using a bit of a hack <a href="https://groups.google.com/forum/#!topic/packrat-discuss/sm46dsvLxSk">we can</a> just version manage/ship the <code>packrat.lock</code> file and let packrat try and restore the rest.</p>
<pre class="sourceCode r"><code class="sourceCode r">packrat::<span class="kw">packify</span>()
<span class="kw">source</span>(<span class="st">&quot;.Rprofile&quot;</span>); <span class="kw">readRenviron</span>(<span class="st">&quot;.Renviron&quot;</span>)
packrat::<span class="kw">restore</span>()
<span class="kw">source</span>(<span class="st">&quot;.Rprofile&quot;</span>); <span class="kw">readRenviron</span>(<span class="st">&quot;.Renviron&quot;</span>)</code></pre>
<p>The <code>source</code>/<code>readRenviron</code> calls should really be restarts to R. Tried replacing this with calls to <code>Rscript -e &quot;packrat::packify()</code> etc. but that fails to find <code>packrat</code> on the second call. (Attempting to reinstall it doesn’t work either).</p>
<p>Provided the sources haven’t disappeared from their locations on Github, CRAN, etc., I think this strategy should work just fine. More long-term, we would want to archive a tarball with the <code>packrat.sources</code>, perhaps downloading it from a script as I currently do with the cache archive.</p>
<h2 id="knitcitations">knitcitations</h2>
<p>Debugging check reveals some pretty tricky behavior on R’s part: it wants to check the R code in my vignette even though it’s not building the vignette. It does this by tangling out the code chunks, which ignores in-line code. Not sure if this should be a bug in knitr or R, but it can’t be my fault ;-). See <a href="https://github.com/yihui/knitr/issues/784">knitr/issues/784</a></p>
<p>With checks passing, have sent v0.6 to CRAN. Fingers crossed…</p>
<p>Milestones for <a href="https://github.com/cboettig/knitcitations/issues?milestone=4&amp;state=open">version 0.7</a> should be able to address the print formatting issues, hopefully as a new <code>citation_format</code> option and without breaking backwards compatibility.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/28/notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/07/integrating-github-project-repos-into-the-notebook.html">Integrating Github Project Repos Into The Notebook</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 07 May 2014</p>

<article>
<div class="excerpt">
<p>For a while now most of my active research is developed through <code>.Rmd</code> scripts connected to a particular project repository (something I discuss at length in <a href="http://www.carlboettiger.info/2014/05/05/knitr-workflow-challenges.html">deep challenges with knitr workflows</a>). In the <a href="http://www.carlboettiger.info/2014/05/06/steps-to-a-more-portable-workflow.html">previous post</a> I discuss creating a <code>template</code> package with a more transparent organization of files, such as moving manuscripts from <code>inst/doc/</code> to simply <code>manuscripts/</code>. This left these exploratory analysis scripts in <code>inst/examples</code> in a similarly unintuitive place. Though I like having these scripts as part of the repository (which keeps everything for a project in one place, as it were), like the manuscript they aren’t really part of the R package, particularly as I have gotten better at creating proper unit tests in place of just rerunning dynamic scripts occasionally.</p>
<p>I’ve also been nagged by the idea of having to always just link to these nice dynamic documents from my lab notebook. Sure Github renders the markdown so that it’s easy enough to see highlighted code and figures etc., but it still makes them seem rather external. Occasionally I would copy the complete <code>.md</code> file into a notebook post, but this divorced it of it’s original version history and associated <code>.Rmd</code> source.</p>
<p>One option would be to move them all directly into my lab notebook, <code>.Rmd</code> files and all. This would integrate the scripts more nicely than Github’s own rendering, matching the URL and look and feel of my notebook. It would also allow for javascript elements such as MathJax equations, Google Analytics, and Disqus that are not possible when only linking to an <code>.md</code> file on Github.</p>
<p>In the recent <a href="https://github.com/ropensci/docs">ropensci/docs</a> project we are exploring a way to have Jekyll automatically compile (potentially with caching) a site that uses <code>.Rmd</code> posts and deploy to Github all using <code>travis</code>, but we’re not quite finished and this is potentially fragile particularly with the hundreds of posts in this notebook. Besides this, the notebook structure is rather temporally oriented, (posts are chronological and reflected in my URL structure) while these scripts are largely project-oriented. (Consistent use of categories and tags would ameliorate this).</p>
<h3 id="embedding-images-in-.rmd-outputs">Embedding images in <code>.Rmd</code> outputs</h3>
<p>A persistent challenge has been how best to deal with images created by these scripts, some of which I may run many times. By default <code>knitr</code> creates <code>png</code> images, which as binary files are ill suited for committing to Github, and which could bloat a repository rather quickly. For a long while I have used custom hooks to push these images to <code>flickr</code>, (see <a href="http://flickr.com/cboettig">flickr.com/cboettig</a>), inserting the permanent flickr URL into the output markdown.</p>
<p>Recently Martin Fenner convinced me that <code>svg</code> files would both render more nicely across a range of devices (being vector graphics), and could be easily committed to Github as they are text-based (XML) files, so that reproducing the same image in repeated runs wouldn’t take up any more space. We can then browse a nice version history of the any particular figure, and this also keeps all the output material together, making it easier to archive permanently (certainly nicer than my old archiving solution using data URIs.). Lastly, <code>svg</code> is both web native, being a standard namespace of HTML5, and potentially interactive, as the <a href="http://www.omegahat.org/SVGAnnotation/">SVGAnnotation</a> R package illustrates. So, lots of advantages in using <code>svg</code> graphics.</p>
<p>Using <code>svg</code> files also bring some unique challenges. Unlike when <code>png</code> files are added to Github, webpages cannot directly link them since Github enforces rendering them as text instead of an image through its choice of HTML header, for security reasons. This means the only way to link to an <code>svg</code> file on Github is to have that file on a <code>gh-pages</code> branch, where it can be rendered as a website. A distinct disadvantage of this approach is that while we can link to a specific version of any file on Github, we see only the most recent version rendered on the website created by a <code>gh-pages</code> branch.</p>
<p>On the other hand, having the <code>svg</code> files on the <code>gh-pages</code> branch further keeps down the footprint of the project <code>master</code> branch. This leads rather naturally to the idea that the <code>.Rmd</code> files and their <code>.md</code> outputs should also appear on the <code>gh-pages</code> branch. This removes them from their awkward home in <code>inst/examples/</code>, and enables all the benefits of custom CSS, custom javascript, and custom URLs that we don’t have on Github’s rendering.</p>
<p>To provide a consistent look and feel, I merely copied over the <code>_layouts</code> and <code>_includes</code> from my lab notebook, tweaking them slightly to use the assets already hosted there. I add custom domain name for the all my <code>gh-pages</code> as a sub-domain, <code>io.carlboettiger.info</code> <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, and now instead of having script output appear like so:</p>
<p><a href="https://github.com/cboettig/nonparametric-bayes/blob/7dd8fc444cb9d20d839286eac8068b3099ea9b6a/inst/examples/gaussian-process-basics.md">nonparametric-bayes/inst/examples/gaussian-process-basics.md</a></p>
<p>I have the same page rendered on my <code>io</code> sub-domain:</p>
<p><a href="http://io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html">io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html</a></p>
<p>with its mathjax, disqus, matching css, URL and nav elements.</p>
<h2 id="landing-pages">Landing pages</h2>
<p>An obvious extension of this approach is to grab a copy of the repository README and rename it <code>index.md</code> and add a yaml header such that it serves as a landing page for the repository. A few lines of Liquid code can then generate the links to the other output scripts, as in this example:</p>
<p><a href="http://io.carlboettiger.info/nonparametric-bayes/">io.carlboettiger.info/nonparametric-bayes</a></p>
<h2 id="template">Template</h2>
<p>I have added a <code>gh-pages</code> branch with this set up to my new <code>template</code> repository, with some more <a href="http://io.carlboettiger.info/template/README">basic documentation and examples</a>.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There’s no need to use a different sub-domain than the rest of my website, other than that it would require my notebook be hosted on the <a href="https://github.com/cboettig/cboettig.github.com">cboettig.github.com</a> repo instead of <a href="https://github.com/cboettig/labnotebook">labnotebook</a>. However I prefer keeping my hosting on the repository I already have, and it also seems a bit unorthodox to host all my repositories on my main domain. In particular, it increases the chance for URL collisions if I create a repository with the same name as a page or directory on my website. Having gh-pages on the <code>io</code> sub-domain feels like just the right amount of separation to me.<a href="#fnref1">↩</a></p></li>
</ol>
</section>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/07/integrating-github-project-repos-into-the-notebook.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/06/steps-to-a-more-portable-workflow.html">Steps To A More Portable Workflow</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 06 May 2014</p>

<article>
<div class="excerpt">
<p>While I have made <a href="http://www.carlboettiger.info/2012/05/06/research-workflow.html">my workflow</a> for most of my ongoing projects available on Github for some time, this does not mean that it has been particularly easy to follow. Further, as I move from project to project I have slowly improved how I handle projects. For instance, I have since added unit tests (with <code>testthat</code>) and continuous integration (with <a href="http://travis-ci.org">travis-ci</a>) to my repositories, and my handling of manuscripts has gotten more automated, with richer latex templates, yaml metadata, and simpler and more powerful makefiles.</p>
<p>Though I have typically used my most recent project as a template for my next one (not so trivial as I work on several at a time), I realized it would make sense to just maintain a general template repo with all the latest goodies. I have now launched my <a href="https://github.com/cboettig/template">template</a> on Github.</p>
<p>I toyed with the idea of just treating the manuscript as a standard vignette, but this would make <code>pandoc</code> an external dependency for the package, putting an unecessary burden on <code>travis</code> and users. I settled on creating a <code>manuscripts</code> directory in the project root folder as the most semantically obvious place. This is added to <code>.Rbuildignore</code> as it doesn’t fit the standard structure of an R package, but since it is not a vignette and cannot be built with the package dependencies anyhow, this seems to make sense to me.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>The manuscript itslef is written in <code>.Rmd</code>, with a <code>yaml</code> header for the usual metadata of authors, affiliations, and so forth. Pandoc’s recent support for <a href="https://github.com/cboettig/template/blob/master/manuscripts/manuscript.Rmd#L1-27">yaml metadata</a> makes it much easier to use <code>.Rmd</code> with a LaTeX template, making <code>.Rnw</code> rather unnecessary. <a href="https://github.com/cboettig/template/blob/master/manuscripts/components/elsarticle.latex">My template</a> includes a custom <code>LaTeX</code> template that includes pandoc’s macros for inserting authors, affiliations, and so forth in the correct LaTeX elements, though pandoc’s <a href="https://github.com/jgm/pandoc-templates/blob/master/default.latex">default template</a> is rather good and already has macros for most things in place (meaning you can merely declare the layout or font in the yaml header and magically see the tex interpret it).</p>
<p>I have tried to keep the <code>manuscripts</code> directory relatively clean, placing <code>csl</code>, <code>bibtex</code>, <code>figures/</code>, <code>cache/</code> and other such files in a <code>components/</code> sub-directory. I have also tried to keep the <code>Makefile</code> as platform-independent as possible by having it call little Rscripts (also housed in <code>components/</code>) rather than commandline utilities like <code>sed -i</code> and <code>wget</code> that may not behave the same way on all platforms.</p>
<p>Lastly, Ryan Batts recently convinced me that providing binary cache files of results was an important way to allow a reader to quickly engage in exploring an analysis without having to first let potentially long-running code execute. <code>knitr</code> provides an excellent way to create and manage this caching on a code chunk by chunk level, which is also crucial when editing a dynamic document with intensive code (no one wants to rerun your MCMC just to rebuild the pdf). Since git/Github seems like a poor option for distributing binaries, I have for the moment just archived the cache on a (university) web server and added a Make/Rscript line to that can restore it from that location. Upon publication this cache could be permanently archived (along with plain text tables of the graphs) and then installed from that archive instead.</p>
<p>I have also added a separate <a href="https://github.com/cboettig/template/blob/master/manuscripts/README.md">README</a> in the manuscripts directory to provide some guidance to a user seeking to build the manuscript.</p>
<p>Examples of an active projects currently using this layout for manuscripts, etc include <a href="https://github.com/ropensci/RNeXML">RNeXML</a> and <a href="https://github.com/cboettig/nonparametric-bayes/">nonparametric-bayes</a></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Perhaps I should not have the manuscript on the master branch at all, but putting it on another branch would defeat the purpose of having it in an obviously-named directory of the repository home page where it is most easy to discover.<a href="#fnref1">↩</a></p></li>
</ol>
</section>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/06/steps-to-a-more-portable-workflow.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

    <div class="row">
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/05/knitr-workflow-challenges.html">Deep challenges to dynamic documentation in daily workflows</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 05 May 2014</p>

<article>
<div class="excerpt">
<p>We often discuss dynamic documents such as <code>Sweave</code> and <code>knitr</code> in reference to final products such as publications or software package vignettes. In this case, all the elements involved are already fixed: external functions, code, text, and so forth. The dynamic documentation engine is really just a tool to combine them (knit them together). Using dynamic documentation on a day-to-day basis on ongoing research presents a compelling opportunity but a rather more complex challenge as well. The code base grows, some of it gets turned into external custom functions where it continues to change. One analysis script branches into multiple that vary this or that. The text and figures are likewise subject to the same revision as the code, expanding and contracting, or being removed or shunted off into an appendix.</p>
<p>Structuring a dynamic document when all the parts are morphing and moving is one of the major opportunities for the dynamic approach, but also the most challenging. Here I describe some of those challenges along with various tricks I have adopted to deal with them, mostly in hopes that someone with a better strategy might be inspired to fill me in.</p>
<h2 id="the-old-way">The old way</h2>
<p>For a while now I have been using the <a href="http://yihui.name/knitr">knitr</a> dynamic documentation/reproducible research software for my project workflow. Most discussion of dynamic documentation focuses on ‘finished’ products such as journal articles or reports. Over the past year, I have found the dynamic documentation framework to be particularly useful as I develop ideas, and remarkably more challenging to then integrate into a final paper in a way that really takes advantage of its features. I explain both in some detail here.</p>
<p>My former workflow followed a pattern no doubt familiar to many:</p>
<ul>
<li>Bash away in an R terminal, paste useful bits into an R script…</li>
<li>Write manuscript separately, pasting in figures, tables, and in-line values returned from R.</li>
</ul>
<p>This doesn’t leave much of a record of what I did or why, which is particularly frustrating when some discussion reminds me of an earlier idea.</p>
<h2 id="dynamic-docs-.rmd-files">Dynamic docs: <code>.Rmd</code> files</h2>
<p>When I begin a new project, I now start off writing a <code>.Rmd</code> file, intermixing notes to myself and code chunks. Chunks break up the code into conceptual elements, markdown gives me a more expressive way to write notes than comment lines do. Output figures, tables, and in-line values inserted. So far so good. I version manage this creature in git/Github. Great, now I have a trackable history of what is going on, and all is well:</p>
<ol type="1">
<li><p>Document my thinking and code as I go along on a single file scratch-pad</p></li>
<li><p>Version-stamped history of what I put in and what I got out on each step of the way</p></li>
<li><p>Rich markup with equations, figures, tables, embedded.</p></li>
<li><p>Caching of script chunks, allowing me to tweak and rerun an analysis without having to execute the whole script. While we can of course duplicate that behavior with careful save and load commands in a script, in knitr this comes for free.</p></li>
</ol>
<h2 id="limitations-to-.rmd-alone">Limitations to .Rmd alone</h2>
<ol type="1">
<li><p>As I go along, the <code>.Rmd</code> files starts getting too big and cluttered to easily follow the big picture of what I’m trying to do.</p></li>
<li><p>Before long, my investigation branches. Having followed one <code>.Rmd</code> script to some interesting results, I start a new <code>.Rmd</code> script representing a new line of investigation. This new direction will nevertheless want to re-use large amounts of code from the first file.</p></li>
</ol>
<h2 id="a-solution-the-r-package-research-compendium-approach">A solution? The R package “research compendium” approach</h2>
<p>I start abstracting tasks performed in chunks into functions, so I can re-use these things elsewhere, loop over them, and document them carefully somewhere I can reference that won’t be in the way of what I’m thinking. I start to move these functions into <code>R/</code> directory of an R package structure, documenting with <code>Roxygen</code>. I write unit tests for these functions (in <code>inst/tests</code>) to have quick tests to check their sanity without running my big scripts (recent habit). The package structure helps me:</p>
<ul>
<li>Reuse the same code between two analyses without copy-paste or getting our of sync</li>
<li>Document complicated algorithms outside of my working scripts</li>
<li>Test complicated algorithms outside of my working scripts (<code>devtools::check</code> and/or unit tests)</li>
<li>Manage dependencies on other packages (DESCRIPTION, NAMESPACE), including other projects of mine</li>
</ul>
<p>This runs into trouble in several ways.</p>
<h2 id="problem-1-reuse-of-code-chunks">Problem 1: Reuse of code chunks</h2>
<p>What to do with code I want to reuse across blocks but do not want to write as a function, document, or test?</p>
<p><em>Perhaps this category of problem doesn’t exist, except in my laziness.</em></p>
<p>This situation arises all the time, usually through the following mechanism: almost any script performs several steps that are best represented as chunks calling different functions, such as <code>load_data</code>, <code>set_fixed_parameters</code>, <code>fit_model</code>, <code>plot_fits</code>, etc. I then want to re-run almost the same script, but with a slightly different configuration (such as a different data set or extra iterations in the fixed parameters). For just a few such cases, it doesn’t make sense to write these into a single function,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> instead, I copy this script to a new file and make the changes there.</p>
<p>This is great until I want to change something in about the way both scripts behave that cannot be handled just by changing the <code>R/</code> functions they share. Plotting options are a good example of this (I tend to avoid wrapping <code>ggplot</code> calls as separate functions, as it seems to obfuscate what is otherwise a rather semantic and widely recognized, if sometimes verbose, function call).</p>
<p>I have explored using <code>knitr</code>’s support for external chunk inclusion, which allows me to maintain a single R script with all commonly used chunks, and then import these chunks into multiple <code>.Rmd</code> files. An example of this can be seen in my <code>nonparametric-bayes</code> repo, where several files (in the same directory) draw most of their code from <a href="https://github.com/cboettig/nonparametric-bayes/blob/9232dfd814c40e3c48c5a837be110a870d8639da/inst/examples/BUGS/external-chunks.R">external-chunks.R</a>.</p>
<h2 id="problem-2-package-level-reproducibility">Problem 2: package-level reproducibility</h2>
<p><em>Minor/relatively easy to fix.</em></p>
<p>Separate files can frustrate reproducibility of a given commit. As I change the functions in <code>R/</code>, the <code>.Rmd</code> file can give different results despite being unchanged. (Or fail to reflect changes because it is caching chunks and does not recognize the function definitions have changed underneath it). Git provides a solution to this: since the <code>.Rmd</code> file lives in the same git repository (<code>inst/examples</code>) as the package, I can make sure the whole repository matches the hash of the <code>.Rmd</code> file: <code>install_github(&quot;packagename&quot;, &quot;cboettig&quot;, &quot;hash&quot;)</code>.</p>
<p>This solution is not fail-safe: the installed version, the potentially uncommitted (but possibly installed) version of the R functions in the working directory, and the R functions present at the commit of the <code>.Rmd</code> file (and thus matching the hash) could all be different. If we commit and install before every <code>knit</code>, we can avoid these potential errors (at the cost of some computational overhead), restoring reproducibility to the chain.</p>
<h2 id="problem-3-synthesizing-results-into-a-manuscript">Problem 3: Synthesizing results into a manuscript</h2>
<p>In some ways this is the easiest part, since the code-base is relatively static and it is just a matter of selecting which results and figures to include and what code is necessary to generate it. A few organizational challenges remain:</p>
<p>While we generally want <code>knitr</code> code chunks for the figures and tables that will appear, we usually aren’t interested in displaying much, if any, of the actual code in the document text (unlike the examples until this point, where this was a major advantage of the knitr approach). In principle, this is as simple as setting <code>echo=FALSE</code> in the global chunk options. In practice, it means there is little benefit to having the chunks interwoven in the document. What I tend to want is having all the chunks run at the beginning, such that any variables or results can easily be added (and their appearance tweaked by editing the code) as figure chunks or in-line expressions. The only purpose of maintaining chunks instead of a simple script is the piecewise caching of chunk dependencies which can help debugging.</p>
<p>Since displaying the code is suppressed, we are then left with the somewhat ironic challenge of how best to present code as a supplement. One option is simply to point to the source <code>.Rmd</code>, another is to use the <code>tangle()</code> option to extract all the code as a separate <code>.R</code> file. In either case, the user must also identify the correct version of the R package itself for the external <code>R/</code> functions.</p>
<h2 id="problem-4-branching-into-other-projects">Problem 4: Branching into other projects</h2>
<p>Things get most complicated when projects begin to branch into other projects. In an ideal world this is simple: a new idea can be explored on a new branch of the version control system and merged back in when necessary, and an entirely new project can be built as a new R package in a different repo that depends on the existing project. After several examples of each, I have learned that it is not so simple. Despite the nice tools, I’ve learned I still need to be careful in managing my workflows in order to leave behind material that is understandable, reproducible, and reflects clear provenance. So far, I’ve learned this the hard way. I use this last section of the post to reflect on two of my own examples, as writing this helps me work through what I should have done differently.</p>
<h3 id="example-warning-signals-project">example: warning-signals project</h3>
<p>For instance, my work on early warning signals dates back to the start of my <a href="http://openwetware.org/wiki/User:Carl_Boettiger/Notebook/Stochastic_Population_Dynamics/2010/02/09">open notebook on openwetware</a>, when my code lived on a Google code page which seems to have disappeared. (At the time it was part of my ‘stochastic population dynamics’ project). When I moved to Github, this project got it’s own repository, <a href="https://github.com/cboettig/warningsignals">warningsignals</a>, though after a major re-factorization of the code I moved to a new repository, <a href="https://github.com/cboettig/earlywarning">earlywarning</a>. Okay, so far that was due to me not really knowing what I was doing.</p>
<p>My first paper on this topic was based on the master branch of that repository, which still contains the code required. When one of the R dependencies was moved from CRAN I was able to update the codebase to reflect the replacement package (see issue <a href="https://github.com/cboettig/earlywarning/issues/10">#10</a>). Even before that paper appeared I started exploring other issues on different <a href="https://github.com/cboettig/earlywarning/network">branches</a>, with the <code>prosecutor</code> branch eventually becoming it’s own paper, and then it’s <a href="https://github.com/cboettig/prosecutors-fallacy/">own repository</a>.</p>
<p>That paper sparked a comment letter in response to it, and the analysis involved in my reply piece was just developed on the same master branch of the prosecutor-fallacy repository. This leaves me with a total of three repositories across four branches, with one repo that corresponds more-or-less directly to a paper, one to two papers, and one to no papers.</p>
<p>All four branches have diverged and unmerge-able code. Despite sharing and reusing functions across these projects, I often found it better to simply change the function on the new branch or new repo as I desired for the new work. These changes could not be easily merged back as they broke the original function calls of the earlier work.</p>
<p>Hindsight being 20-20, it would have been preferable that I had maintained one repository, perhaps developed each paper on a different branch and clearly tagged the commit corresponding to the submission of each publication. Ideally these could be merged back where possible to a master branch. Tagged commits provide a more natural solution than unmerged branches to deal with changes to the package that would break methods from earlier publications.</p>
<h3 id="example-optimal-control-projects">example: optimal control projects</h3>
<p>A different line of research began through a NIMBioS working group called “Pretty Darn Good Control”, beginning it’s digital life in my <a href="https://github.com/cboettig/pdg_control">pdg_control</a> repository. Working in different break-out groups as well as further investigation on my own soon created several different projects. Some of these have continue running towards publication, others terminating in dead ends, and still others becoming completely separate lines of work. Later work I have done in optimal control, such <a href="https://github.com/cboettig/nonparametric-bayes">nonparametric-bayes</a> and <a href="https://github.com/cboettig/multiple_uncertainty">multiple_uncertainty</a> depend on this package for certain basic functions, though both also contain their own diverged versions of functions that first appeared in <a href="https://github.com/cboettig/pdg_control">pdg_control</a>.</p>
<p>Because the topics are rather different and the shared code footprint is quite small, separate repositories probably makes more sense here. Still, managing the code dependencies in separate repositories requires extra care, as checking out the right version of the focal repository does not guarantee that one will also have the right version of the [pdg_control] repository. Ideally I should note the hash of [pdg_control] on which I depend, and preferably install that package at that hash (easy enough thanks to <code>devtools</code>), since depending on a separate project that is also still changing can be troublesome. Alternatively it might make more sense to just duplicate the original code and remove this potentially frail dependency. After all, documenting the provenance need not rely on the dependency, and it is more natural to think of these separate repos as divergent forks.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If I have a lot of different configurations, it may make sense to wrap up all these steps into a single function that takes input data and/or parameters as it’s argument and outputs a data frame with the results and inputs.<a href="#fnref1">↩</a></p></li>
</ol>
</section>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/05/knitr-workflow-challenges.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/04/why-I-sign-my-reviews.html">why I sign my reviews</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 04 May 2014</p>

<article>
<div class="excerpt">
<p>For the past four years I have made an effort to sign all my reviews (which I try to keep to about one a month). It isn’t because I believe in radical openness or something crazy like that. Its really just my self interest involved – at least mostly. Writing a review is an incredibly time consuming, and largely thankless task. Supposedly anonymous peer review is supposed to protect the reviewer, particularly the scenario of the less established scientist critiquing the work of the more established. I am sure it occasionally serves that purpose. On the other hand, that very scenario can be the <em>most</em> profitable time to sign a review. Really, when are you more likely to get an esteemed colleague to closely read your every argument than when you’re holding up their publication?</p>
<p>While the possibility of a vindictive and powerful author sounds daunting, but rather inconsistent with my impression of most scientists, who are more apt to be impressed by an intelligent even if flawed critique than by simple praise. I find it hardest to sign a review that I have found very little constructive criticism to offer, though after a decade of being trained to critique science one can always find something. (Of course signing can be hard on the occasional terrible paper for which it is hard to offer much constructive criticism, but fortunately that has been very rare). Both authors and other reviewers (who are sometimes sent the other reviews, a practice I find very educational as a reviewer) have on occasion commented or complemented me on reviews or acknowledged me in the papers, suggesting that the practice does indeed provide for some simple recognition. At times, it may sow seeds for future collaboration.</p>
<p>Signing my reviews has on occasion given the author a chance to follow up with me directly. While I’m not certain about journal policies in this regard, I suspect we can assume that we’re all adults capable of civil discussion. In any event, a phone call or even a few back-and-forth emails can be immensely more efficient in allowing an author to clarify elements that I have sometimes misunderstood or been unable to follow from the text, as well as making it easier to communicate my difficulties with the paper. In my experience this has resulted in both a faster and more satisfactory resolution to issues that have led to see some papers published more quickly and without as many tedious multiple rounds of revision. Given that many competitive journals simply cut off papers that might otherwise be successful with a bit more dialog between reviewer and author, because multiple “Revise and resubmits” put too much demand on editors, this seems like a desirable outcome for all involved. I’m not suggesting that such direct dialog is always desirable, but that no doubt many of us have been in the position in which a little dialog might have resolved issues more satisfactorily.</p>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/04/why-I-sign-my-reviews.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
        <div class="col-md-3">
          <header><h4><a href="/2014/05/02/scientific-computing-notes.html">Scientific Computing Notes</a></h4></header>
<p><span></span></p>
<p style="font-style:italic"> 02 May 2014</p>

<article>
<div class="excerpt">
<p>In my experience EC2 is good for some things but not for others. Consequently having some funding allocated for it would be great, but it would still be necessary to have other resources as well. I’ve found EC2 very good for running some reasonably portable analysis where you temporarily want some extra processors or memory. On the other hand, I’ve had some frustrations with it as well. You don’t have a persistent development environment unless you explicitly make and maintain a machine image, which means installing any software dependencies from scratch; sometimes a particular nuisance when you aren’t familiar with the architecture. So this adds more overhead on your time relative to administering your own machine, and much more than a university cluster with a human administrator. Obviously the latter is much more costly, but seems to scale well at least where I’ve seen that in universities.</p>
<p>Costs for maintaining a persistent image (e.g. for a web server), and costs for large data storage on things like S3 still seem a bit high, though actually the Berkeley cloud service listed on it’s IST site looks competitive. EC2 isn’t a great option for tasks that require more processors than can be fit with shared memory on a single node, where the architecture of a cluster is better for things that use MPI. Lastly, I want to think about what environment will be most accessible to my students and postdocs. While surely it’s good for them to learn their way around computing environments, at some point the barriers to enter or keep current with some of the cloud platforms in particular become a distraction from the research.</p>
<hr />
<ul>
<li>See Glenn Lockwood on <a href="http://glennklockwood.blogspot.com/2013/04/quick-mpi-cluster-setup-on-amazon-ec2.html">MPI setup</a> and <a href="http://glennklockwood.blogspot.com/2013/04/mpi-benchmarks-amazon-ec2-cluster.html">MPI benchmarks</a> on EC2.</li>
</ul>
 
<!-- not that raw_content depends on custom generator, 
     see _plugins/jekyll-labnotebook-plugins/raw_content --> 
</article> 
<p> <a href="/2014/05/02/scientific-computing-notes.html"> <em>Read more</em></a> </p>
<br />
<br />


        </div>
      
    </div>

  </div>
</div> <!--end row -->

<div class="row socialicons">
  <div class="col-md-11 col-md-offset-1">
      <p> <a href="/archive.html"><i class="icon-calendar"></i> All entries by date</a></p> 
      <p> <a href="/categories.html"><i class="icon-list"></i> All entries by category</a> </p>
      <p> <a href="/tags.html"><i class="icon-tags"></i> All entries by tag</a> </p>
  </div> <!--end col-md-9 -->
</div> <!--end row -->




      <footer class="footer">

<!--************** FOAF information to social networks ***************************** -->
  <div class="row">
    <div class="col-md-3 col-xs-4 socialicons" style="font-size:20px" typeof="foaf:Person" about="http://www.carlboettiger.info#me">
      <p>
          <script type="text/javascript" src="/assets/js/obfuscate-email-link.js" language="javascript"></script> 

          <a rel="foaf:account" alt="twitter" href="https://twitter.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Twitter'); 
             return false;"><span class="showtooltip" title="follow me on twitter (reading, discussing)"><i class="fa fa-twitter"></i></span></a> 

          <a rel="foaf:account" alt="github" href="https://github.com/cboettig" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Github'); 
             return false;"><span class="showtooltip" title="follow me on Github (code, research)"><i class="fa fa-github"></i></span></a>
      <!--
          <a rel="foaf:account" href="https://plus.google.com/" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'GPlus'); 
             return false;"><i class="fa fa-google-plus"></i></a>

          <a rel="foaf:account" href="http://www.mendeley.com/profiles/carl-boettiger" 
             onclick="recordOutboundLink(this, 'Outbound Links', 'Mendeley'); 
             return false;"><img src="/assets/img/icon-mendeley.png" alt="mendeley" /></a> 

           citations on google-scholar

           stackoverflow
      -->
      <a alt="rss" rel="foaf:weblog", type="application/atom+xml" href="/blog.xml"  
         class="showtooltip" title="RSS feeds for my blog-style entries. See the feed on my lab notebook (/atom.xml) to follow all entries instead." 
         onclick="recordOutboundLink(this, 'Outbound Links', 'RSS'); 
         return false;"><i class="fa fa-rss"></i></a>
       </p>
    </div>

    
    <!--**************** End social links **************************** -->


    <div class="col-md-4 col-md-offset-1 col-xs-4">
      <p><a onclick="recordOutboundLink(this, 'Outbound Links', 'ONS_claim'); return false;" href="http://onsclaims.wikispaces.com/"><img src="/assets/img/ons-aci2-icon.svg" alt="ONS" class="showtooltip" title="An Open Notebook Science (ONS) project claim: Entry provides all content (AC) immediately (I) or without significant delay.  See link for details"/></a></p>
    </div>


    <div class="col-md-3 col-md-offset-1 col-xs-4">
      <p>
      <a rel="license" property="http://creativecommons.org/ns#license" href="http://creativecommons.org/publicdomain/zero/1.0/" onclick="recordOutboundLink(this, 'Outbound Links', 'CC0'); return false;"><img src="/assets/img/cc-zero.svg" alt="CC0"/></a> 
      </p>
    </div>
  </div>


  
<!-- COinS metadata (for citation managers like Zotero etc), goes in body text -->
  <span
      class="Z3988" 
      title="ctx_ver=Z39.88-2004
      &amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc
      &amp;rfr_id=info%3Asid%2Focoins.info%3Agenerator
      &amp;rft.title=Lab Notebook
      &amp;rft.creator=Carl Boettiger
      &amp;rft.date=
      &amp;rft.language=EN
      &amp;rft.rights=CC0
      &amp;rft_id=http://www.carlboettiger.info/lab-notebook.html">
  </span>


</footer>




          <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

    <!-- JQuery, used on a few pages (still?) -->
    <!-- <script type="text/javascript" src="/assets/js/jquery.js"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Equations using MathJax -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });       </script>
    <!-- Twitter Bootstrap Javascript -->
    <!--  <script src="/assets/js/bootstrap.min.js"></script> -->
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>


    

        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-18401403-1']);
          _gaq.push(['_trackPageview']);
          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
  </script>



<script type="text/javascript">
function recordOutboundLink(link, category, action) {
  try {
    var pageTracker=_gat._getTracker("UA-18401403-1");
    pageTracker._trackEvent(category, action);
    setTimeout('document.location = "' + link.href + '"', 100)
  }catch(err){}
}
</script>




    </div>
  </body>
</html>
   
