---
categories:
- ecology
date: 2015-01-23T00:00:00Z
tags:
- pdg-control
url: /2015/01/23/mdptoolbox-ex/
---



<p>Testing this out using <code>MDPtoolbox</code>, but not working yet…</p>
<pre class="r"><code>library(MDPtoolbox)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loading required package: linprog</code></pre>
<pre><code>## Loading required package: lpSolve</code></pre>
<pre class="r"><code>library(ggplot2)</code></pre>
<pre class="r"><code>f &lt;- function(x, h, p){ 
  x &lt;- x - h
  p[[1]] * x / {1 + p[[2]] * x}  
}
p &lt;- c(a = 6, b = 0.05)

profit &lt;- function(x, h) pmin(x,h) - 0.1 * h  # vectorized for x
x_grid &lt;- seq(0, 150, length = 151)
h_grid &lt;- c(0,50,100)
delta &lt;- 0.001
sigma_g &lt;- 0.01
z_g &lt;- function() 1 + rlnorm(1, 0, sigma_g)
pdfn &lt;- function(P, s) dlnorm(P, 0, s)</code></pre>
<pre class="r"><code>M &lt;- pdgControl::determine_SDP_matrix(f, p, x_grid, h_grid, sigma_g, pdfn)
out &lt;- pdgControl::value_iteration(M, x_grid, h_grid, 1, 0, profit, delta)</code></pre>
<pre><code>## Warning in max(which(D == 1)): no non-missing arguments to max; returning -
## Inf</code></pre>
<p>Building the transition probability matrix:</p>
<pre class="r"><code>transition_matrix &lt;- function(f, p, x_grid, h_grid, sigma_g,
                                 pdfn=function(P, s) dlnorm(P, 0, s)){
  n_x &lt;- length(x_grid)
  n_h &lt;- length(h_grid)
  SDP_matrix &lt;-  array(0, c(length(x_grid), length(x_grid),length(h_grid)))
  
  for(h_i in 1:n_h){
    h &lt;- h_grid[h_i]
    # Cycle over x values
    for(i in 1:n_x){ 
      ## Calculate the expected transition  
      x1 &lt;- x_grid[i]
      x2_expected &lt;- f(x1, h, p)
      ## If expected 0, go to 0 with probabilty 1
      if( x2_expected == 0) 
        SDP_matrix[i,1,h_i] &lt;- 1  
      else {
        # relative probability of a transition to that state
        ProportionalChance &lt;- x_grid / x2_expected
        Prob &lt;- pdfn(ProportionalChance, sigma_g)
        
        if(sum(Prob) &gt; 0)
        # Store normalized probabilities in row
          SDP_matrix[i,,h_i] &lt;- Prob/sum(Prob)
        else 
          SDP_matrix[i,,h_i] &lt;- c(1, numeric(n_x-1))
        
        if(anyNA(SDP_matrix[i,,h_i]))
          recover()
      }
    }
  }
  SDP_matrix
}</code></pre>
<p>Building the reward matrix:</p>
<pre class="r"><code># [S, A] array

R &lt;- t(sapply(x_grid, function(x)
  sapply(h_grid, function(h)
  profit(x,h)
)))</code></pre>
<p>Note that in <span class="math inline">\(f\)</span> we must define the harvesting take place before the population recruits.</p>
<pre class="r"><code>P &lt;- transition_matrix(f, p, x_grid, h_grid, sigma_g, pdfn)</code></pre>
<pre class="r"><code>x0 &lt;- (p[[1]]-1)/p[[2]]
i &lt;- which.min(abs(x_grid-x0))

V0 &lt;- rep(0, length(x_grid))</code></pre>
<pre class="r"><code>mdp_check(P=P,R=R)</code></pre>
<pre><code>## [1] &quot;&quot;</code></pre>
<pre class="r"><code>out &lt;- mdp_value_iteration(P, R, discount = 1 / (1 + delta), epsilon=0.001, max_iter = 5e3, V0=V0)</code></pre>
<pre><code>## [1] &quot;MDP Toolbox: iterations stopped by maximum number of iteration condition&quot;</code></pre>
<pre class="r"><code>policy &lt;- out$policy

#out &lt;- mdp_finite_horizon(P, R, discount=1 / (1 + delta), 10)
#policy &lt;- out$policy[,1]</code></pre>
<p>Whoops, this doesn’t look right:</p>
<pre class="r"><code>plot(x_grid, h_grid[policy])</code></pre>
<p><img src="/lab-notebook/2015-01-23-mdptoolbox-ex_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
